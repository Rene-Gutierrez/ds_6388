# k-Means Clustering

## Introduction 

**k-Means clustering** is one of the most widely used unsupervised learning algorithms for **partitioning data** into distinct groups based on similarity. It is a simple yet powerful method that minimizes the **within-cluster variation**.

---

### Intuition Behind k-Means
Imagine you are tasked with grouping points on a map. k-Means aims to achieve this by:

✅ Placing \( k \) “centers” (called **centroids**) on the map.  
✅ Assigning each point to the **closest centroid**.  
✅ Adjusting the centroid positions to minimize the **distance between points and their assigned centroid**.  
✅ Repeating this process until the clusters are stable.  

The algorithm tries to minimize the **within-cluster sum of squares (WCSS)** — essentially grouping points that are **closer to each other** than to points in other clusters.  

---

### Applications of k-Means

k-Means is highly versatile and used in various domains:

- **Business and Marketing**
  - **Customer Segmentation:** Group customers based on purchasing behavior.  
  - **Market Segmentation:** Identify distinct user profiles for targeted marketing.  
- **Image and Video Processing**
  - **Color Quantization:** Compress images by reducing the number of colors.  
  - **Object Detection:** Cluster pixel intensities for image segmentation.  
- **Anomaly Detection**
  - Identify outliers by clustering data points and flagging those farthest from the centroids.  
- **Bioinformatics**
  - Cluster gene expression data for identifying gene functions.  

---

### Choosing the Number of Clusters \( k \)

Choosing \( k \) is critical for performance. Common techniques include:

✅ **Elbow Method:** Plot WCSS against \( k \) and identify the point where the reduction in WCSS slows down (the "elbow").  
✅ **Silhouette Score:** Measures how well each point fits into its cluster (higher is better).  
✅ **Gap Statistic:** Compares WCSS to that expected under random data.

---

### R Implementation of k-Means

#### Step 1: Load Libraries and Simulate Data

```{r k_means_simulated_data}
# Load necessary library
library(ggplot2)

# Simulate example data
set.seed(123)
n <- 300  # Number of points
X <- data.frame(
  x = c(rnorm(n/3, 0, 1), rnorm(n/3, 4, 1), rnorm(n/3, 8, 1)),
  y = c(rnorm(n/3, 0, 1), rnorm(n/3, 4, 1), rnorm(n/3, 8, 1))
)
```

#### Step 2: Perform k-Means Clustering

```{r k_means_run}
# Perform k-Means clustering
set.seed(123)  # Ensures reproducibility
k <- 3          # Number of clusters
kmeans_result <- kmeans(X, centers = k, nstart = 25)

# Add cluster labels to the dataset
X$cluster <- as.factor(kmeans_result$cluster)
```

#### Step 3: Visualize the Results

```{r k_means_visualization}
# Visualize the result
ggplot(X, aes(x = x, y = y, color = cluster)) +
  geom_point(size = 2) +
  geom_point(data = as.data.frame(kmeans_result$centers), 
             aes(x = x, y = y), 
             color = "black", size = 4, shape = 8) +
  labs(title = "k-Means Clustering", subtitle = "k = 3")
```

#### Step 4: Determine the Optimal Number of Clusters Using the Elbow Method

```{r k_means_optimal_cluster_elbow}
# Elbow method to find optimal k
wcss <- numeric(10)
for (k in 1:10) {
  wcss[k] <- kmeans(X, centers = k, nstart = 25)$tot.withinss
}

# Plot the WCSS to identify the 'elbow'
plot(1:10, wcss, type = "b", pch = 19, frame = FALSE,
     xlab = "Number of Clusters (k)", 
     ylab = "WCSS (Within-Cluster Sum of Squares)")
```


---

### Strengths and Weaknesses of k-Means

✅ **Strengths:**  
- Efficient for large datasets.  
- Simple and intuitive.  
- Fast convergence with \( O(n k d i) \) complexity.  

❗️**Weaknesses:**  
- Assumes **spherical clusters**.  
- Sensitive to **initialization** (use **k-Means++** to improve).  
- Struggles with **non-convex shapes** or **clusters with varying densities**.  
- May converge to **local minima** — multiple runs improve stability.  

---

### When to Use k-Means

✅ Use k-Means when:
- The data has **compact, well-separated clusters**.  
- The feature space is **low to moderate-dimensional**.  
- Speed is a priority (k-Means is fast).  

❗️Avoid k-Means when:
- Clusters are **non-convex** or **overlapping** (consider **DBSCAN** or **Spectral Clustering**).  
- The data has **significant outliers**.  

---

### Conclusion k-means
k-Means is a powerful yet simple clustering algorithm that performs well in many practical applications. While its efficiency is attractive, thoughtful parameter tuning and multiple runs are essential for achieving optimal results.

## Problem Statement

Given a dataset with \( n \) observations and \( d \) features:

\[
\mathbf{X} = \{ \mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n \} \quad \text{where} \quad \mathbf{x}_i \in \mathbb{R}^d
\]

The goal is to partition the data into \( k \) clusters \( \{ C_1, C_2, \dots, C_k \} \) such that the total **within-cluster sum of squares (WCSS)** is minimized:

\[
\min_{C_1, \dots, C_k} \sum_{j=1}^{k} \sum_{\mathbf{x}_i \in C_j} \| \mathbf{x}_i - \boldsymbol{\mu}_j \|^2
\]

where:
- \( \boldsymbol{\mu}_j = \frac{1}{|C_j|} \sum_{\mathbf{x}_i \in C_j} \mathbf{x}_i \) is the **centroid** of cluster \( C_j \).
- The objective minimizes the sum of squared distances between points and their respective cluster centroids.

---

## Naive Solution

**Why Can We Solve the Partition Problem by Evaluating Every Possible Partition?** To understand why a "naive" method — where we evaluate **all possible groupings of elements** — can correctly compute the number of partitions, we need to analyze the combinatorial structure of partitions.

---

### Why Evaluating All Possible Partitions Works?

To see why the naive approach is valid, we can build partitions in an **incremental way**:

#### Step 1: Start with an Empty Set

Suppose we have a set \( \{1, 2, \dots, n \} \).

#### Step 2: Add Elements One by One

When adding the \( i \)-th element to an existing partition:
- It can join **any existing subset**.
- Or it can form a **new subset**.

Thus, at each step, there are **two choices**:
- Place the element in one of the existing clusters.
- Start a new cluster with the element alone.

This combinatorial logic corresponds directly to the recurrence relation for Stirling numbers of the second kind:

\[
S(n, k) = k \cdot S(n-1, k) + S(n-1, k-1)
\]

- The first term represents placing the new element in one of the existing \( k \) groups.  
- The second term represents starting a **new** group, increasing the cluster count from \( k-1 \) to \( k \).  

---

### Why Does This Work for All Possible Partitions?

By systematically generating all possible combinations:
- Every valid partition is explored.  
- Each combination adheres to the non-overlapping, non-empty subset condition.  
- The method ensures no duplicates are counted by the incremental logic.

---

### Efficiency of the Naive Approach

- The naive method effectively explores the **lattice of set partitions**, where each state corresponds to a valid grouping.  
- While correct, this method has exponential complexity: \( O(n!) \) for brute force generation.

This is why the **recursive Stirling number relation** or **dynamic programming** is preferable for larger \( n \).

---

### Key Intuition

✅ Each element has multiple valid placements (existing subset or new subset).  
✅ Exploring all combinations inherently builds all valid partitions.  
✅ The recurrence relation mirrors this exact branching structure.  

## The k-Means Algorithm (Lloyd's algorithm)

The k-Means algorithm uses an **iterative refinement** approach. The steps are:

**Step 1:** **Initialize \( k \) cluster centroids.**  
- Common methods include:
  - **Random Initialization**: Randomly select \( k \) points as centroids.
  - **k-Means++ Initialization** (recommended): Selects initial centroids to maximize cluster separation.

**Step 2:** **Assign points to the nearest centroid.**  
- For each observation \( \mathbf{x}_i \), assign it to the cluster with the closest centroid:

\[
\text{Cluster}(\mathbf{x}_i) = \arg \min_{j} \|\mathbf{x}_i - \boldsymbol{\mu}_j\|^2
\]

**Step 3:** **Update the centroids.**  
- Recalculate each centroid as the mean of its assigned points:

\[
\boldsymbol{\mu}_j = \frac{1}{|C_j|} \sum_{\mathbf{x}_i \in C_j} \mathbf{x}_i
\]

**Step 4:** **Repeat Steps 2 and 3 until convergence.**  
- Convergence occurs when cluster assignments no longer change or centroids stabilize.

---

### Why does k-Means works? 

We need to show that each iteration of the **k-Means algorithm** decreases the **within-cluster sum of squares (WCSS)**, ensuring convergence.

The k-Means algorithm alternates between two steps:

1. **Cluster Assignment Step:** Assign each point to the closest centroid.  
2. **Centroid Update Step:** Recalculate the centroid as the mean of points in each cluster.

We'll prove that **each of these steps decreases the WCSS**.


#### Cluster Assignment Step Decreases WCSS

Suppose we are in the middle of the k-Means algorithm with centroids \( \{ \boldsymbol{\mu}_1, \dots, \boldsymbol{\mu}_k \} \).

- Each point \( \mathbf{x}_i \) is assigned to the nearest centroid.  
- For a point originally assigned to cluster \( C_j \), assume it is reassigned to \( C_\ell \).

Since the assignment rule is:

\[
\|\mathbf{x}_i - \boldsymbol{\mu}_\ell \|^2 \leq \|\mathbf{x}_i - \boldsymbol{\mu}_j \|^2
\]

the WCSS can only **decrease or remain the same**.

✅ **Conclusion:** The assignment step **never increases WCSS**.

---

#### Centroid Update Step Decreases WCSS

Next, consider the effect of updating the centroids.  

Suppose the updated centroid for cluster \( C_j \) is:

\[
\boldsymbol{\mu}_j = \frac{1}{|C_j|} \sum_{\mathbf{x}_i \in C_j} \mathbf{x}_i
\]

We want to show that this new centroid minimizes:

\[
\sum_{\mathbf{x}_i \in C_j} \|\mathbf{x}_i - \boldsymbol{\mu}_j\|^2
\]

From properties of the **sample mean**, the mean is the point that minimizes the sum of squared distances to all points in the cluster:

\[
\boldsymbol{\mu}_j = \arg\min_{\mathbf{y}} \sum_{\mathbf{x}_i \in C_j} \|\mathbf{x}_i - \mathbf{y} \|^2
\]

✅ **Conclusion:** The centroid update step **strictly decreases** WCSS unless the centroid is already optimal.

#### Combined Effect

- The **Cluster Assignment Step** decreases WCSS or leaves it unchanged.  
- The **Centroid Update Step** strictly decreases WCSS unless centroids are optimal.  

Since WCSS is **lower-bounded by zero**, the algorithm must eventually converge.

#### Why Does k-Means Converge?

- Each iteration reduces WCSS.  
- WCSS is **bounded** and **non-negative**.  
- Therefore, by the monotone convergence theorem, the algorithm must eventually reach a local minimum.  

❗️ **Important:** k-Means may converge to a **local minimum**, not necessarily the **global minimum** — this is why multiple initializations (like **k-Means++**) are recommended.

#### Summary

✅ Each iteration of k-Means reduces the WCSS.  
✅ Convergence is guaranteed, though the final solution may only be **locally optimal**.  
✅ Using improved initialization techniques like **k-Means++** helps achieve better results.
