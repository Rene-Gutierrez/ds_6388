<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Factor Analysis | DS 6388 Spring 2025</title>
  <meta name="description" content="6 Factor Analysis | DS 6388 Spring 2025" />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Factor Analysis | DS 6388 Spring 2025" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Factor Analysis | DS 6388 Spring 2025" />
  
  
  

<meta name="author" content="Rene Gutierrez University of Texas at El Paso" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="principal-component-analysis.html"/>
<link rel="next" href="canonical-correlation-analysis-cca.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> DS 6388: Multivariate Statistical Methods for High-dimensional Data</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#calendar"><i class="fa fa-check"></i><b>1.1</b> Calendar</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#importatnt-dates"><i class="fa fa-check"></i><b>1.1.1</b> Importatnt Dates</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#class-schedule"><i class="fa fa-check"></i><b>1.1.2</b> Class Schedule</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>2</b> Prerequisites</a>
<ul>
<li class="chapter" data-level="2.1" data-path="prerequisites.html"><a href="prerequisites.html#linear-algebra"><i class="fa fa-check"></i><b>2.1</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="prerequisites.html"><a href="prerequisites.html#linear-independence"><i class="fa fa-check"></i><b>2.1.1</b> Linear Independence</a></li>
<li class="chapter" data-level="2.1.2" data-path="prerequisites.html"><a href="prerequisites.html#column-space-of-a-matrix"><i class="fa fa-check"></i><b>2.1.2</b> Column Space of a Matrix</a></li>
<li class="chapter" data-level="2.1.3" data-path="prerequisites.html"><a href="prerequisites.html#rank-of-a-matrix"><i class="fa fa-check"></i><b>2.1.3</b> Rank of a Matrix</a></li>
<li class="chapter" data-level="2.1.4" data-path="prerequisites.html"><a href="prerequisites.html#full-rank-matrix"><i class="fa fa-check"></i><b>2.1.4</b> Full Rank Matrix</a></li>
<li class="chapter" data-level="2.1.5" data-path="prerequisites.html"><a href="prerequisites.html#inverse-matrix"><i class="fa fa-check"></i><b>2.1.5</b> Inverse Matrix</a></li>
<li class="chapter" data-level="2.1.6" data-path="prerequisites.html"><a href="prerequisites.html#positive-definite-matrix"><i class="fa fa-check"></i><b>2.1.6</b> Positive Definite Matrix</a></li>
<li class="chapter" data-level="2.1.7" data-path="prerequisites.html"><a href="prerequisites.html#singular-value-decomposition"><i class="fa fa-check"></i><b>2.1.7</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="2.1.8" data-path="prerequisites.html"><a href="prerequisites.html#eigendecomposition"><i class="fa fa-check"></i><b>2.1.8</b> Eigendecomposition</a></li>
<li class="chapter" data-level="2.1.9" data-path="prerequisites.html"><a href="prerequisites.html#idempotent-matrix"><i class="fa fa-check"></i><b>2.1.9</b> Idempotent Matrix</a></li>
<li class="chapter" data-level="2.1.10" data-path="prerequisites.html"><a href="prerequisites.html#determinant-of-a-matrix"><i class="fa fa-check"></i><b>2.1.10</b> Determinant of a Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="prerequisites.html"><a href="prerequisites.html#calculus"><i class="fa fa-check"></i><b>2.2</b> Calculus</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="prerequisites.html"><a href="prerequisites.html#gradient"><i class="fa fa-check"></i><b>2.2.1</b> Gradient</a></li>
<li class="chapter" data-level="2.2.2" data-path="prerequisites.html"><a href="prerequisites.html#hessian-matrix"><i class="fa fa-check"></i><b>2.2.2</b> Hessian Matrix</a></li>
<li class="chapter" data-level="2.2.3" data-path="prerequisites.html"><a href="prerequisites.html#applications-1"><i class="fa fa-check"></i><b>2.2.3</b> Applications:</a></li>
<li class="chapter" data-level="2.2.4" data-path="prerequisites.html"><a href="prerequisites.html#matrix-calculus"><i class="fa fa-check"></i><b>2.2.4</b> Matrix Calculus</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="prerequisites.html"><a href="prerequisites.html#probability"><i class="fa fa-check"></i><b>2.3</b> Probability</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="prerequisites.html"><a href="prerequisites.html#expected-value"><i class="fa fa-check"></i><b>2.3.1</b> Expected Value</a></li>
<li class="chapter" data-level="2.3.2" data-path="prerequisites.html"><a href="prerequisites.html#variance"><i class="fa fa-check"></i><b>2.3.2</b> Variance</a></li>
<li class="chapter" data-level="2.3.3" data-path="prerequisites.html"><a href="prerequisites.html#cross-covariance-matrix"><i class="fa fa-check"></i><b>2.3.3</b> Cross-Covariance Matrix</a></li>
<li class="chapter" data-level="2.3.4" data-path="prerequisites.html"><a href="prerequisites.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>2.3.4</b> Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="2.3.5" data-path="prerequisites.html"><a href="prerequisites.html#chi2-distribution"><i class="fa fa-check"></i><b>2.3.5</b> <span class="math inline">\(\chi^2\)</span> Distribution</a></li>
<li class="chapter" data-level="2.3.6" data-path="prerequisites.html"><a href="prerequisites.html#t-distribution"><i class="fa fa-check"></i><b>2.3.6</b> <span class="math inline">\(t\)</span> Distribution</a></li>
<li class="chapter" data-level="2.3.7" data-path="prerequisites.html"><a href="prerequisites.html#f-distribution"><i class="fa fa-check"></i><b>2.3.7</b> <span class="math inline">\(F\)</span> Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="prerequisites.html"><a href="prerequisites.html#statistics"><i class="fa fa-check"></i><b>2.4</b> Statistics</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="prerequisites.html"><a href="prerequisites.html#bias-of-an-estimator"><i class="fa fa-check"></i><b>2.4.1</b> Bias of an Estimator</a></li>
<li class="chapter" data-level="2.4.2" data-path="prerequisites.html"><a href="prerequisites.html#unbiased-estimator"><i class="fa fa-check"></i><b>2.4.2</b> Unbiased Estimator</a></li>
<li class="chapter" data-level="2.4.3" data-path="prerequisites.html"><a href="prerequisites.html#mean-square-error-of-an-estimator"><i class="fa fa-check"></i><b>2.4.3</b> Mean Square Error of an Estimator</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>3</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introduction.html"><a href="introduction.html#key-challenges"><i class="fa fa-check"></i><b>3.1</b> Key Challenges</a></li>
<li class="chapter" data-level="3.2" data-path="introduction.html"><a href="introduction.html#core-concepts"><i class="fa fa-check"></i><b>3.2</b> Core Concepts</a></li>
<li class="chapter" data-level="3.3" data-path="introduction.html"><a href="introduction.html#applications-4"><i class="fa fa-check"></i><b>3.3</b> Applications</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>4</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="linear-regression.html"><a href="linear-regression.html#machine-learning"><i class="fa fa-check"></i><b>4.1</b> Machine Learning</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="linear-regression.html"><a href="linear-regression.html#ordinary-least-squares"><i class="fa fa-check"></i><b>4.1.1</b> Ordinary Least Squares</a></li>
<li class="chapter" data-level="4.1.2" data-path="linear-regression.html"><a href="linear-regression.html#ridge-regression"><i class="fa fa-check"></i><b>4.1.2</b> Ridge Regression</a></li>
<li class="chapter" data-level="4.1.3" data-path="linear-regression.html"><a href="linear-regression.html#lasso-regression"><i class="fa fa-check"></i><b>4.1.3</b> Lasso Regression</a></li>
<li class="chapter" data-level="4.1.4" data-path="linear-regression.html"><a href="linear-regression.html#elastic-net"><i class="fa fa-check"></i><b>4.1.4</b> Elastic Net</a></li>
<li class="chapter" data-level="4.1.5" data-path="linear-regression.html"><a href="linear-regression.html#elastic-net-as-a-mixed-penalty"><i class="fa fa-check"></i><b>4.1.5</b> <strong>Elastic Net as a Mixed Penalty</strong></a></li>
<li class="chapter" data-level="4.1.6" data-path="linear-regression.html"><a href="linear-regression.html#other-options-of-regularization"><i class="fa fa-check"></i><b>4.1.6</b> Other Options of Regularization</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-linear-regression"><i class="fa fa-check"></i><b>4.2</b> Bayesian Linear Regression</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="linear-regression.html"><a href="linear-regression.html#basic-bayesian-linear-regression"><i class="fa fa-check"></i><b>4.2.1</b> Basic Bayesian Linear Regression</a></li>
<li class="chapter" data-level="4.2.2" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-lasso-regression"><i class="fa fa-check"></i><b>4.2.2</b> Bayesian Lasso Regression</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="linear-regression.html"><a href="linear-regression.html#computational-comparisson"><i class="fa fa-check"></i><b>4.3</b> Computational Comparisson</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="linear-regression.html"><a href="linear-regression.html#set-up"><i class="fa fa-check"></i><b>4.3.1</b> Set-Up</a></li>
<li class="chapter" data-level="4.3.2" data-path="linear-regression.html"><a href="linear-regression.html#simulation"><i class="fa fa-check"></i><b>4.3.2</b> Simulation</a></li>
<li class="chapter" data-level="4.3.3" data-path="linear-regression.html"><a href="linear-regression.html#ols"><i class="fa fa-check"></i><b>4.3.3</b> OLS</a></li>
<li class="chapter" data-level="4.3.4" data-path="linear-regression.html"><a href="linear-regression.html#ridge-regression-1"><i class="fa fa-check"></i><b>4.3.4</b> Ridge Regression</a></li>
<li class="chapter" data-level="4.3.5" data-path="linear-regression.html"><a href="linear-regression.html#lasso"><i class="fa fa-check"></i><b>4.3.5</b> Lasso</a></li>
<li class="chapter" data-level="4.3.6" data-path="linear-regression.html"><a href="linear-regression.html#basic-bayesian-regression"><i class="fa fa-check"></i><b>4.3.6</b> Basic Bayesian Regression</a></li>
<li class="chapter" data-level="4.3.7" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-lasso"><i class="fa fa-check"></i><b>4.3.7</b> Bayesian Lasso</a></li>
<li class="chapter" data-level="4.3.8" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-horseshoe-prior"><i class="fa fa-check"></i><b>4.3.8</b> Bayesian Horseshoe Prior</a></li>
<li class="chapter" data-level="4.3.9" data-path="linear-regression.html"><a href="linear-regression.html#results-comparisson"><i class="fa fa-check"></i><b>4.3.9</b> Results Comparisson</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="linear-regression.html"><a href="linear-regression.html#efficient-computation"><i class="fa fa-check"></i><b>4.4</b> Efficient Computation</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="linear-regression.html"><a href="linear-regression.html#efficient-computation-of-ridge-regression-using-the-woodbury-identity"><i class="fa fa-check"></i><b>4.4.1</b> Efficient Computation of Ridge Regression using the Woodbury Identity</a></li>
<li class="chapter" data-level="4.4.2" data-path="linear-regression.html"><a href="linear-regression.html#efficient-bayesian-sampling-for-gaussian-scale-mixture-priors"><i class="fa fa-check"></i><b>4.4.2</b> Efficient Bayesian Sampling for Gaussian Scale-Mixture Priors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>5</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="5.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#pca-as-a-low-rank-approximation-of-mathbfx"><i class="fa fa-check"></i><b>5.1</b> PCA as a Low-Rank Approximation of <span class="math inline">\(\mathbf{X}\)</span></a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#why-low-rank-approximation-matters"><i class="fa fa-check"></i><b>5.1.1</b> Why Low-Rank Approximation Matters</a></li>
<li class="chapter" data-level="5.1.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#the-singular-value-solution-is-the-best-low-rank-approximation-eckartyoungmirsky-theorem"><i class="fa fa-check"></i><b>5.1.2</b> The Singular Value Solution is the Best Low-Rank Approximation (Eckart–Young–Mirsky Theorem)</a></li>
<li class="chapter" data-level="5.1.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#eckartyoungmirsky-theorem-for-the-spectral-norm"><i class="fa fa-check"></i><b>5.1.3</b> Eckart–Young–Mirsky Theorem for the Spectral Norm</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#variance-maximization-in-pca"><i class="fa fa-check"></i><b>5.2</b> Variance Maximization in PCA</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#first-principal-cmponent"><i class="fa fa-check"></i><b>5.2.1</b> First Principal Cmponent</a></li>
<li class="chapter" data-level="5.2.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#second-principal-component"><i class="fa fa-check"></i><b>5.2.2</b> Second Principal Component</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#parafac-decomposition"><i class="fa fa-check"></i><b>5.3</b> PARAFAC decomposition</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#what-is-a-tensor"><i class="fa fa-check"></i><b>5.3.1</b> What is a Tensor?</a></li>
<li class="chapter" data-level="5.3.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#motivation-why-generalize-pca-to-tensors"><i class="fa fa-check"></i><b>5.3.2</b> Motivation: Why Generalize PCA to Tensors?</a></li>
<li class="chapter" data-level="5.3.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#parafac-decomposition-definition"><i class="fa fa-check"></i><b>5.3.3</b> PARAFAC Decomposition Definition</a></li>
<li class="chapter" data-level="5.3.4" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#how-is-parafac-related-to-pca"><i class="fa fa-check"></i><b>5.3.4</b> How is PARAFAC Related to PCA?</a></li>
<li class="chapter" data-level="5.3.5" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#key-properties-of-parafac-decomposition"><i class="fa fa-check"></i><b>5.3.5</b> Key Properties of PARAFAC Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#high-dimensional-pca"><i class="fa fa-check"></i><b>5.4</b> High-Dimensional PCA</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#high-dimensional-pca-p-gg-n"><i class="fa fa-check"></i><b>5.4.1</b> High-Dimensional PCA (<span class="math inline">\(p \gg n\)</span>)</a></li>
<li class="chapter" data-level="5.4.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#randomized-pca"><i class="fa fa-check"></i><b>5.4.2</b> Randomized PCA</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#principal-components-regression"><i class="fa fa-check"></i><b>5.5</b> Principal Components Regression</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#what-is-principal-components-regression"><i class="fa fa-check"></i><b>5.5.1</b> What is Principal Components Regression?</a></li>
<li class="chapter" data-level="5.5.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#why-use-pcr"><i class="fa fa-check"></i><b>5.5.2</b> Why Use PCR?</a></li>
<li class="chapter" data-level="5.5.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#mathematical-formulation"><i class="fa fa-check"></i><b>5.5.3</b> Mathematical Formulation</a></li>
<li class="chapter" data-level="5.5.4" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#choosing-the-number-of-components"><i class="fa fa-check"></i><b>5.5.4</b> Choosing the Number of Components</a></li>
<li class="chapter" data-level="5.5.5" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#advantages-of-pcr"><i class="fa fa-check"></i><b>5.5.5</b> Advantages of PCR</a></li>
<li class="chapter" data-level="5.5.6" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#disadvantages"><i class="fa fa-check"></i><b>5.5.6</b> Disadvantages</a></li>
<li class="chapter" data-level="5.5.7" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#randomized-pca-and-pcr"><i class="fa fa-check"></i><b>5.5.7</b> Randomized PCA and PCR</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="factor-analysis.html"><a href="factor-analysis.html"><i class="fa fa-check"></i><b>6</b> Factor Analysis</a>
<ul>
<li class="chapter" data-level="6.1" data-path="factor-analysis.html"><a href="factor-analysis.html#introduction-2"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="factor-analysis.html"><a href="factor-analysis.html#how-factor-analysis-works"><i class="fa fa-check"></i><b>6.1.1</b> How Factor Analysis Works</a></li>
<li class="chapter" data-level="6.1.2" data-path="factor-analysis.html"><a href="factor-analysis.html#why-is-factor-analysis-useful"><i class="fa fa-check"></i><b>6.1.2</b> Why is Factor Analysis Useful?</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="factor-analysis.html"><a href="factor-analysis.html#factor-analysis-model"><i class="fa fa-check"></i><b>6.2</b> Factor Analysis Model</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="factor-analysis.html"><a href="factor-analysis.html#model-specification-1"><i class="fa fa-check"></i><b>6.2.1</b> Model Specification</a></li>
<li class="chapter" data-level="6.2.2" data-path="factor-analysis.html"><a href="factor-analysis.html#assumptions"><i class="fa fa-check"></i><b>6.2.2</b> Assumptions</a></li>
<li class="chapter" data-level="6.2.3" data-path="factor-analysis.html"><a href="factor-analysis.html#variance-covariance-matrix"><i class="fa fa-check"></i><b>6.2.3</b> Variance-Covariance Matrix</a></li>
<li class="chapter" data-level="6.2.4" data-path="factor-analysis.html"><a href="factor-analysis.html#implications"><i class="fa fa-check"></i><b>6.2.4</b> Implications</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="factor-analysis.html"><a href="factor-analysis.html#estimation-in-factor-analysis"><i class="fa fa-check"></i><b>6.3</b> Estimation in Factor Analysis</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="factor-analysis.html"><a href="factor-analysis.html#principal-components-method"><i class="fa fa-check"></i><b>6.3.1</b> Principal Components Method</a></li>
<li class="chapter" data-level="6.3.2" data-path="factor-analysis.html"><a href="factor-analysis.html#principal-factor-method"><i class="fa fa-check"></i><b>6.3.2</b> Principal Factor Method</a></li>
<li class="chapter" data-level="6.3.3" data-path="factor-analysis.html"><a href="factor-analysis.html#maximum-likelihood-estimation-mle"><i class="fa fa-check"></i><b>6.3.3</b> Maximum Likelihood Estimation (MLE)</a></li>
<li class="chapter" data-level="6.3.4" data-path="factor-analysis.html"><a href="factor-analysis.html#bayesian-estimation-hierarchical-priors"><i class="fa fa-check"></i><b>6.3.4</b> Bayesian Estimation (Hierarchical Priors)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="factor-analysis.html"><a href="factor-analysis.html#estimating-factor-scores-in-factor-analysis"><i class="fa fa-check"></i><b>6.4</b> Estimating Factor Scores in Factor Analysis</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="factor-analysis.html"><a href="factor-analysis.html#ordinary-least-squares-ols-method"><i class="fa fa-check"></i><b>6.4.1</b> Ordinary Least Squares (OLS) Method</a></li>
<li class="chapter" data-level="6.4.2" data-path="factor-analysis.html"><a href="factor-analysis.html#weighted-least-squares-wls-method-bartletts-method"><i class="fa fa-check"></i><b>6.4.2</b> Weighted Least Squares (WLS) Method (Bartlett’s Method)</a></li>
<li class="chapter" data-level="6.4.3" data-path="factor-analysis.html"><a href="factor-analysis.html#regression-method-thompsons-estimator"><i class="fa fa-check"></i><b>6.4.3</b> Regression Method (Thompson’s Estimator)</a></li>
<li class="chapter" data-level="6.4.4" data-path="factor-analysis.html"><a href="factor-analysis.html#summary-of-factor-score-estimators"><i class="fa fa-check"></i><b>6.4.4</b> Summary of Factor Score Estimators</a></li>
<li class="chapter" data-level="6.4.5" data-path="factor-analysis.html"><a href="factor-analysis.html#key-takeaways"><i class="fa fa-check"></i><b>6.4.5</b> Key Takeaways</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="factor-analysis.html"><a href="factor-analysis.html#factor-analysis-example"><i class="fa fa-check"></i><b>6.5</b> Factor Analysis Example</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="factor-analysis.html"><a href="factor-analysis.html#load-required-libraries"><i class="fa fa-check"></i><b>6.5.1</b> Load Required Libraries</a></li>
<li class="chapter" data-level="6.5.2" data-path="factor-analysis.html"><a href="factor-analysis.html#generate-sample-data"><i class="fa fa-check"></i><b>6.5.2</b> Generate Sample Data</a></li>
<li class="chapter" data-level="6.5.3" data-path="factor-analysis.html"><a href="factor-analysis.html#estimation-of-factor-loadings-and-unique-variances"><i class="fa fa-check"></i><b>6.5.3</b> Estimation of Factor Loadings and Unique Variances</a></li>
<li class="chapter" data-level="6.5.4" data-path="factor-analysis.html"><a href="factor-analysis.html#estimate-factor-scores"><i class="fa fa-check"></i><b>6.5.4</b> Estimate Factor Scores</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html"><i class="fa fa-check"></i><b>7</b> Canonical Correlation Analysis (CCA)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#motivation-1"><i class="fa fa-check"></i><b>7.1</b> Motivation</a></li>
<li class="chapter" data-level="7.2" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#mathematical-formulation-1"><i class="fa fa-check"></i><b>7.2</b> Mathematical Formulation</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#key-properties-6"><i class="fa fa-check"></i><b>7.2.1</b> Key Properties</a></li>
<li class="chapter" data-level="7.2.2" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#implementation-in-r"><i class="fa fa-check"></i><b>7.2.2</b> Implementation in R</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#canonical-directions-estimation"><i class="fa fa-check"></i><b>7.3</b> Canonical Directions Estimation</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#problem-definition-with-random-variables"><i class="fa fa-check"></i><b>7.3.1</b> Problem Definition with Random Variables</a></li>
<li class="chapter" data-level="7.3.2" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#canonical-correlation-maximization-problem"><i class="fa fa-check"></i><b>7.3.2</b> Canonical Correlation Maximization Problem</a></li>
<li class="chapter" data-level="7.3.3" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#solving-for-the-canonical-directions-using-lagrange-multipliers"><i class="fa fa-check"></i><b>7.3.3</b> Solving for the Canonical Directions Using Lagrange Multipliers</a></li>
<li class="chapter" data-level="7.3.4" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#showing-that-the-matrices-have-the-same-eigenvalues"><i class="fa fa-check"></i><b>7.3.4</b> Showing That the Matrices Have the Same Eigenvalues</a></li>
<li class="chapter" data-level="7.3.5" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#showing-that-the-largest-eigenvalue-maximizes-the-objective-function"><i class="fa fa-check"></i><b>7.3.5</b> Showing That the Largest Eigenvalue Maximizes the Objective Function</a></li>
<li class="chapter" data-level="7.3.6" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#conclusion-3"><i class="fa fa-check"></i><b>7.3.6</b> Conclusion</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">DS 6388 Spring 2025</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="factor-analysis" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">6</span> Factor Analysis<a href="factor-analysis.html#factor-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Factor analysis is a <strong>statistical method</strong> used to explain the relationships among observed variables by identifying a smaller number of <strong>latent factors</strong> that account for the observed patterns.</p>
<hr />
<div id="introduction-2" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Introduction<a href="factor-analysis.html#introduction-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Imagine you are a psychologist studying <strong>personality traits</strong>. You conduct a survey where people rate themselves on multiple characteristics like:</p>
<ul>
<li><strong>Talkative</strong><br />
</li>
<li><strong>Outgoing</strong><br />
</li>
<li><strong>Shy</strong><br />
</li>
<li><strong>Prefers being alone</strong></li>
</ul>
<p>At first glance, these seem like <strong>four separate traits</strong>, but you might suspect that they all relate to an <strong>underlying factor</strong>—something like <strong>“Extroversion”</strong>.</p>
<p>Factor Analysis (FA) helps us uncover these <strong>hidden patterns</strong> in data. Instead of treating every survey question as completely independent, FA looks for <strong>common factors</strong> that explain the relationships between them.</p>
<hr />
<div id="how-factor-analysis-works" class="section level3 hasAnchor" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> How Factor Analysis Works<a href="factor-analysis.html#how-factor-analysis-works" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li><strong>You Start with a Big Dataset</strong>
<ul>
<li>Let’s say you have <strong>survey responses from 500 people</strong> on 20 personality traits.<br />
</li>
<li>The goal is to simplify this by identifying a <strong>few core personality factors</strong> that drive the responses.</li>
</ul></li>
<li><strong>FA Looks for Hidden Patterns</strong>
<ul>
<li>Instead of analyzing 20 separate traits, FA groups them into a smaller set of <strong>factors</strong>.<br />
</li>
<li>For example, traits like “Talkative” and “Outgoing” might load onto a factor called <strong>“Extraversion”</strong>.</li>
</ul></li>
<li><strong>Each Trait Has a “Loading”</strong>
<ul>
<li>FA calculates how strongly each trait relates to a factor.<br />
</li>
<li>“Talkative” might have a <strong>high loading</strong> on Extraversion, while “Prefers being alone” has a <strong>negative loading</strong>.</li>
</ul></li>
<li><strong>You End Up with Fewer Factors</strong>
<ul>
<li>Instead of working with 20 different traits, FA might show that <strong>only 3 or 4 key personality factors</strong> explain most of the variation in responses.</li>
</ul></li>
</ol>
<hr />
</div>
<div id="why-is-factor-analysis-useful" class="section level3 hasAnchor" number="6.1.2">
<h3><span class="header-section-number">6.1.2</span> Why is Factor Analysis Useful?<a href="factor-analysis.html#why-is-factor-analysis-useful" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><strong>Psychology</strong>: Identifying core personality traits (e.g., the “Big Five” personality factors).<br />
</li>
<li><strong>Marketing</strong>: Understanding customer preferences (e.g., grouping product features into themes).<br />
</li>
<li><strong>Finance</strong>: Analyzing stock market trends (e.g., finding common trends among different stocks).</li>
</ul>
<p>Factor Analysis is a <strong>powerful tool</strong> for simplifying complex datasets and uncovering <strong>hidden relationships</strong> between variables.</p>
<hr />
</div>
</div>
<div id="factor-analysis-model" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Factor Analysis Model<a href="factor-analysis.html#factor-analysis-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="model-specification-1" class="section level3 hasAnchor" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> Model Specification<a href="factor-analysis.html#model-specification-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We assume that each observed variable <span class="math inline">\(\mathbf{x}\)</span> follows a <strong>Factor Analysis (FA) model</strong>:</p>
<p><span class="math display">\[
\mathbf{x} = \boldsymbol{\mu} + \mathbf{\Lambda} \mathbf{f} + \mathbf{e}
\]</span></p>
<p>where:<br />
- <span class="math inline">\(\mathbf{x} \in \mathbb{R}^{p}\)</span> is the <strong>observed variable</strong> (with mean <span class="math inline">\(\boldsymbol{\mu}\)</span>).<br />
- <span class="math inline">\(\boldsymbol{\mu} \in \mathbb{R}^{p}\)</span> is the <strong>mean vector</strong> of the observed variables.<br />
- <span class="math inline">\(\mathbf{\Lambda} \in \mathbb{R}^{p \times k}\)</span> is the <strong>factor loadings matrix</strong>, mapping <span class="math inline">\(k\)</span> latent factors to <span class="math inline">\(p\)</span> observed variables.<br />
- <span class="math inline">\(\mathbf{f} \in \mathbb{R}^{k}\)</span> is the <strong>latent factor vector</strong>, capturing the shared variation among the observed variables.<br />
- <span class="math inline">\(\mathbf{e} \in \mathbb{R}^{p}\)</span> is the <strong>unique error term</strong>, representing variation in <span class="math inline">\(\mathbf{x}\)</span> that is not explained by the factors.</p>
<hr />
</div>
<div id="assumptions" class="section level3 hasAnchor" number="6.2.2">
<h3><span class="header-section-number">6.2.2</span> Assumptions<a href="factor-analysis.html#assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li><strong>Latent Factors</strong>
<ul>
<li><span class="math inline">\(\mathbb{E}[\mathbf{f}] = 0\)</span> (factors have zero mean).<br />
</li>
<li><span class="math inline">\(\mathbb{C}[\mathbf{f}] = \mathbf{I}_k\)</span> (factors are uncorrelated and have unit variance).</li>
</ul></li>
<li><strong>Error Terms</strong>
<ul>
<li><span class="math inline">\(\mathbb{E}[\mathbf{e}] = 0\)</span> (errors have zero mean).<br />
</li>
<li><span class="math inline">\(\mathbb{C}[\mathbf{e}] = \mathbf{\Psi}\)</span>, where <span class="math inline">\(\mathbf{\Psi}\)</span> is a <strong>diagonal matrix</strong> (each error term has its own variance, and errors are uncorrelated across variables).</li>
</ul></li>
<li><strong>Independence</strong>
<ul>
<li>The latent factors and errors are <strong>independent</strong>:<br />
<span class="math display">\[
\mathbb{C}[\mathbf{f}, \mathbf{e}] = 0.
\]</span></li>
</ul></li>
</ol>
<hr />
</div>
<div id="variance-covariance-matrix" class="section level3 hasAnchor" number="6.2.3">
<h3><span class="header-section-number">6.2.3</span> Variance-Covariance Matrix<a href="factor-analysis.html#variance-covariance-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Taking expectations, we get:</p>
<p><span class="math display">\[
\mathbb{E}[\mathbf{x}] = \boldsymbol{\mu}.
\]</span></p>
<p>The <strong>variance-covariance matrix</strong> of <span class="math inline">\(\mathbf{x}\)</span> is:</p>
<p><span class="math display">\[
\mathbb{C}[\mathbf{x}] = \mathbb{C}[\mathbf{\Lambda} \mathbf{f} + \mathbf{e}].
\]</span></p>
<p>Expanding using linearity of covariance:</p>
<p><span class="math display">\[
\mathbb{C}[\mathbf{x}] = \mathbb{C}[\mathbf{\Lambda} \mathbf{f}] + \mathbb{C}[\mathbf{e}].
\]</span></p>
<p>Since <span class="math inline">\(\mathbf{f}\)</span> has covariance <span class="math inline">\(\mathbf{I}_k\)</span>, we get:</p>
<p><span class="math display">\[
\mathbb{C}[\mathbf{x}] = \mathbf{\Lambda} \mathbb{C}[\mathbf{f}] \mathbf{\Lambda}&#39; + \mathbf{\Psi}.
\]</span></p>
<p>Substituting <span class="math inline">\(\mathbb{C}[\mathbf{f}] = \mathbf{I}_k\)</span>:</p>
<p><span class="math display">\[
\mathbb{C}[\mathbf{x}] = \mathbf{\Lambda} \mathbf{\Lambda}&#39; + \mathbf{\Psi}.
\]</span></p>
<p>This means:
- <span class="math inline">\(\mathbf{\Lambda} \mathbf{\Lambda}&#39;\)</span> represents the <strong>shared variance</strong> explained by the factors.
- <span class="math inline">\(\mathbf{\Psi}\)</span> represents the <strong>unique variance</strong> that is specific to each observed variable.</p>
<hr />
</div>
<div id="implications" class="section level3 hasAnchor" number="6.2.4">
<h3><span class="header-section-number">6.2.4</span> Implications<a href="factor-analysis.html#implications" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li><strong>Factor Analysis models the covariance, not the data itself</strong>
<ul>
<li>Unlike PCA, which focuses on total variance, FA explicitly separates <strong>shared variance</strong> (factors) from <strong>unique variance</strong> (errors).</li>
</ul></li>
<li><strong>Unique variances appear on the diagonal of <span class="math inline">\(\mathbb{C}[\mathbf{x}]\)</span></strong>
<ul>
<li>This explains why FA is useful when trying to <strong>model relationships</strong> between variables rather than just reducing dimensions.</li>
</ul></li>
<li><strong>The presence of <span class="math inline">\(\boldsymbol{\mu}\)</span></strong>
<ul>
<li>If the data is <strong>not mean-centered</strong>, the first step in estimation is usually to subtract <span class="math inline">\(\boldsymbol{\mu}\)</span>, so that the analysis focuses only on variance structure.</li>
</ul></li>
</ol>
</div>
</div>
<div id="estimation-in-factor-analysis" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Estimation in Factor Analysis<a href="factor-analysis.html#estimation-in-factor-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <strong>Factor Analysis (FA) model</strong> is given by:</p>
<p><span class="math display">\[
\mathbf{x} = \boldsymbol{\mu} + \mathbf{\Lambda} \mathbf{f} + \mathbf{e}
\]</span></p>
<p>with the <strong>variance-covariance structure</strong>:</p>
<p><span class="math display">\[
\mathbb{C}[\mathbf{x}] = \mathbf{\Lambda} \mathbf{\Lambda}&#39; + \mathbf{\Psi}.
\]</span></p>
<p>Our goal is to <strong>estimate <span class="math inline">\(\mathbf{\Lambda}\)</span> (factor loadings) and <span class="math inline">\(\mathbf{\Psi}\)</span> (unique variances)</strong>.</p>
<hr />
<div id="principal-components-method" class="section level3 hasAnchor" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Principal Components Method<a href="factor-analysis.html#principal-components-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This method estimates <span class="math inline">\(\mathbf{\Lambda}\)</span> by performing <strong>Principal Component Analysis (PCA)</strong> on the sample covariance matrix and approximating the factors using the leading components.</p>
<div id="step-1-compute-the-sample-covariance-matrix" class="section level4 hasAnchor" number="6.3.1.1">
<h4><span class="header-section-number">6.3.1.1</span> Step 1: Compute the Sample Covariance Matrix<a href="factor-analysis.html#step-1-compute-the-sample-covariance-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Given <span class="math inline">\(n\)</span> observations of <span class="math inline">\(p\)</span> variables in the dataset <span class="math inline">\(\mathbf{X}\)</span>, first compute the <strong>sample covariance matrix</strong>:</p>
<p><span class="math display">\[
\hat{\mathbb{C}}[\mathbf{x}] = \frac{1}{n-1} (\mathbf{X} - \bar{\mathbf{x}}  {\boldsymbol 1} ) (\mathbf{X} - \bar{\mathbf{x}} {\boldsymbol 1} )&#39;.
\]</span></p>
</div>
<div id="step-2-perform-eigen-decomposition" class="section level4 hasAnchor" number="6.3.1.2">
<h4><span class="header-section-number">6.3.1.2</span> Step 2: Perform Eigen-Decomposition<a href="factor-analysis.html#step-2-perform-eigen-decomposition" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Since the <strong>Factor Analysis (FA) model</strong> assumes:</p>
<p><span class="math display">\[
\mathbb{C}[\mathbf{x}] = \mathbf{\Lambda} \mathbf{\Lambda}&#39; + \mathbf{\Psi},
\]</span></p>
<p>we initially approximate <span class="math inline">\(\mathbf{\Psi} \approx 0\)</span>, so that:</p>
<p><span class="math display">\[
\mathbb{C}[\mathbf{x}] \approx \mathbf{\Lambda} \mathbf{\Lambda}&#39;.
\]</span></p>
<p>Perform an <strong>eigenvalue decomposition</strong>:</p>
<p><span class="math display">\[
\hat{\mathbb{C}}[\mathbf{x}] = \mathbf{V} \mathbf{D} \mathbf{V}&#39;,
\]</span></p>
<p>where:
- <span class="math inline">\(\mathbf{V}\)</span> contains the <strong>eigenvectors</strong> (principal directions).
- <span class="math inline">\(\mathbf{D}\)</span> is a diagonal matrix of <strong>eigenvalues</strong>.</p>
</div>
<div id="step-3-select-the-first-k-components" class="section level4 hasAnchor" number="6.3.1.3">
<h4><span class="header-section-number">6.3.1.3</span> Step 3: Select the First <span class="math inline">\(k\)</span> Components<a href="factor-analysis.html#step-3-select-the-first-k-components" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Since we assume a <strong>low-rank factor model</strong> with <span class="math inline">\(k\)</span> factors, we take only the <strong>first <span class="math inline">\(k\)</span> largest eigenvalues</strong> and their corresponding eigenvectors:</p>
<p><span class="math display">\[
\mathbf{V}_k = [\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k].
\]</span></p>
<p>The corresponding <strong>diagonal eigenvalue matrix</strong>:</p>
<p><span class="math display">\[
\mathbf{D}_k = \text{diag}(\lambda_1, \lambda_2, \dots, \lambda_k).
\]</span></p>
</div>
<div id="step-4-compute-the-factor-loadings-mathbflambda" class="section level4 hasAnchor" number="6.3.1.4">
<h4><span class="header-section-number">6.3.1.4</span> Step 4: Compute the Factor Loadings <span class="math inline">\(\mathbf{\Lambda}\)</span><a href="factor-analysis.html#step-4-compute-the-factor-loadings-mathbflambda" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The estimated <strong>factor loadings matrix</strong> is:</p>
<p><span class="math display">\[
\hat{\mathbf{\Lambda}} = \mathbf{V}_k \mathbf{D}_k^{1/2}.
\]</span></p>
</div>
<div id="step-5-estimate-mathbfpsi" class="section level4 hasAnchor" number="6.3.1.5">
<h4><span class="header-section-number">6.3.1.5</span> Step 5: Estimate <span class="math inline">\(\mathbf{\Psi}\)</span><a href="factor-analysis.html#step-5-estimate-mathbfpsi" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To refine the model, we estimate <span class="math inline">\(\mathbf{\Psi}\)</span> as the difference between the <strong>diagonal of the sample covariance matrix</strong> and the diagonal elements of <span class="math inline">\(\mathbf{\Lambda} \mathbf{\Lambda}&#39;\)</span>:</p>
<p><span class="math display">\[
\hat{\mathbf{\Psi}} = \text{diag}(\hat{\mathbb{C}}[\mathbf{x}]) - \text{diag}(\hat{\mathbf{\Lambda}} \hat{\mathbf{\Lambda}}&#39;).
\]</span></p>
<hr />
</div>
</div>
<div id="principal-factor-method" class="section level3 hasAnchor" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> Principal Factor Method<a href="factor-analysis.html#principal-factor-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This method improves upon the <strong>Principal Components Method</strong> by explicitly estimating <span class="math inline">\(\mathbf{\Psi}\)</span> before extracting factors.</p>
<div id="step-1-compute-the-sample-covariance-matrix-1" class="section level4 hasAnchor" number="6.3.2.1">
<h4><span class="header-section-number">6.3.2.1</span> Step 1: Compute the Sample Covariance Matrix<a href="factor-analysis.html#step-1-compute-the-sample-covariance-matrix-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>As before, compute:</p>
<p><span class="math display">\[
\hat{\mathbb{C}}[\mathbf{x}] = \frac{1}{n-1} (\mathbf{X} - \bar{\mathbf{X}})&#39; (\mathbf{X} - \bar{\mathbf{X}}).
\]</span></p>
</div>
<div id="step-2-estimate-mathbfpsi-first" class="section level4 hasAnchor" number="6.3.2.2">
<h4><span class="header-section-number">6.3.2.2</span> Step 2: Estimate <span class="math inline">\(\mathbf{\Psi}\)</span> First<a href="factor-analysis.html#step-2-estimate-mathbfpsi-first" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Instead of assuming <span class="math inline">\(\mathbf{\Psi} \approx 0\)</span>, we estimate it by taking only the diagonal elements of <span class="math inline">\(\hat{\mathbb{C}}[\mathbf{x}]\)</span>:</p>
<p><span class="math display">\[
\hat{\mathbf{\Psi}} = \text{diag}(\hat{\mathbb{C}}[\mathbf{x}]).
\]</span></p>
<p>Then, compute the <strong>reduced correlation matrix</strong>:</p>
<p><span class="math display">\[
\mathbf{R} = \hat{\mathbb{C}}[\mathbf{x}] - \hat{\mathbf{\Psi}}.
\]</span></p>
</div>
<div id="step-3-perform-eigen-decomposition" class="section level4 hasAnchor" number="6.3.2.3">
<h4><span class="header-section-number">6.3.2.3</span> Step 3: Perform Eigen Decomposition<a href="factor-analysis.html#step-3-perform-eigen-decomposition" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We now compute the <strong>eigenvalue decomposition</strong> of <span class="math inline">\(\mathbf{R}\)</span>:</p>
<p><span class="math display">\[
\mathbf{R} = \mathbf{V} \mathbf{D} \mathbf{V}&#39;.
\]</span></p>
<p>The first <span class="math inline">\(k\)</span> <strong>eigenvectors</strong> of <span class="math inline">\(\mathbf{V}\)</span> correspond to the estimated <strong>factor loadings</strong>:</p>
<p><span class="math display">\[
\hat{\mathbf{\Lambda}} = \mathbf{V}_k \mathbf{D}_k^{1/2}.
\]</span></p>
</div>
<div id="step-4-refine-mathbfpsi" class="section level4 hasAnchor" number="6.3.2.4">
<h4><span class="header-section-number">6.3.2.4</span> Step 4: Refine <span class="math inline">\(\mathbf{\Psi}\)</span><a href="factor-analysis.html#step-4-refine-mathbfpsi" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To refine the model, we estimate <span class="math inline">\(\mathbf{\Psi}\)</span> as the difference between the <strong>diagonal of the sample covariance matrix</strong> and the diagonal elements of <span class="math inline">\(\mathbf{\Lambda} \mathbf{\Lambda}&#39;\)</span>:</p>
<p><span class="math display">\[
\hat{\mathbf{\Psi}} = \text{diag}(\hat{\mathbb{C}}[\mathbf{x}]) - \text{diag}(\hat{\mathbf{\Lambda}} \hat{\mathbf{\Lambda}}&#39;).
\]</span></p>
<hr />
</div>
</div>
<div id="maximum-likelihood-estimation-mle" class="section level3 hasAnchor" number="6.3.3">
<h3><span class="header-section-number">6.3.3</span> Maximum Likelihood Estimation (MLE)<a href="factor-analysis.html#maximum-likelihood-estimation-mle" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Instead of relying on the sample covariance, MLE finds <span class="math inline">\(\mathbf{\Lambda}\)</span> and <span class="math inline">\(\mathbf{\Psi}\)</span> by <strong>maximizing the likelihood</strong>:</p>
<p><span class="math display">\[
L(\mathbf{\Lambda}, \mathbf{\Psi}) = -\frac{n}{2} \left[ \log |\mathbf{\Lambda} \mathbf{\Lambda}&#39; + \mathbf{\Psi}| + \text{tr}(\hat{\mathbb{C}}[\mathbf{x}] (\mathbf{\Lambda} \mathbf{\Lambda}&#39; + \mathbf{\Psi})^{-1}) \right].
\]</span></p>
<p>The MLE approach:<br />
- Iteratively updates <span class="math inline">\(\mathbf{\Lambda}\)</span> and <span class="math inline">\(\mathbf{\Psi}\)</span>,<br />
- Ensures <strong>optimal factor separation</strong>,<br />
- Can be computed using <strong>Expectation-Maximization (EM) algorithms</strong>.</p>
<hr />
</div>
<div id="bayesian-estimation-hierarchical-priors" class="section level3 hasAnchor" number="6.3.4">
<h3><span class="header-section-number">6.3.4</span> Bayesian Estimation (Hierarchical Priors)<a href="factor-analysis.html#bayesian-estimation-hierarchical-priors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Bayesian Factor Analysis, we place <strong>priors</strong> on <span class="math inline">\(\mathbf{\Lambda}\)</span> and <span class="math inline">\(\mathbf{\Psi}\)</span>:</p>
<p><span class="math display">\[
p(\mathbf{\Lambda}) \sim \mathcal{N}(0, \tau^2),
\]</span></p>
<p><span class="math display">\[
p(\mathbf{\Psi}) \sim \text{Inverse-Gamma}(\alpha, \beta).
\]</span></p>
<p>Using <strong>MCMC sampling</strong>, we estimate posterior distributions for <span class="math inline">\(\mathbf{\Lambda}\)</span> and <span class="math inline">\(\mathbf{\Psi}\)</span>, providing <strong>better regularization</strong> when data is noisy.</p>
<hr />
<div id="summary-1" class="section level4 hasAnchor" number="6.3.4.1">
<h4><span class="header-section-number">6.3.4.1</span> Summary<a href="factor-analysis.html#summary-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li><strong>Principal Components Method:</strong> A quick approximation using PCA.<br />
</li>
<li><strong>Principal Factor Method:</strong> Improves upon PCA by <strong>subtracting <span class="math inline">\(\mathbf{\Psi}\)</span> before factor extraction</strong>.<br />
</li>
<li><strong>Maximum Likelihood Estimation:</strong> Statistically optimal, found via optimization.<br />
</li>
<li><strong>Bayesian Methods:</strong> Regularized approach when priors are available.</li>
</ol>
<hr />
</div>
</div>
</div>
<div id="estimating-factor-scores-in-factor-analysis" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Estimating Factor Scores in Factor Analysis<a href="factor-analysis.html#estimating-factor-scores-in-factor-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Once we have estimated the <strong>factor loadings</strong> <span class="math inline">\(\mathbf{\Lambda}\)</span> and the <strong>unique variances</strong> <span class="math inline">\(\mathbf{\Psi}\)</span>, we can compute the <strong>factor scores</strong>, which represent the estimated values of the latent factors for each observation.</p>
<p>Since the factors <span class="math inline">\(\mathbf{f}_i\)</span> are unobserved, we estimate them from the observed data <span class="math inline">\(\mathbf{x}_i\)</span>.</p>
<p>The factors themselves can be estimated using different methods.</p>
<div id="ordinary-least-squares-ols-method" class="section level3 hasAnchor" number="6.4.1">
<h3><span class="header-section-number">6.4.1</span> Ordinary Least Squares (OLS) Method<a href="factor-analysis.html#ordinary-least-squares-ols-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>OLS method</strong> estimates factor scores by minimizing the sum of squared residuals between the observed variables and their representation through the factor model. The objective is:</p>
<p><span class="math display">\[
\min_{\mathbf{f}_i} \|\mathbf{x}_i - \boldsymbol{\mu} - \mathbf{\Lambda} \mathbf{f}_i \|^2.
\]</span></p>
<p>Solving for <span class="math inline">\(\mathbf{f}_i\)</span>, the <strong>OLS estimator</strong> for the factor scores is:</p>
<p><span class="math display">\[
\hat{\mathbf{f}}_i = (\mathbf{\Lambda}&#39;\mathbf{\Lambda})^{-1} \mathbf{\Lambda}&#39; (\mathbf{x}_i - \boldsymbol{\mu}).
\]</span></p>
<p>In practice, we use the estimated factor loadings <span class="math inline">\(\hat{\mathbf{\Lambda}}\)</span> and the sample mean <span class="math inline">\(\bar{\mathbf{x}}\)</span>:</p>
<p><span class="math display">\[
\hat{\mathbf{f}}_i = (\hat{\mathbf{\Lambda}}&#39; \hat{\mathbf{\Lambda}})^{-1} \hat{\mathbf{\Lambda}}&#39; (\mathbf{x}_i - \bar{\mathbf{x}}).
\]</span></p>
<p>This method treats the estimation as a regression problem, solving for the factor scores that best reproduce the observed data.</p>
<hr />
</div>
<div id="weighted-least-squares-wls-method-bartletts-method" class="section level3 hasAnchor" number="6.4.2">
<h3><span class="header-section-number">6.4.2</span> Weighted Least Squares (WLS) Method (Bartlett’s Method)<a href="factor-analysis.html#weighted-least-squares-wls-method-bartletts-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>WLS method</strong>, also known as <strong>Bartlett’s estimator</strong>, adjusts the OLS approach by weighting the residuals by the inverse of their specific variances. This gives more weight to variables with <strong>lower unique variances</strong>, reflecting greater confidence in those measurements.</p>
<p>The objective function is:</p>
<p><span class="math display">\[
\min_{\mathbf{f}_i} (\mathbf{x}_i - \boldsymbol{\mu} - \mathbf{\Lambda} \mathbf{f}_i)&#39; \mathbf{\Psi}^{-1} (\mathbf{x}_i - \boldsymbol{\mu} - \mathbf{\Lambda} \mathbf{f}_i).
\]</span></p>
<p>Solving for <span class="math inline">\(\mathbf{f}_i\)</span>, the <strong>WLS estimator</strong> is:</p>
<p><span class="math display">\[
\hat{\mathbf{f}}_i = (\mathbf{\Lambda}&#39; \mathbf{\Psi}^{-1} \mathbf{\Lambda})^{-1} \mathbf{\Lambda}&#39; \mathbf{\Psi}^{-1} (\mathbf{x}_i - \boldsymbol{\mu}).
\]</span></p>
<p>Using the estimated parameters, we have:</p>
<p><span class="math display">\[
\hat{\mathbf{f}}_i = (\hat{\mathbf{\Lambda}}&#39; \hat{\mathbf{\Psi}}^{-1} \hat{\mathbf{\Lambda}})^{-1} \hat{\mathbf{\Lambda}}&#39; \hat{\mathbf{\Psi}}^{-1} (\mathbf{x}_i - \hat{\boldsymbol{\mu}}).
\]</span></p>
<p>This method emphasizes variables with <strong>higher reliability</strong> (lower unique variances) when estimating the factor scores.</p>
<hr />
</div>
<div id="regression-method-thompsons-estimator" class="section level3 hasAnchor" number="6.4.3">
<h3><span class="header-section-number">6.4.3</span> Regression Method (Thompson’s Estimator)<a href="factor-analysis.html#regression-method-thompsons-estimator" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>Regression Method</strong>, also known as <strong>Thompson’s estimator</strong>, derives factor scores by considering the <strong>conditional expectation of the factors given the observed data</strong>. To obtain this result, we first derive the <strong>joint distribution of the observed variables <span class="math inline">\(\mathbf{x}_i\)</span> and the factors <span class="math inline">\(\mathbf{f}_i\)</span></strong>.</p>
<hr />
<div id="joint-distribution-of-mathbfx_i-and-mathbff_i" class="section level4 hasAnchor" number="6.4.3.1">
<h4><span class="header-section-number">6.4.3.1</span> Joint Distribution of <span class="math inline">\(\mathbf{x}_i\)</span> and <span class="math inline">\(\mathbf{f}_i\)</span><a href="factor-analysis.html#joint-distribution-of-mathbfx_i-and-mathbff_i" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We assume that both the <strong>observed data</strong> <span class="math inline">\(\mathbf{x}_i\)</span> and the <strong>latent factors</strong> <span class="math inline">\(\mathbf{f}_i\)</span> follow a <strong>multivariate normal distribution</strong>:</p>
<p><span class="math display">\[
\begin{bmatrix}
\mathbf{x}_i \\
\mathbf{f}_i
\end{bmatrix}
\sim \mathcal{N} \left(
\begin{bmatrix}
\boldsymbol{\mu} \\
\mathbf{0}
\end{bmatrix},
\begin{bmatrix}
\mathbf{\Sigma}_{xx} &amp; \mathbf{\Sigma}_{xf} \\
\mathbf{\Sigma}_{fx} &amp; \mathbf{\Sigma}_{ff}
\end{bmatrix}
\right),
\]</span></p>
<p>where:
- <span class="math inline">\(\mathbf{\Sigma}_{xx} = \mathbb{C}[\mathbf{x}] = \mathbf{\Lambda} \mathbf{\Lambda}&#39; + \mathbf{\Psi}\)</span> (covariance of observed variables).
- <span class="math inline">\(\mathbf{\Sigma}_{ff} = \mathbb{C}[\mathbf{f}] = \mathbf{I}_k\)</span> (factors are standardized to have identity covariance).
- <span class="math inline">\(\mathbf{\Sigma}_{xf} = \mathbb{C}[\mathbf{x}, \mathbf{f}] = \mathbf{\Lambda}\)</span> (cross-covariance between observed variables and factors).</p>
<p>Thus, the <strong>joint distribution</strong> is:</p>
<p><span class="math display">\[
\begin{bmatrix}
\mathbf{x}_i \\
\mathbf{f}_i
\end{bmatrix}
\sim \mathcal{N} \left(
\begin{bmatrix}
\boldsymbol{\mu} \\
\mathbf{0}
\end{bmatrix},
\begin{bmatrix}
\mathbf{\Lambda} \mathbf{\Lambda}&#39; + \mathbf{\Psi} &amp; \mathbf{\Lambda} \\
\mathbf{\Lambda}&#39; &amp; \mathbf{I}_k
\end{bmatrix}
\right).
\]</span></p>
<hr />
</div>
<div id="conditional-distribution-of-mathbff_i-given-mathbfx_i" class="section level4 hasAnchor" number="6.4.3.2">
<h4><span class="header-section-number">6.4.3.2</span> Conditional Distribution of <span class="math inline">\(\mathbf{f}_i\)</span> Given <span class="math inline">\(\mathbf{x}_i\)</span><a href="factor-analysis.html#conditional-distribution-of-mathbff_i-given-mathbfx_i" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Using the standard formula for the conditional expectation of a multivariate normal distribution:</p>
<p><span class="math display">\[
\mathbb{E}[\mathbf{f}_i | \mathbf{x}_i] = \mathbf{\Sigma}_{fx} \mathbf{\Sigma}_{xx}^{-1} (\mathbf{x}_i - \boldsymbol{\mu}).
\]</span></p>
<p>Substituting <span class="math inline">\(\mathbf{\Sigma}_{fx} = \mathbf{\Lambda}&#39;\)</span> and <span class="math inline">\(\mathbf{\Sigma}_{xx} = \mathbf{\Lambda} \mathbf{\Lambda}&#39; + \mathbf{\Psi}\)</span>:</p>
<p><span class="math display">\[
\hat{\mathbf{f}}_i = \mathbb{E}[\mathbf{f}_i | \mathbf{x}_i] = \mathbf{\Lambda}&#39; (\mathbf{\Lambda} \mathbf{\Lambda}&#39; + \mathbf{\Psi})^{-1} (\mathbf{x}_i - \boldsymbol{\mu}).
\]</span></p>
<p>Thus, the <strong>Thompson factor score estimator</strong> is:</p>
<p><span class="math display">\[
\hat{\mathbf{f}}_i = \mathbf{\Lambda}&#39; (\mathbf{\Lambda} \mathbf{\Lambda}&#39; + \mathbf{\Psi})^{-1} (\mathbf{x}_i - \boldsymbol{\mu}).
\]</span></p>
<hr />
</div>
<div id="interpretation-of-thompsons-estimator" class="section level4 hasAnchor" number="6.4.3.3">
<h4><span class="header-section-number">6.4.3.3</span> Interpretation of Thompson’s Estimator<a href="factor-analysis.html#interpretation-of-thompsons-estimator" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>This estimator <strong>balances the influence of observed variables</strong> based on both <strong>common variance (factor loadings)</strong> and <strong>unique variances (specific errors)</strong>.</li>
<li>Unlike OLS and Bartlett’s estimator, this method <strong>considers the full covariance structure of the observed variables</strong>.</li>
<li>Since it is derived from the <strong>multivariate normal assumption</strong>, it provides the <strong>minimum mean squared error (MMSE) estimate</strong> of the factor scores.</li>
</ul>
<hr />
</div>
</div>
<div id="summary-of-factor-score-estimators" class="section level3 hasAnchor" number="6.4.4">
<h3><span class="header-section-number">6.4.4</span> Summary of Factor Score Estimators<a href="factor-analysis.html#summary-of-factor-score-estimators" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<table>
<colgroup>
<col width="28%" />
<col width="46%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Method</strong></th>
<th><strong>Formula for Factor Scores</strong></th>
<th><strong>Key Feature</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>OLS Estimator</strong></td>
<td><span class="math inline">\((\mathbf{\Lambda}&#39; \mathbf{\Lambda})^{-1} \mathbf{\Lambda}&#39; (\mathbf{x}_i - \boldsymbol{\mu})\)</span></td>
<td>Best fit using least squares</td>
</tr>
<tr class="even">
<td><strong>WLS (Bartlett) Estimator</strong></td>
<td><span class="math inline">\((\mathbf{\Lambda}&#39; \mathbf{\Psi}^{-1} \mathbf{\Lambda})^{-1} \mathbf{\Lambda}&#39; \mathbf{\Psi}^{-1} (\mathbf{x}_i - \boldsymbol{\mu})\)</span></td>
<td>Ensures factor scores are uncorrelated</td>
</tr>
<tr class="odd">
<td><strong>Regression (Thompson) Estimator</strong></td>
<td><span class="math inline">\(\mathbf{\Lambda}&#39; (\mathbf{\Lambda} \mathbf{\Lambda}&#39; + \mathbf{\Psi})^{-1} (\mathbf{x}_i - \boldsymbol{\mu})\)</span></td>
<td>Uses full covariance structure</td>
</tr>
</tbody>
</table>
<hr />
</div>
<div id="key-takeaways" class="section level3 hasAnchor" number="6.4.5">
<h3><span class="header-section-number">6.4.5</span> Key Takeaways<a href="factor-analysis.html#key-takeaways" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><strong>Factor scores estimate the latent variables</strong> based on the observed data.</li>
<li><strong>Different estimation methods</strong> lead to different interpretations of factor scores.</li>
<li>The <strong>choice of estimator depends on the application</strong>—some prioritize minimizing error, while others ensure uncorrelated factors.</li>
</ul>
<hr />
</div>
</div>
<div id="factor-analysis-example" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Factor Analysis Example<a href="factor-analysis.html#factor-analysis-example" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Here’s a <strong>full implementation of Factor Analysis</strong> in R, covering:</p>
<ol style="list-style-type: decimal">
<li><strong>Estimating the factor loadings <span class="math inline">\(\mathbf{\Lambda}\)</span> and unique variances <span class="math inline">\(\mathbf{\Psi}\)</span></strong>
<ul>
<li><strong>Principal Components Method</strong><br />
</li>
<li><strong>Maximum Likelihood Estimation (MLE)</strong></li>
</ul></li>
<li><strong>Estimating the factor scores</strong> using three methods:
<ul>
<li><strong>Ordinary Least Squares (OLS)</strong><br />
</li>
<li><strong>Weighted Least Squares (Bartlett’s method)</strong><br />
</li>
<li><strong>Regression (Thompson’s estimator)</strong></li>
</ul></li>
</ol>
<hr />
<div id="load-required-libraries" class="section level3 hasAnchor" number="6.5.1">
<h3><span class="header-section-number">6.5.1</span> Load Required Libraries<a href="factor-analysis.html#load-required-libraries" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="factor-analysis.html#cb97-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb97-2"><a href="factor-analysis.html#cb97-2" tabindex="-1"></a><span class="fu">library</span>(psych)  <span class="co"># For factor analysis functions</span></span>
<span id="cb97-3"><a href="factor-analysis.html#cb97-3" tabindex="-1"></a><span class="fu">library</span>(MASS)   <span class="co"># For matrix operations</span></span></code></pre></div>
<hr />
</div>
<div id="generate-sample-data" class="section level3 hasAnchor" number="6.5.2">
<h3><span class="header-section-number">6.5.2</span> Generate Sample Data<a href="factor-analysis.html#generate-sample-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We generate <strong>synthetic data</strong> with a predefined factor structure.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="factor-analysis.html#cb98-1" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb98-2"><a href="factor-analysis.html#cb98-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb98-3"><a href="factor-analysis.html#cb98-3" tabindex="-1"></a></span>
<span id="cb98-4"><a href="factor-analysis.html#cb98-4" tabindex="-1"></a><span class="co"># Define parameters</span></span>
<span id="cb98-5"><a href="factor-analysis.html#cb98-5" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">500</span>  <span class="co"># Number of observations</span></span>
<span id="cb98-6"><a href="factor-analysis.html#cb98-6" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">6</span>    <span class="co"># Number of observed variables</span></span>
<span id="cb98-7"><a href="factor-analysis.html#cb98-7" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">2</span>    <span class="co"># Number of factors</span></span>
<span id="cb98-8"><a href="factor-analysis.html#cb98-8" tabindex="-1"></a></span>
<span id="cb98-9"><a href="factor-analysis.html#cb98-9" tabindex="-1"></a><span class="co"># True factor loadings (simulated)</span></span>
<span id="cb98-10"><a href="factor-analysis.html#cb98-10" tabindex="-1"></a>Lambda_true <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="fl">0.9</span>, <span class="fl">0.8</span>, <span class="fl">0.7</span>,  <span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>, </span>
<span id="cb98-11"><a href="factor-analysis.html#cb98-11" tabindex="-1"></a>                        <span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>,  <span class="fl">0.9</span>, <span class="fl">0.8</span>, <span class="fl">0.7</span>), <span class="at">nrow =</span> p, <span class="at">ncol =</span> k)</span>
<span id="cb98-12"><a href="factor-analysis.html#cb98-12" tabindex="-1"></a></span>
<span id="cb98-13"><a href="factor-analysis.html#cb98-13" tabindex="-1"></a><span class="co"># Unique variances (diagonal matrix)</span></span>
<span id="cb98-14"><a href="factor-analysis.html#cb98-14" tabindex="-1"></a>Psi_true <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="fu">c</span>(<span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.2</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.2</span>))</span>
<span id="cb98-15"><a href="factor-analysis.html#cb98-15" tabindex="-1"></a></span>
<span id="cb98-16"><a href="factor-analysis.html#cb98-16" tabindex="-1"></a><span class="co"># Generate factor scores (standard normal)</span></span>
<span id="cb98-17"><a href="factor-analysis.html#cb98-17" tabindex="-1"></a>F_scores <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n <span class="sc">*</span> k), <span class="at">nrow =</span> n, <span class="at">ncol =</span> k)</span>
<span id="cb98-18"><a href="factor-analysis.html#cb98-18" tabindex="-1"></a></span>
<span id="cb98-19"><a href="factor-analysis.html#cb98-19" tabindex="-1"></a><span class="co"># Generate observed data</span></span>
<span id="cb98-20"><a href="factor-analysis.html#cb98-20" tabindex="-1"></a>E_noise <span class="ot">&lt;-</span> <span class="fu">mvrnorm</span>(n, <span class="at">mu =</span> <span class="fu">rep</span>(<span class="dv">0</span>, p), <span class="at">Sigma =</span> Psi_true)  <span class="co"># Unique variances</span></span>
<span id="cb98-21"><a href="factor-analysis.html#cb98-21" tabindex="-1"></a>X <span class="ot">&lt;-</span> F_scores <span class="sc">%*%</span> <span class="fu">t</span>(Lambda_true) <span class="sc">+</span> E_noise  <span class="co"># Factor model</span></span>
<span id="cb98-22"><a href="factor-analysis.html#cb98-22" tabindex="-1"></a></span>
<span id="cb98-23"><a href="factor-analysis.html#cb98-23" tabindex="-1"></a><span class="co"># Standardize data</span></span>
<span id="cb98-24"><a href="factor-analysis.html#cb98-24" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">scale</span>(X)</span></code></pre></div>
<hr />
</div>
<div id="estimation-of-factor-loadings-and-unique-variances" class="section level3 hasAnchor" number="6.5.3">
<h3><span class="header-section-number">6.5.3</span> Estimation of Factor Loadings and Unique Variances<a href="factor-analysis.html#estimation-of-factor-loadings-and-unique-variances" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="principal-components-method-1" class="section level4 hasAnchor" number="6.5.3.1">
<h4><span class="header-section-number">6.5.3.1</span> Principal Components Method<a href="factor-analysis.html#principal-components-method-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="factor-analysis.html#cb99-1" tabindex="-1"></a><span class="co"># Compute covariance matrix</span></span>
<span id="cb99-2"><a href="factor-analysis.html#cb99-2" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="fu">cov</span>(X)</span>
<span id="cb99-3"><a href="factor-analysis.html#cb99-3" tabindex="-1"></a></span>
<span id="cb99-4"><a href="factor-analysis.html#cb99-4" tabindex="-1"></a><span class="co"># Perform eigen decomposition</span></span>
<span id="cb99-5"><a href="factor-analysis.html#cb99-5" tabindex="-1"></a>eig <span class="ot">&lt;-</span> <span class="fu">eigen</span>(S)</span>
<span id="cb99-6"><a href="factor-analysis.html#cb99-6" tabindex="-1"></a></span>
<span id="cb99-7"><a href="factor-analysis.html#cb99-7" tabindex="-1"></a><span class="co"># Extract first k eigenvectors and eigenvalues</span></span>
<span id="cb99-8"><a href="factor-analysis.html#cb99-8" tabindex="-1"></a>Lambda_pc <span class="ot">&lt;-</span> eig<span class="sc">$</span>vectors[, <span class="dv">1</span><span class="sc">:</span>k] <span class="sc">%*%</span> <span class="fu">diag</span>(<span class="fu">sqrt</span>(eig<span class="sc">$</span>values[<span class="dv">1</span><span class="sc">:</span>k]))</span>
<span id="cb99-9"><a href="factor-analysis.html#cb99-9" tabindex="-1"></a></span>
<span id="cb99-10"><a href="factor-analysis.html#cb99-10" tabindex="-1"></a><span class="co"># Estimate unique variances Psi</span></span>
<span id="cb99-11"><a href="factor-analysis.html#cb99-11" tabindex="-1"></a>Psi_pc <span class="ot">&lt;-</span> <span class="fu">diag</span>(S) <span class="sc">-</span> <span class="fu">diag</span>(Lambda_pc <span class="sc">%*%</span> <span class="fu">t</span>(Lambda_pc))</span>
<span id="cb99-12"><a href="factor-analysis.html#cb99-12" tabindex="-1"></a></span>
<span id="cb99-13"><a href="factor-analysis.html#cb99-13" tabindex="-1"></a><span class="co"># Display results</span></span>
<span id="cb99-14"><a href="factor-analysis.html#cb99-14" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Factor Loadings (Principal Components Method):&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Factor Loadings (Principal Components Method):&quot;</code></pre>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="factor-analysis.html#cb101-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(Lambda_pc, <span class="dv">3</span>))</span></code></pre></div>
<pre><code>##        [,1]   [,2]
## [1,]  0.659 -0.641
## [2,]  0.612 -0.651
## [3,]  0.628 -0.649
## [4,] -0.689 -0.611
## [5,] -0.705 -0.572
## [6,] -0.677 -0.596</code></pre>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="factor-analysis.html#cb103-1" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Unique Variances (Principal Components Method):&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Unique Variances (Principal Components Method):&quot;</code></pre>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="factor-analysis.html#cb105-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(Psi_pc, <span class="dv">3</span>))</span></code></pre></div>
<pre><code>## [1] 0.156 0.203 0.184 0.152 0.175 0.186</code></pre>
<hr />
</div>
<div id="maximum-likelihood-estimation-mle-1" class="section level4 hasAnchor" number="6.5.3.2">
<h4><span class="header-section-number">6.5.3.2</span> Maximum Likelihood Estimation (MLE)<a href="factor-analysis.html#maximum-likelihood-estimation-mle-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="factor-analysis.html#cb107-1" tabindex="-1"></a><span class="co"># Perform MLE Factor Analysis</span></span>
<span id="cb107-2"><a href="factor-analysis.html#cb107-2" tabindex="-1"></a>fa_mle <span class="ot">&lt;-</span> <span class="fu">factanal</span>(X, <span class="at">factors =</span> k, <span class="at">rotation =</span> <span class="st">&quot;none&quot;</span>)</span>
<span id="cb107-3"><a href="factor-analysis.html#cb107-3" tabindex="-1"></a></span>
<span id="cb107-4"><a href="factor-analysis.html#cb107-4" tabindex="-1"></a><span class="co"># Extract factor loadings</span></span>
<span id="cb107-5"><a href="factor-analysis.html#cb107-5" tabindex="-1"></a>Lambda_mle <span class="ot">&lt;-</span> fa_mle<span class="sc">$</span>loadings[, <span class="dv">1</span><span class="sc">:</span>k]</span>
<span id="cb107-6"><a href="factor-analysis.html#cb107-6" tabindex="-1"></a></span>
<span id="cb107-7"><a href="factor-analysis.html#cb107-7" tabindex="-1"></a><span class="co"># Estimate unique variances Psi</span></span>
<span id="cb107-8"><a href="factor-analysis.html#cb107-8" tabindex="-1"></a>Psi_mle <span class="ot">&lt;-</span> <span class="fu">diag</span>(S) <span class="sc">-</span> <span class="fu">diag</span>(Lambda_mle <span class="sc">%*%</span> <span class="fu">t</span>(Lambda_mle))</span>
<span id="cb107-9"><a href="factor-analysis.html#cb107-9" tabindex="-1"></a></span>
<span id="cb107-10"><a href="factor-analysis.html#cb107-10" tabindex="-1"></a><span class="co"># Display results</span></span>
<span id="cb107-11"><a href="factor-analysis.html#cb107-11" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Factor Loadings (MLE Method):&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Factor Loadings (MLE Method):&quot;</code></pre>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="factor-analysis.html#cb109-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(Lambda_mle, <span class="dv">3</span>))</span></code></pre></div>
<pre><code>##      Factor1 Factor2
## [1,]  -0.575   0.685
## [2,]  -0.500   0.649
## [3,]  -0.528   0.663
## [4,]   0.726   0.518
## [5,]   0.716   0.469
## [6,]   0.686   0.481</code></pre>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="factor-analysis.html#cb111-1" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Unique Variances (MLE Method):&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Unique Variances (MLE Method):&quot;</code></pre>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="factor-analysis.html#cb113-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(Psi_mle, <span class="dv">3</span>))</span></code></pre></div>
<pre><code>## [1] 0.200 0.328 0.281 0.204 0.268 0.298</code></pre>
<hr />
</div>
</div>
<div id="estimate-factor-scores" class="section level3 hasAnchor" number="6.5.4">
<h3><span class="header-section-number">6.5.4</span> Estimate Factor Scores<a href="factor-analysis.html#estimate-factor-scores" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="ordinary-least-squares-ols-factor-scores" class="section level4 hasAnchor" number="6.5.4.1">
<h4><span class="header-section-number">6.5.4.1</span> Ordinary Least Squares (OLS) Factor Scores<a href="factor-analysis.html#ordinary-least-squares-ols-factor-scores" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="factor-analysis.html#cb115-1" tabindex="-1"></a><span class="co"># Compute OLS factor scores</span></span>
<span id="cb115-2"><a href="factor-analysis.html#cb115-2" tabindex="-1"></a>F_ols <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(Lambda_mle) <span class="sc">%*%</span> Lambda_mle) <span class="sc">%*%</span> <span class="fu">t</span>(Lambda_mle) <span class="sc">%*%</span> <span class="fu">t</span>(X)</span>
<span id="cb115-3"><a href="factor-analysis.html#cb115-3" tabindex="-1"></a>F_ols <span class="ot">&lt;-</span> <span class="fu">t</span>(F_ols)  <span class="co"># Transpose to match dimensions</span></span>
<span id="cb115-4"><a href="factor-analysis.html#cb115-4" tabindex="-1"></a></span>
<span id="cb115-5"><a href="factor-analysis.html#cb115-5" tabindex="-1"></a><span class="co"># Display first few factor scores</span></span>
<span id="cb115-6"><a href="factor-analysis.html#cb115-6" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Factor Scores (OLS Method):&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Factor Scores (OLS Method):&quot;</code></pre>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="factor-analysis.html#cb117-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(F_ols[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, ], <span class="dv">3</span>))</span></code></pre></div>
<pre><code>##      Factor1 Factor2
## [1,]  -0.454  -1.093
## [2,]  -0.230  -1.224
## [3,]  -0.039   1.097
## [4,]   0.909   0.484
## [5,]  -0.952  -0.539</code></pre>
<hr />
</div>
<div id="weighted-least-squares-bartletts-method" class="section level4 hasAnchor" number="6.5.4.2">
<h4><span class="header-section-number">6.5.4.2</span> Weighted Least Squares (Bartlett’s Method)<a href="factor-analysis.html#weighted-least-squares-bartletts-method" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="factor-analysis.html#cb119-1" tabindex="-1"></a><span class="co"># Compute WLS factor scores</span></span>
<span id="cb119-2"><a href="factor-analysis.html#cb119-2" tabindex="-1"></a>F_wls <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(Lambda_mle) <span class="sc">%*%</span> (Lambda_mle <span class="sc">/</span> Psi_mle)) <span class="sc">%*%</span> </span>
<span id="cb119-3"><a href="factor-analysis.html#cb119-3" tabindex="-1"></a>          <span class="fu">t</span>(Lambda_mle) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">/</span> Psi_mle</span></code></pre></div>
<pre><code>## Warning in solve(t(Lambda_mle) %*% (Lambda_mle/Psi_mle)) %*% t(Lambda_mle) %*% : longer object length is not a multiple of shorter object length</code></pre>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="factor-analysis.html#cb121-1" tabindex="-1"></a>F_wls <span class="ot">&lt;-</span> <span class="fu">t</span>(F_wls)</span>
<span id="cb121-2"><a href="factor-analysis.html#cb121-2" tabindex="-1"></a></span>
<span id="cb121-3"><a href="factor-analysis.html#cb121-3" tabindex="-1"></a><span class="co"># Display first few factor scores</span></span>
<span id="cb121-4"><a href="factor-analysis.html#cb121-4" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Factor Scores (Bartlett’s Method):&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Factor Scores (Bartlett’s Method):&quot;</code></pre>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="factor-analysis.html#cb123-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(F_wls[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, ], <span class="dv">3</span>))</span></code></pre></div>
<pre><code>##      Factor1 Factor2
## [1,]  -0.551  -0.838
## [2,]  -0.192  -1.512
## [3,]  -0.048   0.932
## [4,]   1.128   0.364
## [5,]  -0.841  -0.652</code></pre>
<hr />
</div>
<div id="regression-thompsons-estimator" class="section level4 hasAnchor" number="6.5.4.3">
<h4><span class="header-section-number">6.5.4.3</span> Regression (Thompson’s Estimator)<a href="factor-analysis.html#regression-thompsons-estimator" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="factor-analysis.html#cb125-1" tabindex="-1"></a><span class="co"># Compute Regression factor scores</span></span>
<span id="cb125-2"><a href="factor-analysis.html#cb125-2" tabindex="-1"></a>F_reg <span class="ot">&lt;-</span> <span class="fu">t</span>(Lambda_mle) <span class="sc">%*%</span> <span class="fu">solve</span>(Lambda_mle <span class="sc">%*%</span> <span class="fu">t</span>(Lambda_mle) <span class="sc">+</span> <span class="fu">diag</span>(Psi_mle), <span class="fu">t</span>(X))</span>
<span id="cb125-3"><a href="factor-analysis.html#cb125-3" tabindex="-1"></a>F_reg <span class="ot">&lt;-</span> <span class="fu">t</span>(F_reg)</span>
<span id="cb125-4"><a href="factor-analysis.html#cb125-4" tabindex="-1"></a></span>
<span id="cb125-5"><a href="factor-analysis.html#cb125-5" tabindex="-1"></a><span class="co"># Display first few factor scores</span></span>
<span id="cb125-6"><a href="factor-analysis.html#cb125-6" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Factor Scores (Regression Method - Thompson’s Estimator):&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Factor Scores (Regression Method - Thompson’s Estimator):&quot;</code></pre>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="factor-analysis.html#cb127-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(F_reg[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, ], <span class="dv">3</span>))</span></code></pre></div>
<pre><code>##      Factor1 Factor2
## [1,]  -0.470  -0.883
## [2,]  -0.187  -1.093
## [3,]  -0.003   0.933
## [4,]   0.765   0.396
## [5,]  -0.902  -0.412</code></pre>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="principal-component-analysis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="canonical-correlation-analysis-cca.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
