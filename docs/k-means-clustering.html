<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8 k-Means Clustering | DS 6388 Spring 2025</title>
  <meta name="description" content="8 k-Means Clustering | DS 6388 Spring 2025" />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="8 k-Means Clustering | DS 6388 Spring 2025" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8 k-Means Clustering | DS 6388 Spring 2025" />
  
  
  

<meta name="author" content="Rene Gutierrez University of Texas at El Paso" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="canonical-correlation-analysis-cca.html"/>
<link rel="next" href="expectation-maximization.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> DS 6388: Multivariate Statistical Methods for High-dimensional Data</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#calendar"><i class="fa fa-check"></i><b>1.1</b> Calendar</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#importatnt-dates"><i class="fa fa-check"></i><b>1.1.1</b> Importatnt Dates</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#class-schedule"><i class="fa fa-check"></i><b>1.1.2</b> Class Schedule</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>2</b> Prerequisites</a>
<ul>
<li class="chapter" data-level="2.1" data-path="prerequisites.html"><a href="prerequisites.html#linear-algebra"><i class="fa fa-check"></i><b>2.1</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="prerequisites.html"><a href="prerequisites.html#linear-independence"><i class="fa fa-check"></i><b>2.1.1</b> Linear Independence</a></li>
<li class="chapter" data-level="2.1.2" data-path="prerequisites.html"><a href="prerequisites.html#column-space-of-a-matrix"><i class="fa fa-check"></i><b>2.1.2</b> Column Space of a Matrix</a></li>
<li class="chapter" data-level="2.1.3" data-path="prerequisites.html"><a href="prerequisites.html#rank-of-a-matrix"><i class="fa fa-check"></i><b>2.1.3</b> Rank of a Matrix</a></li>
<li class="chapter" data-level="2.1.4" data-path="prerequisites.html"><a href="prerequisites.html#full-rank-matrix"><i class="fa fa-check"></i><b>2.1.4</b> Full Rank Matrix</a></li>
<li class="chapter" data-level="2.1.5" data-path="prerequisites.html"><a href="prerequisites.html#inverse-matrix"><i class="fa fa-check"></i><b>2.1.5</b> Inverse Matrix</a></li>
<li class="chapter" data-level="2.1.6" data-path="prerequisites.html"><a href="prerequisites.html#positive-definite-matrix"><i class="fa fa-check"></i><b>2.1.6</b> Positive Definite Matrix</a></li>
<li class="chapter" data-level="2.1.7" data-path="prerequisites.html"><a href="prerequisites.html#singular-value-decomposition"><i class="fa fa-check"></i><b>2.1.7</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="2.1.8" data-path="prerequisites.html"><a href="prerequisites.html#eigendecomposition"><i class="fa fa-check"></i><b>2.1.8</b> Eigendecomposition</a></li>
<li class="chapter" data-level="2.1.9" data-path="prerequisites.html"><a href="prerequisites.html#idempotent-matrix"><i class="fa fa-check"></i><b>2.1.9</b> Idempotent Matrix</a></li>
<li class="chapter" data-level="2.1.10" data-path="prerequisites.html"><a href="prerequisites.html#determinant-of-a-matrix"><i class="fa fa-check"></i><b>2.1.10</b> Determinant of a Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="prerequisites.html"><a href="prerequisites.html#calculus"><i class="fa fa-check"></i><b>2.2</b> Calculus</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="prerequisites.html"><a href="prerequisites.html#gradient"><i class="fa fa-check"></i><b>2.2.1</b> Gradient</a></li>
<li class="chapter" data-level="2.2.2" data-path="prerequisites.html"><a href="prerequisites.html#hessian-matrix"><i class="fa fa-check"></i><b>2.2.2</b> Hessian Matrix</a></li>
<li class="chapter" data-level="2.2.3" data-path="prerequisites.html"><a href="prerequisites.html#applications-1"><i class="fa fa-check"></i><b>2.2.3</b> Applications:</a></li>
<li class="chapter" data-level="2.2.4" data-path="prerequisites.html"><a href="prerequisites.html#matrix-calculus"><i class="fa fa-check"></i><b>2.2.4</b> Matrix Calculus</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="prerequisites.html"><a href="prerequisites.html#probability"><i class="fa fa-check"></i><b>2.3</b> Probability</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="prerequisites.html"><a href="prerequisites.html#expected-value"><i class="fa fa-check"></i><b>2.3.1</b> Expected Value</a></li>
<li class="chapter" data-level="2.3.2" data-path="prerequisites.html"><a href="prerequisites.html#variance"><i class="fa fa-check"></i><b>2.3.2</b> Variance</a></li>
<li class="chapter" data-level="2.3.3" data-path="prerequisites.html"><a href="prerequisites.html#cross-covariance-matrix"><i class="fa fa-check"></i><b>2.3.3</b> Cross-Covariance Matrix</a></li>
<li class="chapter" data-level="2.3.4" data-path="prerequisites.html"><a href="prerequisites.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>2.3.4</b> Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="2.3.5" data-path="prerequisites.html"><a href="prerequisites.html#chi2-distribution"><i class="fa fa-check"></i><b>2.3.5</b> <span class="math inline">\(\chi^2\)</span> Distribution</a></li>
<li class="chapter" data-level="2.3.6" data-path="prerequisites.html"><a href="prerequisites.html#t-distribution"><i class="fa fa-check"></i><b>2.3.6</b> <span class="math inline">\(t\)</span> Distribution</a></li>
<li class="chapter" data-level="2.3.7" data-path="prerequisites.html"><a href="prerequisites.html#f-distribution"><i class="fa fa-check"></i><b>2.3.7</b> <span class="math inline">\(F\)</span> Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="prerequisites.html"><a href="prerequisites.html#statistics"><i class="fa fa-check"></i><b>2.4</b> Statistics</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="prerequisites.html"><a href="prerequisites.html#bias-of-an-estimator"><i class="fa fa-check"></i><b>2.4.1</b> Bias of an Estimator</a></li>
<li class="chapter" data-level="2.4.2" data-path="prerequisites.html"><a href="prerequisites.html#unbiased-estimator"><i class="fa fa-check"></i><b>2.4.2</b> Unbiased Estimator</a></li>
<li class="chapter" data-level="2.4.3" data-path="prerequisites.html"><a href="prerequisites.html#mean-square-error-of-an-estimator"><i class="fa fa-check"></i><b>2.4.3</b> Mean Square Error of an Estimator</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>3</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introduction.html"><a href="introduction.html#key-challenges"><i class="fa fa-check"></i><b>3.1</b> Key Challenges</a></li>
<li class="chapter" data-level="3.2" data-path="introduction.html"><a href="introduction.html#core-concepts"><i class="fa fa-check"></i><b>3.2</b> Core Concepts</a></li>
<li class="chapter" data-level="3.3" data-path="introduction.html"><a href="introduction.html#applications-4"><i class="fa fa-check"></i><b>3.3</b> Applications</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>4</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="linear-regression.html"><a href="linear-regression.html#machine-learning"><i class="fa fa-check"></i><b>4.1</b> Machine Learning</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="linear-regression.html"><a href="linear-regression.html#ordinary-least-squares"><i class="fa fa-check"></i><b>4.1.1</b> Ordinary Least Squares</a></li>
<li class="chapter" data-level="4.1.2" data-path="linear-regression.html"><a href="linear-regression.html#ridge-regression"><i class="fa fa-check"></i><b>4.1.2</b> Ridge Regression</a></li>
<li class="chapter" data-level="4.1.3" data-path="linear-regression.html"><a href="linear-regression.html#lasso-regression"><i class="fa fa-check"></i><b>4.1.3</b> Lasso Regression</a></li>
<li class="chapter" data-level="4.1.4" data-path="linear-regression.html"><a href="linear-regression.html#elastic-net"><i class="fa fa-check"></i><b>4.1.4</b> Elastic Net</a></li>
<li class="chapter" data-level="4.1.5" data-path="linear-regression.html"><a href="linear-regression.html#elastic-net-as-a-mixed-penalty"><i class="fa fa-check"></i><b>4.1.5</b> <strong>Elastic Net as a Mixed Penalty</strong></a></li>
<li class="chapter" data-level="4.1.6" data-path="linear-regression.html"><a href="linear-regression.html#other-options-of-regularization"><i class="fa fa-check"></i><b>4.1.6</b> Other Options of Regularization</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-linear-regression"><i class="fa fa-check"></i><b>4.2</b> Bayesian Linear Regression</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="linear-regression.html"><a href="linear-regression.html#basic-bayesian-linear-regression"><i class="fa fa-check"></i><b>4.2.1</b> Basic Bayesian Linear Regression</a></li>
<li class="chapter" data-level="4.2.2" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-lasso-regression"><i class="fa fa-check"></i><b>4.2.2</b> Bayesian Lasso Regression</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="linear-regression.html"><a href="linear-regression.html#computational-comparisson"><i class="fa fa-check"></i><b>4.3</b> Computational Comparisson</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="linear-regression.html"><a href="linear-regression.html#set-up"><i class="fa fa-check"></i><b>4.3.1</b> Set-Up</a></li>
<li class="chapter" data-level="4.3.2" data-path="linear-regression.html"><a href="linear-regression.html#simulation"><i class="fa fa-check"></i><b>4.3.2</b> Simulation</a></li>
<li class="chapter" data-level="4.3.3" data-path="linear-regression.html"><a href="linear-regression.html#ols"><i class="fa fa-check"></i><b>4.3.3</b> OLS</a></li>
<li class="chapter" data-level="4.3.4" data-path="linear-regression.html"><a href="linear-regression.html#ridge-regression-1"><i class="fa fa-check"></i><b>4.3.4</b> Ridge Regression</a></li>
<li class="chapter" data-level="4.3.5" data-path="linear-regression.html"><a href="linear-regression.html#lasso"><i class="fa fa-check"></i><b>4.3.5</b> Lasso</a></li>
<li class="chapter" data-level="4.3.6" data-path="linear-regression.html"><a href="linear-regression.html#basic-bayesian-regression"><i class="fa fa-check"></i><b>4.3.6</b> Basic Bayesian Regression</a></li>
<li class="chapter" data-level="4.3.7" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-lasso"><i class="fa fa-check"></i><b>4.3.7</b> Bayesian Lasso</a></li>
<li class="chapter" data-level="4.3.8" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-horseshoe-prior"><i class="fa fa-check"></i><b>4.3.8</b> Bayesian Horseshoe Prior</a></li>
<li class="chapter" data-level="4.3.9" data-path="linear-regression.html"><a href="linear-regression.html#results-comparisson"><i class="fa fa-check"></i><b>4.3.9</b> Results Comparisson</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="linear-regression.html"><a href="linear-regression.html#efficient-computation"><i class="fa fa-check"></i><b>4.4</b> Efficient Computation</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="linear-regression.html"><a href="linear-regression.html#efficient-computation-of-ridge-regression-using-the-woodbury-identity"><i class="fa fa-check"></i><b>4.4.1</b> Efficient Computation of Ridge Regression using the Woodbury Identity</a></li>
<li class="chapter" data-level="4.4.2" data-path="linear-regression.html"><a href="linear-regression.html#efficient-bayesian-sampling-for-gaussian-scale-mixture-priors"><i class="fa fa-check"></i><b>4.4.2</b> Efficient Bayesian Sampling for Gaussian Scale-Mixture Priors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>5</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="5.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#pca-as-a-low-rank-approximation-of-mathbfx"><i class="fa fa-check"></i><b>5.1</b> PCA as a Low-Rank Approximation of <span class="math inline">\(\mathbf{X}\)</span></a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#why-low-rank-approximation-matters"><i class="fa fa-check"></i><b>5.1.1</b> Why Low-Rank Approximation Matters</a></li>
<li class="chapter" data-level="5.1.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#the-singular-value-solution-is-the-best-low-rank-approximation-eckartyoungmirsky-theorem"><i class="fa fa-check"></i><b>5.1.2</b> The Singular Value Solution is the Best Low-Rank Approximation (Eckart–Young–Mirsky Theorem)</a></li>
<li class="chapter" data-level="5.1.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#eckartyoungmirsky-theorem-for-the-spectral-norm"><i class="fa fa-check"></i><b>5.1.3</b> Eckart–Young–Mirsky Theorem for the Spectral Norm</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#variance-maximization-in-pca"><i class="fa fa-check"></i><b>5.2</b> Variance Maximization in PCA</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#first-principal-cmponent"><i class="fa fa-check"></i><b>5.2.1</b> First Principal Cmponent</a></li>
<li class="chapter" data-level="5.2.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#second-principal-component"><i class="fa fa-check"></i><b>5.2.2</b> Second Principal Component</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#parafac-decomposition"><i class="fa fa-check"></i><b>5.3</b> PARAFAC decomposition</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#what-is-a-tensor"><i class="fa fa-check"></i><b>5.3.1</b> What is a Tensor?</a></li>
<li class="chapter" data-level="5.3.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#motivation-why-generalize-pca-to-tensors"><i class="fa fa-check"></i><b>5.3.2</b> Motivation: Why Generalize PCA to Tensors?</a></li>
<li class="chapter" data-level="5.3.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#parafac-decomposition-definition"><i class="fa fa-check"></i><b>5.3.3</b> PARAFAC Decomposition Definition</a></li>
<li class="chapter" data-level="5.3.4" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#how-is-parafac-related-to-pca"><i class="fa fa-check"></i><b>5.3.4</b> How is PARAFAC Related to PCA?</a></li>
<li class="chapter" data-level="5.3.5" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#key-properties-of-parafac-decomposition"><i class="fa fa-check"></i><b>5.3.5</b> Key Properties of PARAFAC Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#high-dimensional-pca"><i class="fa fa-check"></i><b>5.4</b> High-Dimensional PCA</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#high-dimensional-pca-p-gg-n"><i class="fa fa-check"></i><b>5.4.1</b> High-Dimensional PCA (<span class="math inline">\(p \gg n\)</span>)</a></li>
<li class="chapter" data-level="5.4.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#randomized-pca"><i class="fa fa-check"></i><b>5.4.2</b> Randomized PCA</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#principal-components-regression"><i class="fa fa-check"></i><b>5.5</b> Principal Components Regression</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#what-is-principal-components-regression"><i class="fa fa-check"></i><b>5.5.1</b> What is Principal Components Regression?</a></li>
<li class="chapter" data-level="5.5.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#why-use-pcr"><i class="fa fa-check"></i><b>5.5.2</b> Why Use PCR?</a></li>
<li class="chapter" data-level="5.5.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#mathematical-formulation"><i class="fa fa-check"></i><b>5.5.3</b> Mathematical Formulation</a></li>
<li class="chapter" data-level="5.5.4" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#choosing-the-number-of-components"><i class="fa fa-check"></i><b>5.5.4</b> Choosing the Number of Components</a></li>
<li class="chapter" data-level="5.5.5" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#advantages-of-pcr"><i class="fa fa-check"></i><b>5.5.5</b> Advantages of PCR</a></li>
<li class="chapter" data-level="5.5.6" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#disadvantages"><i class="fa fa-check"></i><b>5.5.6</b> Disadvantages</a></li>
<li class="chapter" data-level="5.5.7" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#randomized-pca-and-pcr"><i class="fa fa-check"></i><b>5.5.7</b> Randomized PCA and PCR</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="factor-analysis.html"><a href="factor-analysis.html"><i class="fa fa-check"></i><b>6</b> Factor Analysis</a>
<ul>
<li class="chapter" data-level="6.1" data-path="factor-analysis.html"><a href="factor-analysis.html#introduction-2"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="factor-analysis.html"><a href="factor-analysis.html#how-factor-analysis-works"><i class="fa fa-check"></i><b>6.1.1</b> How Factor Analysis Works</a></li>
<li class="chapter" data-level="6.1.2" data-path="factor-analysis.html"><a href="factor-analysis.html#why-is-factor-analysis-useful"><i class="fa fa-check"></i><b>6.1.2</b> Why is Factor Analysis Useful?</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="factor-analysis.html"><a href="factor-analysis.html#factor-analysis-model"><i class="fa fa-check"></i><b>6.2</b> Factor Analysis Model</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="factor-analysis.html"><a href="factor-analysis.html#model-specification-1"><i class="fa fa-check"></i><b>6.2.1</b> Model Specification</a></li>
<li class="chapter" data-level="6.2.2" data-path="factor-analysis.html"><a href="factor-analysis.html#assumptions"><i class="fa fa-check"></i><b>6.2.2</b> Assumptions</a></li>
<li class="chapter" data-level="6.2.3" data-path="factor-analysis.html"><a href="factor-analysis.html#variance-covariance-matrix"><i class="fa fa-check"></i><b>6.2.3</b> Variance-Covariance Matrix</a></li>
<li class="chapter" data-level="6.2.4" data-path="factor-analysis.html"><a href="factor-analysis.html#implications"><i class="fa fa-check"></i><b>6.2.4</b> Implications</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="factor-analysis.html"><a href="factor-analysis.html#estimation-in-factor-analysis"><i class="fa fa-check"></i><b>6.3</b> Estimation in Factor Analysis</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="factor-analysis.html"><a href="factor-analysis.html#principal-components-method"><i class="fa fa-check"></i><b>6.3.1</b> Principal Components Method</a></li>
<li class="chapter" data-level="6.3.2" data-path="factor-analysis.html"><a href="factor-analysis.html#principal-factor-method"><i class="fa fa-check"></i><b>6.3.2</b> Principal Factor Method</a></li>
<li class="chapter" data-level="6.3.3" data-path="factor-analysis.html"><a href="factor-analysis.html#maximum-likelihood-estimation-mle"><i class="fa fa-check"></i><b>6.3.3</b> Maximum Likelihood Estimation (MLE)</a></li>
<li class="chapter" data-level="6.3.4" data-path="factor-analysis.html"><a href="factor-analysis.html#bayesian-estimation-hierarchical-priors"><i class="fa fa-check"></i><b>6.3.4</b> Bayesian Estimation (Hierarchical Priors)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="factor-analysis.html"><a href="factor-analysis.html#estimating-factor-scores-in-factor-analysis"><i class="fa fa-check"></i><b>6.4</b> Estimating Factor Scores in Factor Analysis</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="factor-analysis.html"><a href="factor-analysis.html#ordinary-least-squares-ols-method"><i class="fa fa-check"></i><b>6.4.1</b> Ordinary Least Squares (OLS) Method</a></li>
<li class="chapter" data-level="6.4.2" data-path="factor-analysis.html"><a href="factor-analysis.html#weighted-least-squares-wls-method-bartletts-method"><i class="fa fa-check"></i><b>6.4.2</b> Weighted Least Squares (WLS) Method (Bartlett’s Method)</a></li>
<li class="chapter" data-level="6.4.3" data-path="factor-analysis.html"><a href="factor-analysis.html#regression-method-thompsons-estimator"><i class="fa fa-check"></i><b>6.4.3</b> Regression Method (Thompson’s Estimator)</a></li>
<li class="chapter" data-level="6.4.4" data-path="factor-analysis.html"><a href="factor-analysis.html#summary-of-factor-score-estimators"><i class="fa fa-check"></i><b>6.4.4</b> Summary of Factor Score Estimators</a></li>
<li class="chapter" data-level="6.4.5" data-path="factor-analysis.html"><a href="factor-analysis.html#key-takeaways"><i class="fa fa-check"></i><b>6.4.5</b> Key Takeaways</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="factor-analysis.html"><a href="factor-analysis.html#factor-analysis-example"><i class="fa fa-check"></i><b>6.5</b> Factor Analysis Example</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="factor-analysis.html"><a href="factor-analysis.html#load-required-libraries"><i class="fa fa-check"></i><b>6.5.1</b> Load Required Libraries</a></li>
<li class="chapter" data-level="6.5.2" data-path="factor-analysis.html"><a href="factor-analysis.html#generate-sample-data"><i class="fa fa-check"></i><b>6.5.2</b> Generate Sample Data</a></li>
<li class="chapter" data-level="6.5.3" data-path="factor-analysis.html"><a href="factor-analysis.html#estimation-of-factor-loadings-and-unique-variances"><i class="fa fa-check"></i><b>6.5.3</b> Estimation of Factor Loadings and Unique Variances</a></li>
<li class="chapter" data-level="6.5.4" data-path="factor-analysis.html"><a href="factor-analysis.html#estimate-factor-scores"><i class="fa fa-check"></i><b>6.5.4</b> Estimate Factor Scores</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html"><i class="fa fa-check"></i><b>7</b> Canonical Correlation Analysis (CCA)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#motivation-1"><i class="fa fa-check"></i><b>7.1</b> Motivation</a></li>
<li class="chapter" data-level="7.2" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#mathematical-formulation-1"><i class="fa fa-check"></i><b>7.2</b> Mathematical Formulation</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#key-properties-6"><i class="fa fa-check"></i><b>7.2.1</b> Key Properties</a></li>
<li class="chapter" data-level="7.2.2" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#implementation-in-r"><i class="fa fa-check"></i><b>7.2.2</b> Implementation in R</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#canonical-directions-estimation"><i class="fa fa-check"></i><b>7.3</b> Canonical Directions Estimation</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#problem-definition-with-random-variables"><i class="fa fa-check"></i><b>7.3.1</b> Problem Definition with Random Variables</a></li>
<li class="chapter" data-level="7.3.2" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#canonical-correlation-maximization-problem"><i class="fa fa-check"></i><b>7.3.2</b> Canonical Correlation Maximization Problem</a></li>
<li class="chapter" data-level="7.3.3" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#solving-for-the-canonical-directions-using-lagrange-multipliers"><i class="fa fa-check"></i><b>7.3.3</b> Solving for the Canonical Directions Using Lagrange Multipliers</a></li>
<li class="chapter" data-level="7.3.4" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#showing-that-the-matrices-have-the-same-eigenvalues"><i class="fa fa-check"></i><b>7.3.4</b> Showing That the Matrices Have the Same Eigenvalues</a></li>
<li class="chapter" data-level="7.3.5" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#showing-that-the-largest-eigenvalue-maximizes-the-objective-function"><i class="fa fa-check"></i><b>7.3.5</b> Showing That the Largest Eigenvalue Maximizes the Objective Function</a></li>
<li class="chapter" data-level="7.3.6" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#conclusion-3"><i class="fa fa-check"></i><b>7.3.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#hd-cca"><i class="fa fa-check"></i><b>7.4</b> HD CCA</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#regularized-canonical-correlation-analysis-ridge-cca"><i class="fa fa-check"></i><b>7.4.1</b> Regularized Canonical Correlation Analysis (Ridge CCA)</a></li>
<li class="chapter" data-level="7.4.2" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#sparse-canonical-correlation-analysis-sparse-cca"><i class="fa fa-check"></i><b>7.4.2</b> Sparse Canonical Correlation Analysis (Sparse CCA)**</a></li>
<li class="chapter" data-level="7.4.3" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#low-rank-approximation-cca-randomized-svd-approach"><i class="fa fa-check"></i><b>7.4.3</b> Low-Rank Approximation CCA (Randomized SVD Approach)</a></li>
<li class="chapter" data-level="7.4.4" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#factor-model-based-cca"><i class="fa fa-check"></i><b>7.4.4</b> Factor Model-Based CCA</a></li>
<li class="chapter" data-level="7.4.5" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#comparison-of-methods-for-high-dimensional-cca"><i class="fa fa-check"></i><b>7.4.5</b> Comparison of Methods for High-Dimensional CCA</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#conclusion-which-method-to-use"><i class="fa fa-check"></i><b>7.5</b> <strong>Conclusion: Which Method to Use?</strong></a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="k-means-clustering.html"><a href="k-means-clustering.html"><i class="fa fa-check"></i><b>8</b> k-Means Clustering</a>
<ul>
<li class="chapter" data-level="8.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#introduction-3"><i class="fa fa-check"></i><b>8.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#intuition-behind-k-means"><i class="fa fa-check"></i><b>8.1.1</b> Intuition Behind k-Means</a></li>
<li class="chapter" data-level="8.1.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#applications-of-k-means"><i class="fa fa-check"></i><b>8.1.2</b> Applications of k-Means</a></li>
<li class="chapter" data-level="8.1.3" data-path="k-means-clustering.html"><a href="k-means-clustering.html#choosing-the-number-of-clusters-k"><i class="fa fa-check"></i><b>8.1.3</b> Choosing the Number of Clusters <span class="math inline">\(k\)</span></a></li>
<li class="chapter" data-level="8.1.4" data-path="k-means-clustering.html"><a href="k-means-clustering.html#r-implementation-of-k-means"><i class="fa fa-check"></i><b>8.1.4</b> R Implementation of k-Means</a></li>
<li class="chapter" data-level="8.1.5" data-path="k-means-clustering.html"><a href="k-means-clustering.html#strengths-and-weaknesses-of-k-means"><i class="fa fa-check"></i><b>8.1.5</b> Strengths and Weaknesses of k-Means</a></li>
<li class="chapter" data-level="8.1.6" data-path="k-means-clustering.html"><a href="k-means-clustering.html#when-to-use-k-means"><i class="fa fa-check"></i><b>8.1.6</b> When to Use k-Means</a></li>
<li class="chapter" data-level="8.1.7" data-path="k-means-clustering.html"><a href="k-means-clustering.html#conclusion-k-means"><i class="fa fa-check"></i><b>8.1.7</b> Conclusion k-means</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#problem-statement"><i class="fa fa-check"></i><b>8.2</b> Problem Statement</a></li>
<li class="chapter" data-level="8.3" data-path="k-means-clustering.html"><a href="k-means-clustering.html#naive-solution"><i class="fa fa-check"></i><b>8.3</b> Naive Solution</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#why-evaluating-all-possible-partitions-works"><i class="fa fa-check"></i><b>8.3.1</b> Why Evaluating All Possible Partitions Works?</a></li>
<li class="chapter" data-level="8.3.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#why-does-this-work-for-all-possible-partitions"><i class="fa fa-check"></i><b>8.3.2</b> Why Does This Work for All Possible Partitions?</a></li>
<li class="chapter" data-level="8.3.3" data-path="k-means-clustering.html"><a href="k-means-clustering.html#efficiency-of-the-naive-approach"><i class="fa fa-check"></i><b>8.3.3</b> Efficiency of the Naive Approach</a></li>
<li class="chapter" data-level="8.3.4" data-path="k-means-clustering.html"><a href="k-means-clustering.html#key-intuition"><i class="fa fa-check"></i><b>8.3.4</b> Key Intuition</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="k-means-clustering.html"><a href="k-means-clustering.html#the-k-means-algorithm-lloyds-algorithm"><i class="fa fa-check"></i><b>8.4</b> The k-Means Algorithm (Lloyd’s algorithm)</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#why-does-k-means-works"><i class="fa fa-check"></i><b>8.4.1</b> Why does k-Means works?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="expectation-maximization.html"><a href="expectation-maximization.html"><i class="fa fa-check"></i><b>9</b> Expectation Maximization</a>
<ul>
<li class="chapter" data-level="9.1" data-path="expectation-maximization.html"><a href="expectation-maximization.html#introduction-4"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="expectation-maximization.html"><a href="expectation-maximization.html#model-framework"><i class="fa fa-check"></i><b>9.2</b> Model Framework</a></li>
<li class="chapter" data-level="9.3" data-path="expectation-maximization.html"><a href="expectation-maximization.html#algorithm"><i class="fa fa-check"></i><b>9.3</b> Algorithm</a></li>
<li class="chapter" data-level="9.4" data-path="expectation-maximization.html"><a href="expectation-maximization.html#proof-of-em-convergence"><i class="fa fa-check"></i><b>9.4</b> Proof of EM Convergence</a></li>
<li class="chapter" data-level="9.5" data-path="expectation-maximization.html"><a href="expectation-maximization.html#conclusion-4"><i class="fa fa-check"></i><b>9.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="gaussian-mixture-models.html"><a href="gaussian-mixture-models.html"><i class="fa fa-check"></i><b>10</b> Gaussian Mixture Models</a>
<ul>
<li class="chapter" data-level="10.1" data-path="gaussian-mixture-models.html"><a href="gaussian-mixture-models.html#introduction-5"><i class="fa fa-check"></i><b>10.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="gaussian-mixture-models.html"><a href="gaussian-mixture-models.html#model-definition"><i class="fa fa-check"></i><b>10.1.1</b> Model Definition</a></li>
<li class="chapter" data-level="10.1.2" data-path="gaussian-mixture-models.html"><a href="gaussian-mixture-models.html#latent-variable-interpretation"><i class="fa fa-check"></i><b>10.1.2</b> Latent Variable Interpretation</a></li>
<li class="chapter" data-level="10.1.3" data-path="gaussian-mixture-models.html"><a href="gaussian-mixture-models.html#parameters-to-estimate"><i class="fa fa-check"></i><b>10.1.3</b> Parameters to Estimate</a></li>
<li class="chapter" data-level="10.1.4" data-path="gaussian-mixture-models.html"><a href="gaussian-mixture-models.html#summary-3"><i class="fa fa-check"></i><b>10.1.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="gaussian-mixture-models.html"><a href="gaussian-mixture-models.html#em-on-gmm"><i class="fa fa-check"></i><b>10.2</b> EM on GMM</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="gaussian-mixture-models.html"><a href="gaussian-mixture-models.html#original-model"><i class="fa fa-check"></i><b>10.2.1</b> Original model</a></li>
<li class="chapter" data-level="10.2.2" data-path="gaussian-mixture-models.html"><a href="gaussian-mixture-models.html#new-model"><i class="fa fa-check"></i><b>10.2.2</b> New model</a></li>
<li class="chapter" data-level="10.2.3" data-path="gaussian-mixture-models.html"><a href="gaussian-mixture-models.html#model-equivalency"><i class="fa fa-check"></i><b>10.2.3</b> Model Equivalency</a></li>
<li class="chapter" data-level="10.2.4" data-path="gaussian-mixture-models.html"><a href="gaussian-mixture-models.html#expectation-computation"><i class="fa fa-check"></i><b>10.2.4</b> Expectation Computation</a></li>
<li class="chapter" data-level="10.2.5" data-path="gaussian-mixture-models.html"><a href="gaussian-mixture-models.html#maximization-step"><i class="fa fa-check"></i><b>10.2.5</b> Maximization Step</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">DS 6388 Spring 2025</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="k-means-clustering" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">8</span> k-Means Clustering<a href="k-means-clustering.html#k-means-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introduction-3" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> Introduction<a href="k-means-clustering.html#introduction-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>k-Means clustering</strong> is one of the most widely used unsupervised learning algorithms for <strong>partitioning data</strong> into distinct groups based on similarity. It is a simple yet powerful method that minimizes the <strong>within-cluster variation</strong>.</p>
<hr />
<div id="intuition-behind-k-means" class="section level3 hasAnchor" number="8.1.1">
<h3><span class="header-section-number">8.1.1</span> Intuition Behind k-Means<a href="k-means-clustering.html#intuition-behind-k-means" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Imagine you are tasked with grouping points on a map. k-Means aims to achieve this by:</p>
<p>✅ Placing <span class="math inline">\(k\)</span> “centers” (called <strong>centroids</strong>) on the map.<br />
✅ Assigning each point to the <strong>closest centroid</strong>.<br />
✅ Adjusting the centroid positions to minimize the <strong>distance between points and their assigned centroid</strong>.<br />
✅ Repeating this process until the clusters are stable.</p>
<p>The algorithm tries to minimize the <strong>within-cluster sum of squares (WCSS)</strong> — essentially grouping points that are <strong>closer to each other</strong> than to points in other clusters.</p>
<hr />
</div>
<div id="applications-of-k-means" class="section level3 hasAnchor" number="8.1.2">
<h3><span class="header-section-number">8.1.2</span> Applications of k-Means<a href="k-means-clustering.html#applications-of-k-means" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>k-Means is highly versatile and used in various domains:</p>
<ul>
<li><strong>Business and Marketing</strong>
<ul>
<li><strong>Customer Segmentation:</strong> Group customers based on purchasing behavior.<br />
</li>
<li><strong>Market Segmentation:</strong> Identify distinct user profiles for targeted marketing.<br />
</li>
</ul></li>
<li><strong>Image and Video Processing</strong>
<ul>
<li><strong>Color Quantization:</strong> Compress images by reducing the number of colors.<br />
</li>
<li><strong>Object Detection:</strong> Cluster pixel intensities for image segmentation.<br />
</li>
</ul></li>
<li><strong>Anomaly Detection</strong>
<ul>
<li>Identify outliers by clustering data points and flagging those farthest from the centroids.<br />
</li>
</ul></li>
<li><strong>Bioinformatics</strong>
<ul>
<li>Cluster gene expression data for identifying gene functions.</li>
</ul></li>
</ul>
<hr />
</div>
<div id="choosing-the-number-of-clusters-k" class="section level3 hasAnchor" number="8.1.3">
<h3><span class="header-section-number">8.1.3</span> Choosing the Number of Clusters <span class="math inline">\(k\)</span><a href="k-means-clustering.html#choosing-the-number-of-clusters-k" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Choosing <span class="math inline">\(k\)</span> is critical for performance. Common techniques include:</p>
<p>✅ <strong>Elbow Method:</strong> Plot WCSS against <span class="math inline">\(k\)</span> and identify the point where the reduction in WCSS slows down (the “elbow”).<br />
✅ <strong>Silhouette Score:</strong> Measures how well each point fits into its cluster (higher is better).<br />
✅ <strong>Gap Statistic:</strong> Compares WCSS to that expected under random data.</p>
<hr />
</div>
<div id="r-implementation-of-k-means" class="section level3 hasAnchor" number="8.1.4">
<h3><span class="header-section-number">8.1.4</span> R Implementation of k-Means<a href="k-means-clustering.html#r-implementation-of-k-means" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="step-1-load-libraries-and-simulate-data" class="section level4 hasAnchor" number="8.1.4.1">
<h4><span class="header-section-number">8.1.4.1</span> Step 1: Load Libraries and Simulate Data<a href="k-means-clustering.html#step-1-load-libraries-and-simulate-data" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="k-means-clustering.html#cb143-1" tabindex="-1"></a><span class="co"># Load necessary library</span></span>
<span id="cb143-2"><a href="k-means-clustering.html#cb143-2" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb143-3"><a href="k-means-clustering.html#cb143-3" tabindex="-1"></a></span>
<span id="cb143-4"><a href="k-means-clustering.html#cb143-4" tabindex="-1"></a><span class="co"># Simulate example data</span></span>
<span id="cb143-5"><a href="k-means-clustering.html#cb143-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb143-6"><a href="k-means-clustering.html#cb143-6" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">300</span>  <span class="co"># Number of points</span></span>
<span id="cb143-7"><a href="k-means-clustering.html#cb143-7" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb143-8"><a href="k-means-clustering.html#cb143-8" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">3</span>, <span class="dv">8</span>, <span class="dv">1</span>)),</span>
<span id="cb143-9"><a href="k-means-clustering.html#cb143-9" tabindex="-1"></a>  <span class="at">y =</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">3</span>, <span class="dv">8</span>, <span class="dv">1</span>))</span>
<span id="cb143-10"><a href="k-means-clustering.html#cb143-10" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div id="step-2-perform-k-means-clustering" class="section level4 hasAnchor" number="8.1.4.2">
<h4><span class="header-section-number">8.1.4.2</span> Step 2: Perform k-Means Clustering<a href="k-means-clustering.html#step-2-perform-k-means-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="k-means-clustering.html#cb144-1" tabindex="-1"></a><span class="co"># Perform k-Means clustering</span></span>
<span id="cb144-2"><a href="k-means-clustering.html#cb144-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)  <span class="co"># Ensures reproducibility</span></span>
<span id="cb144-3"><a href="k-means-clustering.html#cb144-3" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">3</span>          <span class="co"># Number of clusters</span></span>
<span id="cb144-4"><a href="k-means-clustering.html#cb144-4" tabindex="-1"></a>kmeans_result <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(X, <span class="at">centers =</span> k, <span class="at">nstart =</span> <span class="dv">25</span>)</span>
<span id="cb144-5"><a href="k-means-clustering.html#cb144-5" tabindex="-1"></a></span>
<span id="cb144-6"><a href="k-means-clustering.html#cb144-6" tabindex="-1"></a><span class="co"># Add cluster labels to the dataset</span></span>
<span id="cb144-7"><a href="k-means-clustering.html#cb144-7" tabindex="-1"></a>X<span class="sc">$</span>cluster <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(kmeans_result<span class="sc">$</span>cluster)</span></code></pre></div>
</div>
<div id="step-3-visualize-the-results" class="section level4 hasAnchor" number="8.1.4.3">
<h4><span class="header-section-number">8.1.4.3</span> Step 3: Visualize the Results<a href="k-means-clustering.html#step-3-visualize-the-results" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="k-means-clustering.html#cb145-1" tabindex="-1"></a><span class="co"># Visualize the result</span></span>
<span id="cb145-2"><a href="k-means-clustering.html#cb145-2" tabindex="-1"></a><span class="fu">ggplot</span>(X, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> cluster)) <span class="sc">+</span></span>
<span id="cb145-3"><a href="k-means-clustering.html#cb145-3" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb145-4"><a href="k-means-clustering.html#cb145-4" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> <span class="fu">as.data.frame</span>(kmeans_result<span class="sc">$</span>centers), </span>
<span id="cb145-5"><a href="k-means-clustering.html#cb145-5" tabindex="-1"></a>             <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), </span>
<span id="cb145-6"><a href="k-means-clustering.html#cb145-6" tabindex="-1"></a>             <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">size =</span> <span class="dv">4</span>, <span class="at">shape =</span> <span class="dv">8</span>) <span class="sc">+</span></span>
<span id="cb145-7"><a href="k-means-clustering.html#cb145-7" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;k-Means Clustering&quot;</span>, <span class="at">subtitle =</span> <span class="st">&quot;k = 3&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/k_means_visualization-1.png" width="672" /></p>
</div>
<div id="step-4-determine-the-optimal-number-of-clusters-using-the-elbow-method" class="section level4 hasAnchor" number="8.1.4.4">
<h4><span class="header-section-number">8.1.4.4</span> Step 4: Determine the Optimal Number of Clusters Using the Elbow Method<a href="k-means-clustering.html#step-4-determine-the-optimal-number-of-clusters-using-the-elbow-method" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="k-means-clustering.html#cb146-1" tabindex="-1"></a><span class="co"># Elbow method to find optimal k</span></span>
<span id="cb146-2"><a href="k-means-clustering.html#cb146-2" tabindex="-1"></a>wcss <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="dv">10</span>)</span>
<span id="cb146-3"><a href="k-means-clustering.html#cb146-3" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>) {</span>
<span id="cb146-4"><a href="k-means-clustering.html#cb146-4" tabindex="-1"></a>  wcss[k] <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(X, <span class="at">centers =</span> k, <span class="at">nstart =</span> <span class="dv">25</span>)<span class="sc">$</span>tot.withinss</span>
<span id="cb146-5"><a href="k-means-clustering.html#cb146-5" tabindex="-1"></a>}</span>
<span id="cb146-6"><a href="k-means-clustering.html#cb146-6" tabindex="-1"></a></span>
<span id="cb146-7"><a href="k-means-clustering.html#cb146-7" tabindex="-1"></a><span class="co"># Plot the WCSS to identify the &#39;elbow&#39;</span></span>
<span id="cb146-8"><a href="k-means-clustering.html#cb146-8" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, wcss, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">frame =</span> <span class="cn">FALSE</span>,</span>
<span id="cb146-9"><a href="k-means-clustering.html#cb146-9" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Number of Clusters (k)&quot;</span>, </span>
<span id="cb146-10"><a href="k-means-clustering.html#cb146-10" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;WCSS (Within-Cluster Sum of Squares)&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/k_means_optimal_cluster_elbow-1.png" width="672" /></p>
<hr />
</div>
</div>
<div id="strengths-and-weaknesses-of-k-means" class="section level3 hasAnchor" number="8.1.5">
<h3><span class="header-section-number">8.1.5</span> Strengths and Weaknesses of k-Means<a href="k-means-clustering.html#strengths-and-weaknesses-of-k-means" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>✅ <strong>Strengths:</strong><br />
- Efficient for large datasets.<br />
- Simple and intuitive.<br />
- Fast convergence with <span class="math inline">\(O(n k d i)\)</span> complexity.</p>
<p>❗️<strong>Weaknesses:</strong><br />
- Assumes <strong>spherical clusters</strong>.<br />
- Sensitive to <strong>initialization</strong> (use <strong>k-Means++</strong> to improve).<br />
- Struggles with <strong>non-convex shapes</strong> or <strong>clusters with varying densities</strong>.<br />
- May converge to <strong>local minima</strong> — multiple runs improve stability.</p>
<hr />
</div>
<div id="when-to-use-k-means" class="section level3 hasAnchor" number="8.1.6">
<h3><span class="header-section-number">8.1.6</span> When to Use k-Means<a href="k-means-clustering.html#when-to-use-k-means" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>✅ Use k-Means when:
- The data has <strong>compact, well-separated clusters</strong>.<br />
- The feature space is <strong>low to moderate-dimensional</strong>.<br />
- Speed is a priority (k-Means is fast).</p>
<p>❗️Avoid k-Means when:
- Clusters are <strong>non-convex</strong> or <strong>overlapping</strong> (consider <strong>DBSCAN</strong> or <strong>Spectral Clustering</strong>).<br />
- The data has <strong>significant outliers</strong>.</p>
<hr />
</div>
<div id="conclusion-k-means" class="section level3 hasAnchor" number="8.1.7">
<h3><span class="header-section-number">8.1.7</span> Conclusion k-means<a href="k-means-clustering.html#conclusion-k-means" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>k-Means is a powerful yet simple clustering algorithm that performs well in many practical applications. While its efficiency is attractive, thoughtful parameter tuning and multiple runs are essential for achieving optimal results.</p>
</div>
</div>
<div id="problem-statement" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> Problem Statement<a href="k-means-clustering.html#problem-statement" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Given a dataset with <span class="math inline">\(n\)</span> observations and <span class="math inline">\(d\)</span> features:</p>
<p><span class="math display">\[
\mathbf{X} = \{ \mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n \} \quad \text{where} \quad \mathbf{x}_i \in \mathbb{R}^d
\]</span></p>
<p>The goal is to partition the data into <span class="math inline">\(k\)</span> clusters <span class="math inline">\(\{ C_1, C_2, \dots, C_k \}\)</span> such that the total <strong>within-cluster sum of squares (WCSS)</strong> is minimized:</p>
<p><span class="math display">\[
\min_{C_1, \dots, C_k} \sum_{j=1}^{k} \sum_{\mathbf{x}_i \in C_j} \| \mathbf{x}_i - \boldsymbol{\mu}_j \|^2
\]</span></p>
<p>where:
- <span class="math inline">\(\boldsymbol{\mu}_j = \frac{1}{|C_j|} \sum_{\mathbf{x}_i \in C_j} \mathbf{x}_i\)</span> is the <strong>centroid</strong> of cluster <span class="math inline">\(C_j\)</span>.
- The objective minimizes the sum of squared distances between points and their respective cluster centroids.</p>
<hr />
</div>
<div id="naive-solution" class="section level2 hasAnchor" number="8.3">
<h2><span class="header-section-number">8.3</span> Naive Solution<a href="k-means-clustering.html#naive-solution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Why Can We Solve the Partition Problem by Evaluating Every Possible Partition?</strong> To understand why a “naive” method — where we evaluate <strong>all possible groupings of elements</strong> — can correctly compute the number of partitions, we need to analyze the combinatorial structure of partitions.</p>
<hr />
<div id="why-evaluating-all-possible-partitions-works" class="section level3 hasAnchor" number="8.3.1">
<h3><span class="header-section-number">8.3.1</span> Why Evaluating All Possible Partitions Works?<a href="k-means-clustering.html#why-evaluating-all-possible-partitions-works" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To see why the naive approach is valid, we can build partitions in an <strong>incremental way</strong>:</p>
<div id="step-1-start-with-an-empty-set" class="section level4 hasAnchor" number="8.3.1.1">
<h4><span class="header-section-number">8.3.1.1</span> Step 1: Start with an Empty Set<a href="k-means-clustering.html#step-1-start-with-an-empty-set" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Suppose we have a set <span class="math inline">\(\{1, 2, \dots, n \}\)</span>.</p>
</div>
<div id="step-2-add-elements-one-by-one" class="section level4 hasAnchor" number="8.3.1.2">
<h4><span class="header-section-number">8.3.1.2</span> Step 2: Add Elements One by One<a href="k-means-clustering.html#step-2-add-elements-one-by-one" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>When adding the <span class="math inline">\(i\)</span>-th element to an existing partition:
- It can join <strong>any existing subset</strong>.
- Or it can form a <strong>new subset</strong>.</p>
<p>Thus, at each step, there are <strong>two choices</strong>:
- Place the element in one of the existing clusters.
- Start a new cluster with the element alone.</p>
<p>This combinatorial logic corresponds directly to the recurrence relation for Stirling numbers of the second kind:</p>
<p><span class="math display">\[
S(n, k) = k \cdot S(n-1, k) + S(n-1, k-1)
\]</span></p>
<ul>
<li>The first term represents placing the new element in one of the existing <span class="math inline">\(k\)</span> groups.<br />
</li>
<li>The second term represents starting a <strong>new</strong> group, increasing the cluster count from <span class="math inline">\(k-1\)</span> to <span class="math inline">\(k\)</span>.</li>
</ul>
<hr />
</div>
</div>
<div id="why-does-this-work-for-all-possible-partitions" class="section level3 hasAnchor" number="8.3.2">
<h3><span class="header-section-number">8.3.2</span> Why Does This Work for All Possible Partitions?<a href="k-means-clustering.html#why-does-this-work-for-all-possible-partitions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>By systematically generating all possible combinations:
- Every valid partition is explored.<br />
- Each combination adheres to the non-overlapping, non-empty subset condition.<br />
- The method ensures no duplicates are counted by the incremental logic.</p>
<hr />
</div>
<div id="efficiency-of-the-naive-approach" class="section level3 hasAnchor" number="8.3.3">
<h3><span class="header-section-number">8.3.3</span> Efficiency of the Naive Approach<a href="k-means-clustering.html#efficiency-of-the-naive-approach" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>The naive method effectively explores the <strong>lattice of set partitions</strong>, where each state corresponds to a valid grouping.<br />
</li>
<li>While correct, this method has exponential complexity: <span class="math inline">\(O(n!)\)</span> for brute force generation.</li>
</ul>
<p>This is why the <strong>recursive Stirling number relation</strong> or <strong>dynamic programming</strong> is preferable for larger <span class="math inline">\(n\)</span>.</p>
<hr />
</div>
<div id="key-intuition" class="section level3 hasAnchor" number="8.3.4">
<h3><span class="header-section-number">8.3.4</span> Key Intuition<a href="k-means-clustering.html#key-intuition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>✅ Each element has multiple valid placements (existing subset or new subset).<br />
✅ Exploring all combinations inherently builds all valid partitions.<br />
✅ The recurrence relation mirrors this exact branching structure.</p>
</div>
</div>
<div id="the-k-means-algorithm-lloyds-algorithm" class="section level2 hasAnchor" number="8.4">
<h2><span class="header-section-number">8.4</span> The k-Means Algorithm (Lloyd’s algorithm)<a href="k-means-clustering.html#the-k-means-algorithm-lloyds-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The k-Means algorithm uses an <strong>iterative refinement</strong> approach. The steps are:</p>
<p><strong>Step 1:</strong> <strong>Initialize <span class="math inline">\(k\)</span> cluster centroids.</strong><br />
- Common methods include:
- <strong>Random Initialization</strong>: Randomly select <span class="math inline">\(k\)</span> points as centroids.
- <strong>k-Means++ Initialization</strong> (recommended): Selects initial centroids to maximize cluster separation.</p>
<p><strong>Step 2:</strong> <strong>Assign points to the nearest centroid.</strong><br />
- For each observation <span class="math inline">\(\mathbf{x}_i\)</span>, assign it to the cluster with the closest centroid:</p>
<p><span class="math display">\[
\text{Cluster}(\mathbf{x}_i) = \arg \min_{j} \|\mathbf{x}_i - \boldsymbol{\mu}_j\|^2
\]</span></p>
<p><strong>Step 3:</strong> <strong>Update the centroids.</strong><br />
- Recalculate each centroid as the mean of its assigned points:</p>
<p><span class="math display">\[
\boldsymbol{\mu}_j = \frac{1}{|C_j|} \sum_{\mathbf{x}_i \in C_j} \mathbf{x}_i
\]</span></p>
<p><strong>Step 4:</strong> <strong>Repeat Steps 2 and 3 until convergence.</strong><br />
- Convergence occurs when cluster assignments no longer change or centroids stabilize.</p>
<hr />
<div id="why-does-k-means-works" class="section level3 hasAnchor" number="8.4.1">
<h3><span class="header-section-number">8.4.1</span> Why does k-Means works?<a href="k-means-clustering.html#why-does-k-means-works" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We need to show that each iteration of the <strong>k-Means algorithm</strong> decreases the <strong>within-cluster sum of squares (WCSS)</strong>, ensuring convergence.</p>
<p>The k-Means algorithm alternates between two steps:</p>
<ol style="list-style-type: decimal">
<li><strong>Cluster Assignment Step:</strong> Assign each point to the closest centroid.<br />
</li>
<li><strong>Centroid Update Step:</strong> Recalculate the centroid as the mean of points in each cluster.</li>
</ol>
<p>We’ll prove that <strong>each of these steps decreases the WCSS</strong>.</p>
<div id="cluster-assignment-step-decreases-wcss" class="section level4 hasAnchor" number="8.4.1.1">
<h4><span class="header-section-number">8.4.1.1</span> Cluster Assignment Step Decreases WCSS<a href="k-means-clustering.html#cluster-assignment-step-decreases-wcss" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Suppose we are in the middle of the k-Means algorithm with centroids <span class="math inline">\(\{ \boldsymbol{\mu}_1, \dots, \boldsymbol{\mu}_k \}\)</span>.</p>
<ul>
<li>Each point <span class="math inline">\(\mathbf{x}_i\)</span> is assigned to the nearest centroid.<br />
</li>
<li>For a point originally assigned to cluster <span class="math inline">\(C_j\)</span>, assume it is reassigned to <span class="math inline">\(C_\ell\)</span>.</li>
</ul>
<p>Since the assignment rule is:</p>
<p><span class="math display">\[
\|\mathbf{x}_i - \boldsymbol{\mu}_\ell \|^2 \leq \|\mathbf{x}_i - \boldsymbol{\mu}_j \|^2
\]</span></p>
<p>the WCSS can only <strong>decrease or remain the same</strong>.</p>
<p>✅ <strong>Conclusion:</strong> The assignment step <strong>never increases WCSS</strong>.</p>
<hr />
</div>
<div id="centroid-update-step-decreases-wcss" class="section level4 hasAnchor" number="8.4.1.2">
<h4><span class="header-section-number">8.4.1.2</span> Centroid Update Step Decreases WCSS<a href="k-means-clustering.html#centroid-update-step-decreases-wcss" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Next, consider the effect of updating the centroids.</p>
<p>Suppose the updated centroid for cluster <span class="math inline">\(C_j\)</span> is:</p>
<p><span class="math display">\[
\boldsymbol{\mu}_j = \frac{1}{|C_j|} \sum_{\mathbf{x}_i \in C_j} \mathbf{x}_i
\]</span></p>
<p>We want to show that this new centroid minimizes:</p>
<p><span class="math display">\[
\sum_{\mathbf{x}_i \in C_j} \|\mathbf{x}_i - \boldsymbol{\mu}_j\|^2
\]</span></p>
<p>From properties of the <strong>sample mean</strong>, the mean is the point that minimizes the sum of squared distances to all points in the cluster:</p>
<p><span class="math display">\[
\boldsymbol{\mu}_j = \arg\min_{\mathbf{y}} \sum_{\mathbf{x}_i \in C_j} \|\mathbf{x}_i - \mathbf{y} \|^2
\]</span></p>
<p>✅ <strong>Conclusion:</strong> The centroid update step <strong>strictly decreases</strong> WCSS unless the centroid is already optimal.</p>
</div>
<div id="combined-effect" class="section level4 hasAnchor" number="8.4.1.3">
<h4><span class="header-section-number">8.4.1.3</span> Combined Effect<a href="k-means-clustering.html#combined-effect" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>The <strong>Cluster Assignment Step</strong> decreases WCSS or leaves it unchanged.<br />
</li>
<li>The <strong>Centroid Update Step</strong> strictly decreases WCSS unless centroids are optimal.</li>
</ul>
<p>Since WCSS is <strong>lower-bounded by zero</strong>, the algorithm must eventually converge.</p>
</div>
<div id="why-does-k-means-converge" class="section level4 hasAnchor" number="8.4.1.4">
<h4><span class="header-section-number">8.4.1.4</span> Why Does k-Means Converge?<a href="k-means-clustering.html#why-does-k-means-converge" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Each iteration reduces WCSS.<br />
</li>
<li>WCSS is <strong>bounded</strong> and <strong>non-negative</strong>.<br />
</li>
<li>Therefore, by the monotone convergence theorem, the algorithm must eventually reach a local minimum.</li>
</ul>
<p>❗️ <strong>Important:</strong> k-Means may converge to a <strong>local minimum</strong>, not necessarily the <strong>global minimum</strong> — this is why multiple initializations (like <strong>k-Means++</strong>) are recommended.</p>
</div>
<div id="summary-2" class="section level4 hasAnchor" number="8.4.1.5">
<h4><span class="header-section-number">8.4.1.5</span> Summary<a href="k-means-clustering.html#summary-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>✅ Each iteration of k-Means reduces the WCSS.<br />
✅ Convergence is guaranteed, though the final solution may only be <strong>locally optimal</strong>.<br />
✅ Using improved initialization techniques like <strong>k-Means++</strong> helps achieve better results.</p>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="canonical-correlation-analysis-cca.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="expectation-maximization.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
