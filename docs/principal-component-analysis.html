<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Principal Component Analysis | DS 6388 Spring 2025</title>
  <meta name="description" content="5 Principal Component Analysis | DS 6388 Spring 2025" />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Principal Component Analysis | DS 6388 Spring 2025" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Principal Component Analysis | DS 6388 Spring 2025" />
  
  
  

<meta name="author" content="Rene Gutierrez" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-regression.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> DS 6388: Multivariate Statistical Methods for High-dimensional Data</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#calendar"><i class="fa fa-check"></i><b>1.1</b> Calendar</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#importatnt-dates"><i class="fa fa-check"></i><b>1.1.1</b> Importatnt Dates</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#class-schedule"><i class="fa fa-check"></i><b>1.1.2</b> Class Schedule</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>2</b> Prerequisites</a>
<ul>
<li class="chapter" data-level="2.1" data-path="prerequisites.html"><a href="prerequisites.html#linear-algebra"><i class="fa fa-check"></i><b>2.1</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="prerequisites.html"><a href="prerequisites.html#linear-independence"><i class="fa fa-check"></i><b>2.1.1</b> Linear Independence</a></li>
<li class="chapter" data-level="2.1.2" data-path="prerequisites.html"><a href="prerequisites.html#column-space-of-a-matrix"><i class="fa fa-check"></i><b>2.1.2</b> Column Space of a Matrix</a></li>
<li class="chapter" data-level="2.1.3" data-path="prerequisites.html"><a href="prerequisites.html#rank-of-a-matrix"><i class="fa fa-check"></i><b>2.1.3</b> Rank of a Matrix</a></li>
<li class="chapter" data-level="2.1.4" data-path="prerequisites.html"><a href="prerequisites.html#full-rank-matrix"><i class="fa fa-check"></i><b>2.1.4</b> Full Rank Matrix</a></li>
<li class="chapter" data-level="2.1.5" data-path="prerequisites.html"><a href="prerequisites.html#inverse-matrix"><i class="fa fa-check"></i><b>2.1.5</b> Inverse Matrix</a></li>
<li class="chapter" data-level="2.1.6" data-path="prerequisites.html"><a href="prerequisites.html#positive-definite-matrix"><i class="fa fa-check"></i><b>2.1.6</b> Positive Definite Matrix</a></li>
<li class="chapter" data-level="2.1.7" data-path="prerequisites.html"><a href="prerequisites.html#singular-value-decomposition"><i class="fa fa-check"></i><b>2.1.7</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="2.1.8" data-path="prerequisites.html"><a href="prerequisites.html#eigendecomposition"><i class="fa fa-check"></i><b>2.1.8</b> Eigendecomposition</a></li>
<li class="chapter" data-level="2.1.9" data-path="prerequisites.html"><a href="prerequisites.html#idempotent-matrix"><i class="fa fa-check"></i><b>2.1.9</b> Idempotent Matrix</a></li>
<li class="chapter" data-level="2.1.10" data-path="prerequisites.html"><a href="prerequisites.html#determinant-of-a-matrix"><i class="fa fa-check"></i><b>2.1.10</b> Determinant of a Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="prerequisites.html"><a href="prerequisites.html#calculus"><i class="fa fa-check"></i><b>2.2</b> Calculus</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="prerequisites.html"><a href="prerequisites.html#gradient"><i class="fa fa-check"></i><b>2.2.1</b> Gradient</a></li>
<li class="chapter" data-level="2.2.2" data-path="prerequisites.html"><a href="prerequisites.html#hessian-matrix"><i class="fa fa-check"></i><b>2.2.2</b> Hessian Matrix</a></li>
<li class="chapter" data-level="2.2.3" data-path="prerequisites.html"><a href="prerequisites.html#applications-1"><i class="fa fa-check"></i><b>2.2.3</b> Applications:</a></li>
<li class="chapter" data-level="2.2.4" data-path="prerequisites.html"><a href="prerequisites.html#matrix-calculus"><i class="fa fa-check"></i><b>2.2.4</b> Matrix Calculus</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="prerequisites.html"><a href="prerequisites.html#probability"><i class="fa fa-check"></i><b>2.3</b> Probability</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="prerequisites.html"><a href="prerequisites.html#expected-value"><i class="fa fa-check"></i><b>2.3.1</b> Expected Value</a></li>
<li class="chapter" data-level="2.3.2" data-path="prerequisites.html"><a href="prerequisites.html#variance"><i class="fa fa-check"></i><b>2.3.2</b> Variance</a></li>
<li class="chapter" data-level="2.3.3" data-path="prerequisites.html"><a href="prerequisites.html#cross-covariance-matrix"><i class="fa fa-check"></i><b>2.3.3</b> Cross-Covariance Matrix</a></li>
<li class="chapter" data-level="2.3.4" data-path="prerequisites.html"><a href="prerequisites.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>2.3.4</b> Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="2.3.5" data-path="prerequisites.html"><a href="prerequisites.html#chi2-distribution"><i class="fa fa-check"></i><b>2.3.5</b> <span class="math inline">\(\chi^2\)</span> Distribution</a></li>
<li class="chapter" data-level="2.3.6" data-path="prerequisites.html"><a href="prerequisites.html#t-distribution"><i class="fa fa-check"></i><b>2.3.6</b> <span class="math inline">\(t\)</span> Distribution</a></li>
<li class="chapter" data-level="2.3.7" data-path="prerequisites.html"><a href="prerequisites.html#f-distribution"><i class="fa fa-check"></i><b>2.3.7</b> <span class="math inline">\(F\)</span> Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="prerequisites.html"><a href="prerequisites.html#statistics"><i class="fa fa-check"></i><b>2.4</b> Statistics</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="prerequisites.html"><a href="prerequisites.html#bias-of-an-estimator"><i class="fa fa-check"></i><b>2.4.1</b> Bias of an Estimator</a></li>
<li class="chapter" data-level="2.4.2" data-path="prerequisites.html"><a href="prerequisites.html#unbiased-estimator"><i class="fa fa-check"></i><b>2.4.2</b> Unbiased Estimator</a></li>
<li class="chapter" data-level="2.4.3" data-path="prerequisites.html"><a href="prerequisites.html#mean-square-error-of-an-estimator"><i class="fa fa-check"></i><b>2.4.3</b> Mean Square Error of an Estimator</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>3</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introduction.html"><a href="introduction.html#key-challenges"><i class="fa fa-check"></i><b>3.1</b> Key Challenges</a></li>
<li class="chapter" data-level="3.2" data-path="introduction.html"><a href="introduction.html#core-concepts"><i class="fa fa-check"></i><b>3.2</b> Core Concepts</a></li>
<li class="chapter" data-level="3.3" data-path="introduction.html"><a href="introduction.html#applications-4"><i class="fa fa-check"></i><b>3.3</b> Applications</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>4</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="linear-regression.html"><a href="linear-regression.html#machine-learning"><i class="fa fa-check"></i><b>4.1</b> Machine Learning</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="linear-regression.html"><a href="linear-regression.html#ordinary-least-squares"><i class="fa fa-check"></i><b>4.1.1</b> Ordinary Least Squares</a></li>
<li class="chapter" data-level="4.1.2" data-path="linear-regression.html"><a href="linear-regression.html#ridge-regression"><i class="fa fa-check"></i><b>4.1.2</b> Ridge Regression</a></li>
<li class="chapter" data-level="4.1.3" data-path="linear-regression.html"><a href="linear-regression.html#lasso-regression"><i class="fa fa-check"></i><b>4.1.3</b> Lasso Regression</a></li>
<li class="chapter" data-level="4.1.4" data-path="linear-regression.html"><a href="linear-regression.html#elastic-net"><i class="fa fa-check"></i><b>4.1.4</b> Elastic Net</a></li>
<li class="chapter" data-level="4.1.5" data-path="linear-regression.html"><a href="linear-regression.html#elastic-net-as-a-mixed-penalty"><i class="fa fa-check"></i><b>4.1.5</b> <strong>Elastic Net as a Mixed Penalty</strong></a></li>
<li class="chapter" data-level="4.1.6" data-path="linear-regression.html"><a href="linear-regression.html#other-options-of-regularization"><i class="fa fa-check"></i><b>4.1.6</b> Other Options of Regularization</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-linear-regression"><i class="fa fa-check"></i><b>4.2</b> Bayesian Linear Regression</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="linear-regression.html"><a href="linear-regression.html#basic-bayesian-linear-regression"><i class="fa fa-check"></i><b>4.2.1</b> Basic Bayesian Linear Regression</a></li>
<li class="chapter" data-level="4.2.2" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-lasso-regression"><i class="fa fa-check"></i><b>4.2.2</b> Bayesian Lasso Regression</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="linear-regression.html"><a href="linear-regression.html#computational-comparisson"><i class="fa fa-check"></i><b>4.3</b> Computational Comparisson</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="linear-regression.html"><a href="linear-regression.html#set-up"><i class="fa fa-check"></i><b>4.3.1</b> Set-Up</a></li>
<li class="chapter" data-level="4.3.2" data-path="linear-regression.html"><a href="linear-regression.html#simulation"><i class="fa fa-check"></i><b>4.3.2</b> Simulation</a></li>
<li class="chapter" data-level="4.3.3" data-path="linear-regression.html"><a href="linear-regression.html#ols"><i class="fa fa-check"></i><b>4.3.3</b> OLS</a></li>
<li class="chapter" data-level="4.3.4" data-path="linear-regression.html"><a href="linear-regression.html#ridge-regression-1"><i class="fa fa-check"></i><b>4.3.4</b> Ridge Regression</a></li>
<li class="chapter" data-level="4.3.5" data-path="linear-regression.html"><a href="linear-regression.html#lasso"><i class="fa fa-check"></i><b>4.3.5</b> Lasso</a></li>
<li class="chapter" data-level="4.3.6" data-path="linear-regression.html"><a href="linear-regression.html#basic-bayesian-regression"><i class="fa fa-check"></i><b>4.3.6</b> Basic Bayesian Regression</a></li>
<li class="chapter" data-level="4.3.7" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-lasso"><i class="fa fa-check"></i><b>4.3.7</b> Bayesian Lasso</a></li>
<li class="chapter" data-level="4.3.8" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-horseshoe-prior"><i class="fa fa-check"></i><b>4.3.8</b> Bayesian Horseshoe Prior</a></li>
<li class="chapter" data-level="4.3.9" data-path="linear-regression.html"><a href="linear-regression.html#results-comparisson"><i class="fa fa-check"></i><b>4.3.9</b> Results Comparisson</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="linear-regression.html"><a href="linear-regression.html#efficient-computation"><i class="fa fa-check"></i><b>4.4</b> Efficient Computation</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="linear-regression.html"><a href="linear-regression.html#efficient-computation-of-ridge-regression-using-the-woodbury-identity"><i class="fa fa-check"></i><b>4.4.1</b> Efficient Computation of Ridge Regression using the Woodbury Identity</a></li>
<li class="chapter" data-level="4.4.2" data-path="linear-regression.html"><a href="linear-regression.html#efficient-bayesian-sampling-for-gaussian-scale-mixture-priors"><i class="fa fa-check"></i><b>4.4.2</b> Efficient Bayesian Sampling for Gaussian Scale-Mixture Priors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>5</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="5.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#pca-as-a-low-rank-approximation-of-mathbfx"><i class="fa fa-check"></i><b>5.1</b> PCA as a Low-Rank Approximation of <span class="math inline">\(\mathbf{X}\)</span></a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#why-low-rank-approximation-matters"><i class="fa fa-check"></i><b>5.1.1</b> Why Low-Rank Approximation Matters</a></li>
<li class="chapter" data-level="5.1.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#the-singular-value-solution-is-the-best-low-rank-approximation-eckartyoungmirsky-theorem"><i class="fa fa-check"></i><b>5.1.2</b> The Singular Value Solution is the Best Low-Rank Approximation (Eckart–Young–Mirsky Theorem)</a></li>
<li class="chapter" data-level="5.1.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#eckartyoungmirsky-theorem-for-the-spectral-norm"><i class="fa fa-check"></i><b>5.1.3</b> Eckart–Young–Mirsky Theorem for the Spectral Norm</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#variance-maximization-in-pca"><i class="fa fa-check"></i><b>5.2</b> Variance Maximization in PCA</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#first-principal-cmponent"><i class="fa fa-check"></i><b>5.2.1</b> First Principal Cmponent</a></li>
<li class="chapter" data-level="5.2.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#second-principal-component"><i class="fa fa-check"></i><b>5.2.2</b> Second Principal Component</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#parafac-decomposition"><i class="fa fa-check"></i><b>5.3</b> PARAFAC decomposition</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#what-is-a-tensor"><i class="fa fa-check"></i><b>5.3.1</b> What is a Tensor?</a></li>
<li class="chapter" data-level="5.3.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#motivation-why-generalize-pca-to-tensors"><i class="fa fa-check"></i><b>5.3.2</b> Motivation: Why Generalize PCA to Tensors?</a></li>
<li class="chapter" data-level="5.3.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#parafac-decomposition-definition"><i class="fa fa-check"></i><b>5.3.3</b> PARAFAC Decomposition Definition</a></li>
<li class="chapter" data-level="5.3.4" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#how-is-parafac-related-to-pca"><i class="fa fa-check"></i><b>5.3.4</b> How is PARAFAC Related to PCA?</a></li>
<li class="chapter" data-level="5.3.5" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#key-properties-of-parafac-decomposition"><i class="fa fa-check"></i><b>5.3.5</b> Key Properties of PARAFAC Decomposition</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">DS 6388 Spring 2025</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="principal-component-analysis" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">5</span> Principal Component Analysis<a href="principal-component-analysis.html#principal-component-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Principal Component Analysis (PCA) is a widely used dimensionality reduction technique in statistics and machine learning. While it is often introduced as a method for <strong>finding orthogonal directions of maximum variance</strong>, another fundamental perspective is that <strong>PCA provides the best low-rank approximation of the data matrix <span class="math inline">\(\mathbf{X}\)</span></strong>.</p>
<p>In many real-world datasets, the observed variables exhibit significant redundancy due to correlations between features. This means that the <strong>true intrinsic dimensionality</strong> of the data is often much lower than the number of measured variables. PCA exploits this redundancy by representing the data using a <strong>lower-dimensional subspace</strong> while preserving as much of the original information as possible.</p>
<hr />
<div id="pca-as-a-low-rank-approximation-of-mathbfx" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> PCA as a Low-Rank Approximation of <span class="math inline">\(\mathbf{X}\)</span><a href="principal-component-analysis.html#pca-as-a-low-rank-approximation-of-mathbfx" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let <span class="math inline">\(\mathbf{X}\)</span> be an <span class="math inline">\(n \times p\)</span> data matrix, where <span class="math inline">\(n\)</span> is the number of observations and <span class="math inline">\(p\)</span> is the number of variables. The goal of PCA is to find a low-rank representation of <span class="math inline">\(\mathbf{X}\)</span> that captures its most important structure. This is achieved through the <strong>Singular Value Decomposition (SVD)</strong>:</p>
<p><span class="math display">\[
\mathbf{X} = \sum_{i=1}^{r} \sigma_i \mathbf{u}_i \mathbf{v}_i&#39;
\]</span></p>
<p>where:
- <span class="math inline">\(\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_r &gt; 0\)</span> are the <strong>singular values</strong> of <span class="math inline">\(\mathbf{X}\)</span>,
- <span class="math inline">\(\mathbf{u}_i\)</span> and <span class="math inline">\(\mathbf{v}_i\)</span> are the corresponding <strong>left and right singular vectors</strong>,
- <span class="math inline">\(r = \text{rank}(\mathbf{X})\)</span>.</p>
<p>PCA provides a <strong>rank-<span class="math inline">\(k\)</span> approximation</strong> by retaining only the <strong>top <span class="math inline">\(k\)</span> singular values and singular vectors</strong>, leading to:</p>
<p><span class="math display">\[
\mathbf{X}_k = \sum_{i=1}^{k} \sigma_i \mathbf{u}_i \mathbf{v}_i&#39;.
\]</span></p>
<p>By the <strong>Eckart–Young–Mirsky theorem</strong>, this is the best approximation to <span class="math inline">\(\mathbf{X}\)</span> under the <strong>Frobenius norm</strong>, meaning it minimizes the reconstruction error:</p>
<p><span class="math display">\[
\|\mathbf{X} - \mathbf{X}_k\|_F^2 = \sum_{i=k+1}^{r} \sigma_i^2.
\]</span></p>
<hr />
<div id="why-low-rank-approximation-matters" class="section level3 hasAnchor" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> Why Low-Rank Approximation Matters<a href="principal-component-analysis.html#why-low-rank-approximation-matters" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li><strong>Dimensionality Reduction</strong>
<ul>
<li>The original data matrix <span class="math inline">\(\mathbf{X}\)</span> is often high-dimensional and contains redundant information.<br />
</li>
<li>PCA allows us to approximate <span class="math inline">\(\mathbf{X}\)</span> with a much smaller representation while preserving most of the variance.</li>
</ul></li>
<li><strong>Noise Reduction</strong>
<ul>
<li>Higher-order singular values <span class="math inline">\(\sigma_{k+1}, \dots, \sigma_r\)</span> are often associated with noise rather than meaningful structure.<br />
</li>
<li>Truncating these components leads to a <strong>denoised version of the data</strong>.</li>
</ul></li>
<li><strong>Compression and Storage Efficiency</strong>
<ul>
<li>Instead of storing <span class="math inline">\(n \times p\)</span> raw data values, we only need to store the <strong>top <span class="math inline">\(k\)</span> singular values and vectors</strong>, which is much more memory-efficient.</li>
</ul></li>
<li><strong>Feature Extraction</strong>
<ul>
<li>The <strong>principal components</strong> (columns of <span class="math inline">\(\mathbf{V}_k\)</span>) define a new <strong>basis</strong> for the data that is more informative and often interpretable.<br />
</li>
<li>These features can be used for <strong>classification, clustering, and visualization</strong>.</li>
</ul></li>
</ol>
<hr />
</div>
<div id="the-singular-value-solution-is-the-best-low-rank-approximation-eckartyoungmirsky-theorem" class="section level3 hasAnchor" number="5.1.2">
<h3><span class="header-section-number">5.1.2</span> The Singular Value Solution is the Best Low-Rank Approximation (Eckart–Young–Mirsky Theorem)<a href="principal-component-analysis.html#the-singular-value-solution-is-the-best-low-rank-approximation-eckartyoungmirsky-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We want to prove that the <strong>truncated Singular Value Decomposition (SVD)</strong> provides the <strong>best rank-<span class="math inline">\(k\)</span> approximation</strong> to a matrix <span class="math inline">\(\mathbf{X}\)</span> in the <strong>Frobenius norm</strong> and <strong>spectral norm</strong>. This result is known as the <strong>Eckart–Young–Mirsky theorem</strong>.</p>
<hr />
<div id="problem-statement-eckartyoungmirsky-theorem" class="section level4 hasAnchor" number="5.1.2.1">
<h4><span class="header-section-number">5.1.2.1</span> Problem Statement (Eckart–Young–Mirsky Theorem)<a href="principal-component-analysis.html#problem-statement-eckartyoungmirsky-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let <span class="math inline">\(\mathbf{X}\)</span> be an <span class="math inline">\(n \times p\)</span> matrix with <strong>full SVD</strong>:</p>
<p><span class="math display">\[
\mathbf{X} = \sum_{i=1}^{r} \sigma_i \mathbf{u}_i \mathbf{v}_i&#39;
\]</span></p>
<p>where:
- <span class="math inline">\(\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_r &gt; 0\)</span> are the <strong>singular values</strong> (ordered decreasingly),
- <span class="math inline">\(\mathbf{u}_i\)</span> are the left singular vectors (columns of <span class="math inline">\(\mathbf{U}\)</span>, an <span class="math inline">\(n \times n\)</span> orthonormal matrix),
- <span class="math inline">\(\mathbf{v}_i\)</span> are the right singular vectors (columns of <span class="math inline">\(\mathbf{V}\)</span>, a <span class="math inline">\(p \times p\)</span> orthonormal matrix),
- <span class="math inline">\(r = \text{rank}(\mathbf{X})\)</span>.</p>
<p>We seek a <strong>rank-<span class="math inline">\(k\)</span> matrix</strong> <span class="math inline">\(\mathbf{Y}\)</span> that best approximates <span class="math inline">\(\mathbf{X}\)</span> by minimizing the <strong>Frobenius norm error</strong>:</p>
<p><span class="math display">\[
\min_{\text{rank}(\mathbf{Y}) = k} \|\mathbf{X} - \mathbf{Y}\|_F.
\]</span></p>
<p>The theorem states that the best rank-<span class="math inline">\(k\)</span> approximation is given by the <strong>truncated SVD</strong>:</p>
<p><span class="math display">\[
\mathbf{X}_k = \sum_{i=1}^{k} \sigma_i \mathbf{u}_i \mathbf{v}_i&#39;.
\]</span></p>
<p>We now prove this claim.</p>
<hr />
</div>
<div id="proof-eckartyoungmirsky-theorem" class="section level4 hasAnchor" number="5.1.2.2">
<h4><span class="header-section-number">5.1.2.2</span> Proof (Eckart–Young–Mirsky Theorem)<a href="principal-component-analysis.html#proof-eckartyoungmirsky-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We will show this in four steps.</p>
<ol style="list-style-type: decimal">
<li>Step 1: The <strong>Frobenius norm</strong> of <span class="math inline">\(\mathbf{X}\)</span> is:</li>
</ol>
<p><span class="math display">\[
\|\mathbf{X}\|_F^2 = \sum_{i=1}^{r} \sigma_i^2.
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Step 2: Any <strong>rank-<span class="math inline">\(k\)</span> approximation</strong> <span class="math inline">\(\mathbf{Y}\)</span> can be written in terms of <strong>some</strong> linear combination of singular vectors:</li>
</ol>
<p><span class="math display">\[
\mathbf{Y} = \sum_{i=1}^{k} a_i \mathbf{u}_i \mathbf{v}_i&#39;.
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Step 3: The residual error</li>
</ol>
<p><span class="math display">\[
\|\mathbf{X} - \mathbf{Y}\|_F^2 = \sum_{i=1}^{k} (\sigma_i - a_i)^2 + \sum_{i=k+1}^{r} \sigma_i^2
\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li>Step 4: The optimal choice is <strong><span class="math inline">\(a_i = \sigma_i\)</span></strong> for the first <span class="math inline">\(k\)</span> terms and <strong><span class="math inline">\(a_i = 0\)</span> for the rest</strong>.</li>
</ol>
<p>For Step 1, we want to show that the <strong>Frobenius norm</strong> of a matrix <span class="math inline">\(\mathbf{X}\)</span> is equal to the sum of the squared singular values:</p>
<p><span class="math display">\[
\|\mathbf{X}\|_F^2 = \sum_{i=1}^{r} \sigma_i^2.
\]</span></p>
<p>The <strong>Frobenius norm</strong> of an <span class="math inline">\(n \times p\)</span> matrix <span class="math inline">\(\mathbf{X}\)</span> is defined as:</p>
<p><span class="math display">\[
\|\mathbf{X}\|_F = \sqrt{\sum_{i=1}^{n} \sum_{j=1}^{p} x_{ij}^2}.
\]</span></p>
<p>Squaring both sides:</p>
<p><span class="math display">\[
\|\mathbf{X}\|_F^2 = \sum_{i=1}^{n} \sum_{j=1}^{p} x_{ij}^2.
\]</span></p>
<p>An alternative way to express the Frobenius norm is using the <strong>trace function</strong>:</p>
<p><span class="math display">\[
\|\mathbf{X}\|_F^2 = \text{Tr}(\mathbf{X}&#39; \mathbf{X}).
\]</span></p>
<p>where <span class="math inline">\(\text{Tr}(\mathbf{A})\)</span> denotes the <strong>trace</strong> (sum of the diagonal elements) of a square matrix <span class="math inline">\(\mathbf{A}\)</span>.</p>
<p>To see this note that</p>
<p><span class="math display">\[
\|\mathbf{X}\|_F^2 = \sum_{i=1}^{n} \sum_{j=1}^{p} x_{ij}^2 = \sum_{j=1}^{p} \sum_{i=1}^{n} x_{ij}^2 = \sum_{j=1}^{p}  {\boldsymbol x} _j&#39;  {\boldsymbol x} _j = \text{Tr}(\mathbf{X}&#39; \mathbf{X}).
\]</span></p>
<p>From the <strong>SVD decomposition</strong>, we write:</p>
<p><span class="math display">\[
\mathbf{X} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}&#39;
\]</span></p>
<p>where:
- <span class="math inline">\(\mathbf{U}\)</span> is an <span class="math inline">\(n \times n\)</span> orthonormal matrix of <strong>left singular vectors</strong>,
- <span class="math inline">\(\mathbf{V}\)</span> is a <span class="math inline">\(p \times p\)</span> orthonormal matrix of <strong>right singular vectors</strong>,
- <span class="math inline">\(\mathbf{\Sigma}\)</span> is an <span class="math inline">\(n \times p\)</span> diagonal matrix of <strong>singular values</strong>:</p>
<p><span class="math display">\[
\mathbf{\Sigma} =
\begin{bmatrix}
\sigma_1 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \sigma_2 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; 0 &amp; \sigma_3 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; \sigma_r \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots
\end{bmatrix}.
\]</span></p>
<p>The rank of <span class="math inline">\(\mathbf{X}\)</span> is <span class="math inline">\(r\)</span>, meaning it has <strong><span class="math inline">\(r\)</span> nonzero singular values</strong>.</p>
<p>Now, compute <span class="math inline">\(\mathbf{X}&#39; \mathbf{X}\)</span>:</p>
<p><span class="math display">\[
\mathbf{X}&#39; \mathbf{X} = (\mathbf{U} \mathbf{\Sigma} \mathbf{V}&#39;)&#39; (\mathbf{U} \mathbf{\Sigma} \mathbf{V}&#39;).
\]</span></p>
<p>Using the transpose property <span class="math inline">\((\mathbf{A} \mathbf{B})&#39; = \mathbf{B}&#39; \mathbf{A}&#39;\)</span>:</p>
<p><span class="math display">\[
\mathbf{X}&#39; \mathbf{X} = \mathbf{V} \mathbf{\Sigma}&#39; \mathbf{U}&#39; \mathbf{U} \mathbf{\Sigma} \mathbf{V}&#39;.
\]</span></p>
<p>Since <span class="math inline">\(\mathbf{U}\)</span> is orthonormal (<span class="math inline">\(\mathbf{U}&#39; \mathbf{U} = \mathbf{I}\)</span>), this simplifies to:</p>
<p><span class="math display">\[
\mathbf{X}&#39; \mathbf{X} = \mathbf{V} \mathbf{\Sigma}&#39; \mathbf{\Sigma} \mathbf{V}&#39;.
\]</span></p>
<p>Since <span class="math inline">\(\mathbf{\Sigma}&#39; \mathbf{\Sigma}\)</span> is a diagonal matrix with squared singular values <span class="math inline">\(\sigma_i^2\)</span>, we get:</p>
<p><span class="math display">\[
\mathbf{X}&#39; \mathbf{X} = \mathbf{V}
\begin{bmatrix}
\sigma_1^2 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \sigma_2^2 &amp; \cdots &amp; 0 \\
0 &amp; 0 &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; \sigma_r^2
\end{bmatrix}
\mathbf{V}&#39;.
\]</span></p>
<p>Taking the <strong>trace</strong> on both sides:</p>
<p><span class="math display">\[
\text{Tr}(\mathbf{X}&#39; \mathbf{X}) = \text{Tr}(\mathbf{V} \mathbf{\Sigma}&#39; \mathbf{\Sigma} \mathbf{V}&#39;).
\]</span></p>
<p>Since the trace is invariant under cyclic permutations:</p>
<p><span class="math display">\[
\text{Tr}(\mathbf{X}&#39; \mathbf{X}) = \text{Tr}( \mathbf{V}&#39; \mathbf{V} \mathbf{\Sigma}&#39; \mathbf{\Sigma}) = \text{Tr}(\mathbf{\Sigma}&#39; \mathbf{\Sigma}).
\]</span></p>
<p>Since <span class="math inline">\(\mathbf{\Sigma}&#39; \mathbf{\Sigma}\)</span> is diagonal, its trace is simply the sum of the diagonal elements:</p>
<p><span class="math display">\[
\text{Tr}(\mathbf{\Sigma}&#39; \mathbf{\Sigma}) = \sum_{i=1}^{r} \sigma_i^2.
\]</span></p>
<p>Since we previously established that:</p>
<p><span class="math display">\[
\|\mathbf{X}\|_F^2 = \text{Tr}(\mathbf{X}&#39; \mathbf{X}),
\]</span></p>
<p>we conclude:</p>
<p><span class="math display">\[
\|\mathbf{X}\|_F^2 = \sum_{i=1}^{r} \sigma_i^2.
\]</span></p>
<p>For Step 2, we claim that any rank-<span class="math inline">\(k\)</span> matrix <span class="math inline">\(\mathbf{Y}\)</span> can be expressed as:</p>
<p><span class="math display">\[
\mathbf{Y} = \sum_{i=1}^{k} a_i \mathbf{u}_i \mathbf{v}_i&#39;.
\]</span></p>
<p>This follows from the fact that the outer products of the singular vectors,</p>
<p><span class="math display">\[
\mathbf{u}_i \mathbf{v}_i&#39;
\]</span></p>
<p>form an <strong>orthonormal basis</strong> for matrices in <span class="math inline">\(\mathbb{R}^{n \times p}\)</span>, meaning that any rank-<span class="math inline">\(k\)</span> matrix can be expressed as a linear combination of at most <span class="math inline">\(k\)</span> of these elements.</p>
<p>To see this, consider the inner product between two <strong>singular vector outer products</strong>, defined as:</p>
<p><span class="math display">\[
\langle \mathbf{u}_i \mathbf{v}_i&#39;, \mathbf{u}_j \mathbf{v}_j&#39; \rangle_F.
\]</span></p>
<p>By the <strong>Frobenius inner product</strong>, the inner product between two matrices <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{B}\)</span> is:</p>
<p><span class="math display">\[
\langle \mathbf{A}, \mathbf{B} \rangle_F = \text{Tr}(\mathbf{A}&#39; \mathbf{B}).
\]</span></p>
<p>Applying this to the singular vectors:</p>
<p><span class="math display">\[
\langle \mathbf{u}_i \mathbf{v}_i&#39;, \mathbf{u}_j \mathbf{v}_j&#39; \rangle_F = \text{Tr} \big( (\mathbf{u}_i \mathbf{v}_i&#39;)&#39; (\mathbf{u}_j \mathbf{v}_j&#39;) \big).
\]</span></p>
<p>Using the transpose property <span class="math inline">\((\mathbf{A} \mathbf{B})&#39; = \mathbf{B}&#39; \mathbf{A}&#39;\)</span>, we get:</p>
<p><span class="math display">\[
\langle \mathbf{u}_i \mathbf{v}_i&#39;, \mathbf{u}_j \mathbf{v}_j&#39; \rangle_F = \text{Tr} \big( \mathbf{v}_i \mathbf{u}_i&#39; \mathbf{u}_j \mathbf{v}_j&#39; \big).
\]</span></p>
<p>Since <span class="math inline">\(\mathbf{U}\)</span> and <span class="math inline">\(\mathbf{V}\)</span> are <strong>orthonormal</strong>, we use:</p>
<p><span class="math display">\[
\mathbf{u}_i&#39; \mathbf{u}_j = \delta_{ij}, \quad \mathbf{v}_i&#39; \mathbf{v}_j = \delta_{ij}.
\]</span></p>
<p>where <span class="math inline">\(\delta_{ij} = 1\)</span>, if <span class="math inline">\(i=j\)</span> and <span class="math inline">\(\delta_{ij} = 0\)</span>, if <span class="math inline">\(i \neq j\)</span>.</p>
<p>Thus,</p>
<p><span class="math display">\[
\langle \mathbf{u}_i \mathbf{v}_i&#39;, \mathbf{u}_j \mathbf{v}_j&#39; \rangle_F = \delta_{ij}.
\]</span></p>
<p>This proves that <span class="math inline">\(\{ \mathbf{u}_i \mathbf{v}_i&#39; \}_{i=1}^{k}\)</span> are <strong>orthonormal under the Frobenius inner product</strong>, therefore linearly independent and forming an spanning bases for a rank <span class="math inline">\(k\)</span> matrix.</p>
<p>For Step 3, mote that the <strong>error matrix</strong> is:</p>
<p><span class="math display">\[
\mathbf{X} - \mathbf{Y} = \sum_{i=1}^{r} \sigma_i \mathbf{u}_i \mathbf{v}_i&#39; - \sum_{i=1}^{k} a_i \mathbf{u}_i \mathbf{v}_i&#39;.
\]</span></p>
<p>Rewriting this sum:</p>
<p><span class="math display">\[
\mathbf{X} - \mathbf{Y} = \sum_{i=1}^{k} (\sigma_i - a_i) \mathbf{u}_i \mathbf{v}_i&#39; + \sum_{i=k+1}^{r} \sigma_i \mathbf{u}_i \mathbf{v}_i&#39;.
\]</span></p>
<p>Since the <strong>singular vector outer products</strong> <span class="math inline">\(\mathbf{u}_i \mathbf{v}_i&#39;\)</span> form an <strong>orthonormal basis</strong> under the Frobenius norm, the squared Frobenius norm of a sum of such terms is the <strong>sum of the squared coefficients</strong>:</p>
<p><span class="math display">\[
\|\mathbf{X} - \mathbf{Y}\|_F^2 = \sum_{i=1}^{k} (\sigma_i - a_i)^2 + \sum_{i=k+1}^{r} \sigma_i^2.
\]</span></p>
<p>Finally, in Step 4, we need to minimize the approximation error:</p>
<p><span class="math display">\[
\sum_{i=1}^{k} (\sigma_i - a_i)^2 + \sum_{i=k+1}^{r} \sigma_i^2,
\]</span></p>
<p>the best choice is clearly:</p>
<p><span class="math display">\[
a_i = \sigma_i, \quad \text{for } i = 1, \dots, k.
\]</span></p>
<p>Thus, the <strong>best rank-<span class="math inline">\(k\)</span> approximation</strong> is:</p>
<p><span class="math display">\[
\mathbf{X}_k = \sum_{i=1}^{k} \sigma_i \mathbf{u}_i \mathbf{v}_i&#39;.
\]</span></p>
<p>This minimizes the error:</p>
<p><span class="math display">\[
\|\mathbf{X} - \mathbf{X}_k\|_F^2 = \sum_{i=k+1}^{r} \sigma_i^2.
\]</span></p>
<p>which confirms the <strong>Eckart–Young–Mirsky theorem</strong>.</p>
<hr />
</div>
</div>
<div id="eckartyoungmirsky-theorem-for-the-spectral-norm" class="section level3 hasAnchor" number="5.1.3">
<h3><span class="header-section-number">5.1.3</span> Eckart–Young–Mirsky Theorem for the Spectral Norm<a href="principal-component-analysis.html#eckartyoungmirsky-theorem-for-the-spectral-norm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>Eckart–Young–Mirsky theorem</strong> is a fundamental result in matrix approximation, stating that the <strong>best rank-k approximation</strong> of a matrix in terms of the <strong>spectral norm</strong> is obtained through its <strong>singular value decomposition (SVD)</strong>. However, unlike in the Frobenius norm case, the solution is not always unique in the spectral norm.</p>
<hr />
<div id="spectral-norm-theorem-statement" class="section level4 hasAnchor" number="5.1.3.1">
<h4><span class="header-section-number">5.1.3.1</span> Spectral Norm Theorem Statement<a href="principal-component-analysis.html#spectral-norm-theorem-statement" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For any matrix <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times p}\)</span> with singular values <span class="math inline">\(\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r &gt; 0\)</span>, the rank-k matrix <span class="math inline">\(\mathbf{X}_k\)</span> that minimizes the spectral norm error <span class="math inline">\(\|\mathbf{X} - \mathbf{Y}\|_2\)</span> is given by:</p>
<p><span class="math display">\[
\mathbf{X}_k = \sum_{i=1}^k \sigma_i \mathbf{u}_i \mathbf{v}_i&#39;,
\]</span></p>
<p>where <span class="math inline">\(\mathbf{u}_i\)</span> and <span class="math inline">\(\mathbf{v}_i\)</span> are the left and right singular vectors corresponding to <span class="math inline">\(\sigma_i\)</span>.</p>
<p>The minimal spectral norm error is:</p>
<p><span class="math display">\[
\|\mathbf{X} - \mathbf{X}_k\|_2 = \sigma_{k+1}.
\]</span></p>
<hr />
</div>
<div id="spectral-norm-proof-sketch" class="section level4 hasAnchor" number="5.1.3.2">
<h4><span class="header-section-number">5.1.3.2</span> Spectral Norm Proof Sketch<a href="principal-component-analysis.html#spectral-norm-proof-sketch" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The spectral norm of a matrix is its largest singular value. Given any rank-k matrix <span class="math inline">\(\mathbf{Y}\)</span> written as before <span class="math inline">\(\mathbf{Y} = \sum_{i=1}^{k} a_i \mathbf{u}_i \mathbf{v}_i&#39;\)</span>, its singular values are <span class="math inline">\(a_1, a_2, \dots, a_k\)</span>. The error matrix <span class="math inline">\(\mathbf{X} - \mathbf{Y}\)</span> has singular values:</p>
<p><span class="math display">\[
\{ |\sigma_1 - a_1|, |\sigma_2 - a_2|, \dots, |\sigma_k - a_k|, \sigma_{k+1}, \dots, \sigma_r \}.
\]</span></p>
<p>To minimize the spectral norm of the error, one must ensure that:</p>
<p><span class="math display">\[
\max \{ |\sigma_1 - a_1|, |\sigma_2 - a_2|, \dots, \sigma_{k+1} \}
\]</span></p>
<p>is as small as possible. A way to achieve the minimal error <span class="math inline">\(\sigma_{k+1}\)</span> is to set <span class="math inline">\(a_i = \sigma_i\)</span> for <span class="math inline">\(i=1, \dots, k\)</span>, as with the spectral norm, but we can construct different rank-k matrices that also achieve this value.</p>
<hr />
</div>
<div id="non-uniqueness-for-the-spectral-norm" class="section level4 hasAnchor" number="5.1.3.3">
<h4><span class="header-section-number">5.1.3.3</span> Non-Uniqueness for the Spectral Norm<a href="principal-component-analysis.html#non-uniqueness-for-the-spectral-norm" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Unlike in the Frobenius norm case, where the truncated SVD solution is unique, the spectral norm allows for <strong>multiple solutions</strong>. If we require:</p>
<p><span class="math display">\[
|\sigma_i - a_i| \leq \sigma_{k+1}, \quad \forall i=1, \dots, k,
\]</span></p>
<p>any such choice of <span class="math inline">\(a_i\)</span> will yield the same spectral norm error <span class="math inline">\(\sigma_{k+1}\)</span>. This provides a class of rank-k approximations that achieve the minimal spectral norm error.</p>
<hr />
<div id="how-to-find-alternative-solutions" class="section level5 hasAnchor" number="5.1.3.3.1">
<h5><span class="header-section-number">5.1.3.3.1</span> How to Find Alternative Solutions<a href="principal-component-analysis.html#how-to-find-alternative-solutions" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>To find alternative rank-k approximations with the same spectral norm error, one can:</p>
<ol style="list-style-type: decimal">
<li><strong>Perturb Singular Values</strong>: Adjust <span class="math inline">\(a_i\)</span> such that <span class="math inline">\(|\sigma_i - a_i| \leq \sigma_{k+1}\)</span>, ensuring no singular value difference exceeds <span class="math inline">\(\sigma_{k+1}\)</span>.</li>
<li><strong>Linear Combinations of Singular Vectors</strong>: Combine singular vectors corresponding to equal singular values to create different approximations.</li>
</ol>
<hr />
</div>
</div>
<div id="conclusion-for-the-spectral-norm" class="section level4 hasAnchor" number="5.1.3.4">
<h4><span class="header-section-number">5.1.3.4</span> Conclusion for the Spectral Norm<a href="principal-component-analysis.html#conclusion-for-the-spectral-norm" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <strong>Eckart–Young–Mirsky theorem</strong> guarantees the minimal spectral norm error with the truncated SVD solution. However, in cases of spectral norm approximation, the solution is <strong>not unique</strong>. By carefully choosing singular values and vectors within the specified bounds, one can construct <strong>alternative solutions</strong> that achieve the same minimal error.</p>
<p>This flexibility in spectral norm approximation highlights the subtle difference between approximations in spectral and Frobenius norms, making it an important consideration in applications like low-rank matrix approximation, data compression, and principal component analysis.</p>
</div>
</div>
</div>
<div id="variance-maximization-in-pca" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Variance Maximization in PCA<a href="principal-component-analysis.html#variance-maximization-in-pca" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Principal Component Analysis (PCA) is traditionally introduced as a method that finds directions (principal components) along which the variance of the data is maximized.</p>
<p>Given a data matrix <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times p}\)</span> with centered columns, PCA seeks an orthonormal set of vectors <span class="math inline">\(\{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_p\}\)</span> such that:</p>
<ul>
<li>The first principal component <span class="math inline">\(\mathbf{v}_1\)</span> maximizes the variance:
<span class="math display">\[
\mathbf{v}_1 = \arg \max_{\|\mathbf{v}\|=1} \text{Var}(\mathbf{X}\mathbf{v}) = \arg \max_{\|\mathbf{v}\|=1} \|\mathbf{X}\mathbf{v}\|_2^2.
\]</span></li>
<li>Subsequent principal components are chosen orthogonal to the previous ones and also maximize the variance.</li>
</ul>
<p>Each principal component corresponds to a singular vector from the Singular Value Decomposition (SVD) of <span class="math inline">\(\mathbf{X}\)</span>:
<span class="math display">\[
\mathbf{X} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}&#39;,
\]</span>
where <span class="math inline">\(\mathbf{\Sigma}\)</span> contains the singular values <span class="math inline">\(\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_r &gt; 0\)</span>.</p>
<p>The first <span class="math inline">\(k\)</span> principal components are the first <span class="math inline">\(k\)</span> columns of <span class="math inline">\(\mathbf{V}\)</span>:
<span class="math display">\[
\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k,
\]</span>
with the corresponding principal component scores given by:
<span class="math display">\[
\mathbf{X} \mathbf{v}_i = \sigma_i \mathbf{u}_i, \quad i = 1, 2, \dots, k.
\]</span></p>
<hr />
<div id="first-principal-cmponent" class="section level3 hasAnchor" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> First Principal Cmponent<a href="principal-component-analysis.html#first-principal-cmponent" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We aim to find the direction <span class="math inline">\(\mathbf{v}_1 \in \mathbb{R}^p\)</span> (a unit vector, <span class="math inline">\(\| \mathbf{v}_1 \| = 1\)</span>) that maximizes the <strong>variance</strong> of the data when projected onto <span class="math inline">\(\mathbf{v}_1\)</span>.</p>
<p>The dataset is represented by <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times p}\)</span> (with centered columns), and the projection of <span class="math inline">\(\mathbf{X}\)</span> onto <span class="math inline">\(\mathbf{v}_1\)</span> is given by <span class="math inline">\(\mathbf{Xv}_1\)</span>.</p>
<hr />
<div id="variance-of-the-projection" class="section level4 hasAnchor" number="5.2.1.1">
<h4><span class="header-section-number">5.2.1.1</span> Variance of the Projection<a href="principal-component-analysis.html#variance-of-the-projection" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The variance of the projected data <span class="math inline">\(\mathbf{Xv}_1\)</span> is:</p>
<p><span class="math display">\[
\text{Var}(\mathbf{Xv}_1) = \frac{1}{n} \| \mathbf{Xv}_1 \|_2^2 = \frac{1}{n} (\mathbf{Xv}_1)&#39;(\mathbf{Xv}_1).
\]</span></p>
<p>Expanding this:</p>
<p><span class="math display">\[
\text{Var}(\mathbf{Xv}_1) = \frac{1}{n} \mathbf{v}_1&#39; \mathbf{X}&#39; \mathbf{X} \mathbf{v}_1.
\]</span></p>
<p>Since <span class="math inline">\(\frac{1}{n} \mathbf{X}&#39; \mathbf{X}\)</span> is the <strong>sample covariance matrix</strong> <span class="math inline">\(\mathbf{S}\)</span>, we rewrite the objective as:</p>
<p><span class="math display">\[
\text{maximize } \mathbf{v}_1&#39; \mathbf{S} \mathbf{v}_1 \quad \text{subject to} \quad \| \mathbf{v}_1 \| = 1.
\]</span></p>
<hr />
</div>
<div id="optimization-problem-rayleigh-quotient" class="section level4 hasAnchor" number="5.2.1.2">
<h4><span class="header-section-number">5.2.1.2</span> Optimization Problem (Rayleigh Quotient)<a href="principal-component-analysis.html#optimization-problem-rayleigh-quotient" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This is a constrained optimization problem that can be solved using the method of <strong>Lagrange multipliers</strong>:</p>
<p><span class="math display">\[
\mathcal{L}(\mathbf{v}_1, \lambda) = \mathbf{v}_1&#39; \mathbf{S} \mathbf{v}_1 - \lambda (\mathbf{v}_1&#39; \mathbf{v}_1 - 1).
\]</span></p>
<p>Taking the derivative with respect to <span class="math inline">\(\mathbf{v}_1\)</span> and setting it to zero gives:</p>
<p><span class="math display">\[
\frac{\partial \mathcal{L}}{\partial \mathbf{v}_1} = 2 \mathbf{S} \mathbf{v}_1 - 2 \lambda \mathbf{v}_1 = 0.
\]</span></p>
<p>Thus:</p>
<p><span class="math display">\[
\mathbf{S} \mathbf{v}_1 = \lambda \mathbf{v}_1.
\]</span></p>
<hr />
</div>
<div id="eigenvalue-problem" class="section level4 hasAnchor" number="5.2.1.3">
<h4><span class="header-section-number">5.2.1.3</span> Eigenvalue Problem<a href="principal-component-analysis.html#eigenvalue-problem" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This is an <strong>eigenvalue problem</strong> where <span class="math inline">\(\lambda\)</span> is an eigenvalue of <span class="math inline">\(\mathbf{S}\)</span> and <span class="math inline">\(\mathbf{v}_1\)</span> is the corresponding eigenvector. The direction <span class="math inline">\(\mathbf{v}_1\)</span> that maximizes the variance is the <strong>eigenvector corresponding to the largest eigenvalue</strong> <span class="math inline">\(\lambda_1\)</span>.</p>
<p>Hence, the <strong>first principal component</strong> <span class="math inline">\(\mathbf{v}_1\)</span> is the eigenvector of <span class="math inline">\(\mathbf{S}\)</span> associated with the largest eigenvalue <span class="math inline">\(\lambda_1\)</span>, and the <strong>maximum variance</strong> is <span class="math inline">\(\lambda_1\)</span>.</p>
<hr />
</div>
<div id="connection-to-svd" class="section level4 hasAnchor" number="5.2.1.4">
<h4><span class="header-section-number">5.2.1.4</span> Connection to SVD<a href="principal-component-analysis.html#connection-to-svd" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Recall that the <strong>SVD</strong> of <span class="math inline">\(\mathbf{X}\)</span> is:</p>
<p><span class="math display">\[
\mathbf{X} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}&#39;.
\]</span></p>
<p>The sample covariance matrix <span class="math inline">\(\mathbf{S} = \frac{1}{n} \mathbf{X}&#39; \mathbf{X}\)</span> can be written as:</p>
<p><span class="math display">\[
\mathbf{S} = \frac{1}{n} \mathbf{V} \mathbf{\Sigma}&#39; \mathbf{U}&#39; \mathbf{U} \mathbf{\Sigma} \mathbf{V}&#39; = \frac{1}{n} \mathbf{V} \mathbf{\Sigma}^2 \mathbf{V}&#39;.
\]</span></p>
<p>The eigenvectors of <span class="math inline">\(\mathbf{S}\)</span> are the columns of <span class="math inline">\(\mathbf{V}\)</span>, and the eigenvalues are the squared singular values <span class="math inline">\(\sigma_i^2\)</span> divided by <span class="math inline">\(n\)</span>.</p>
<p>Thus:
- The <strong>first principal component direction</strong> <span class="math inline">\(\mathbf{v}_1\)</span> is the first column of <span class="math inline">\(\mathbf{V}\)</span> from the SVD.
- The <strong>variance explained</strong> by the first principal component is <span class="math inline">\(\frac{\sigma_1^2}{n}\)</span>.</p>
<hr />
</div>
</div>
<div id="second-principal-component" class="section level3 hasAnchor" number="5.2.2">
<h3><span class="header-section-number">5.2.2</span> Second Principal Component<a href="principal-component-analysis.html#second-principal-component" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>second principal component</strong> <span class="math inline">\(\mathbf{v}_2\)</span> is the direction that:
- Maximizes the variance of the projected data <span class="math inline">\(\mathbf{Xv}_2\)</span>,
- Subject to being <strong>orthogonal</strong> to the first principal component <span class="math inline">\(\mathbf{v}_1\)</span>.</p>
<hr />
<div id="optimization-problem-1" class="section level4 hasAnchor" number="5.2.2.1">
<h4><span class="header-section-number">5.2.2.1</span> Optimization Problem<a href="principal-component-analysis.html#optimization-problem-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We want to maximize:
<span class="math display">\[
\text{Var}(\mathbf{Xv}_2) = \mathbf{v}_2&#39; \mathbf{S} \mathbf{v}_2
\]</span>
subject to:
<span class="math display">\[
\|\mathbf{v}_2\| = 1 \quad \text{and} \quad \mathbf{v}_2&#39; \mathbf{v}_1 = 0.
\]</span></p>
<hr />
</div>
<div id="solution-using-lagrange-multipliers" class="section level4 hasAnchor" number="5.2.2.2">
<h4><span class="header-section-number">5.2.2.2</span> Solution Using Lagrange Multipliers<a href="principal-component-analysis.html#solution-using-lagrange-multipliers" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The constraint <span class="math inline">\(\mathbf{v}_2&#39; \mathbf{v}_1 = 0\)</span> ensures orthogonality to the first principal component.</p>
<p>Solving this optimization problem leads to:
<span class="math display">\[
\mathbf{S} \mathbf{v}_2 = \lambda_2 \mathbf{v}_2
\]</span>
where <span class="math inline">\(\mathbf{v}_2\)</span> is the <strong>eigenvector associated with the second largest eigenvalue</strong> <span class="math inline">\(\lambda_2\)</span> of <span class="math inline">\(\mathbf{S}\)</span>.</p>
<hr />
</div>
<div id="intuition" class="section level4 hasAnchor" number="5.2.2.3">
<h4><span class="header-section-number">5.2.2.3</span> Intuition<a href="principal-component-analysis.html#intuition" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Since the first principal component <span class="math inline">\(\mathbf{v}_1\)</span> has already captured the maximum variance, the second principal component <span class="math inline">\(\mathbf{v}_2\)</span> captures the next largest amount of variance while being orthogonal to <span class="math inline">\(\mathbf{v}_1\)</span>. We can continue in this way to find the next Principal Components.</p>
</div>
</div>
</div>
<div id="parafac-decomposition" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> PARAFAC decomposition<a href="principal-component-analysis.html#parafac-decomposition" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <strong>PARAFAC decomposition</strong> (also known as CANDECOMP/PARAFAC or CP decomposition) is a natural extension of the concept of <strong>Principal Component Analysis (PCA)</strong> from matrices to <strong>higher-order tensors</strong>.</p>
<hr />
<div id="what-is-a-tensor" class="section level3 hasAnchor" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> What is a Tensor?<a href="principal-component-analysis.html#what-is-a-tensor" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A <strong>tensor</strong> is a multi-dimensional array, extending the concept of vectors (1D arrays) and matrices (2D arrays) to higher dimensions:
- A <strong>vector</strong> is a tensor of order 1.
- A <strong>matrix</strong> is a tensor of order 2.
- A <strong>tensor</strong> of order <span class="math inline">\(N\)</span> has <span class="math inline">\(N\)</span> dimensions (also called modes).</p>
<p>For example:
- An image is a matrix (2D tensor).
- A color video can be represented as a 3D tensor (height × width × time).</p>
<hr />
</div>
<div id="motivation-why-generalize-pca-to-tensors" class="section level3 hasAnchor" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Motivation: Why Generalize PCA to Tensors?<a href="principal-component-analysis.html#motivation-why-generalize-pca-to-tensors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>PCA works well with matrices, but many real-world data are naturally represented as tensors:
- <strong>Psychology</strong>: Data from multiple individuals across multiple tests and times.
- <strong>Signal Processing</strong>: Multi-channel EEG data recorded over time.
- <strong>Chemometrics</strong>: Spectral data collected over different conditions.</p>
<p>PCA provides a low-rank approximation for matrices by decomposing them into principal components. The PARAFAC decomposition extends this idea to tensors, finding components along each dimension simultaneously.</p>
<hr />
</div>
<div id="parafac-decomposition-definition" class="section level3 hasAnchor" number="5.3.3">
<h3><span class="header-section-number">5.3.3</span> PARAFAC Decomposition Definition<a href="principal-component-analysis.html#parafac-decomposition-definition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Given a tensor <span class="math inline">\(\mathcal{X} \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N}\)</span>, the <strong>PARAFAC decomposition</strong> expresses <span class="math inline">\(\mathcal{X}\)</span> as a sum of <span class="math inline">\(R\)</span> rank-1 tensors:</p>
<p><span class="math display">\[
\mathcal{X} \approx \sum_{r=1}^{R} \mathbf{a}_r^{(1)} \circ \mathbf{a}_r^{(2)} \circ \cdots \circ \mathbf{a}_r^{(N)},
\]</span></p>
<p>where:
- <span class="math inline">\(R\)</span> is the <strong>rank</strong> of the decomposition.
- <span class="math inline">\(\mathbf{a}_r^{(n)} \in \mathbb{R}^{I_n}\)</span> are vectors along the <span class="math inline">\(n\)</span>-th mode.
- <span class="math inline">\(\circ\)</span> denotes the <strong>outer product</strong>.
- Each rank-1 tensor <span class="math inline">\(\mathbf{a}_r^{(1)} \circ \mathbf{a}_r^{(2)} \circ \cdots \circ \mathbf{a}_r^{(N)}\)</span> is analogous to a <strong>principal component</strong> in PCA.</p>
<hr />
</div>
<div id="how-is-parafac-related-to-pca" class="section level3 hasAnchor" number="5.3.4">
<h3><span class="header-section-number">5.3.4</span> How is PARAFAC Related to PCA?<a href="principal-component-analysis.html#how-is-parafac-related-to-pca" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p><strong>PCA</strong> decomposes a matrix <span class="math inline">\(\mathbf{X}\)</span> into a sum of rank-1 matrices:
<span class="math display">\[
\mathbf{X} \approx \sum_{r=1}^{R} \sigma_r \mathbf{u}_r \mathbf{v}_r&#39;,
\]</span>
where <span class="math inline">\(\mathbf{u}_r\)</span> and <span class="math inline">\(\mathbf{v}_r\)</span> are left and right singular vectors, and <span class="math inline">\(\sigma_r\)</span> are singular values.</p></li>
<li><p><strong>PARAFAC</strong> generalizes this concept by decomposing a tensor <span class="math inline">\(\mathcal{X}\)</span> into a sum of rank-1 tensors. Just like PCA finds directions that capture the most variance in a matrix, PARAFAC finds <strong>multi-dimensional factors</strong> that capture the most structure in a tensor.</p></li>
</ul>
<hr />
</div>
<div id="key-properties-of-parafac-decomposition" class="section level3 hasAnchor" number="5.3.5">
<h3><span class="header-section-number">5.3.5</span> Key Properties of PARAFAC Decomposition<a href="principal-component-analysis.html#key-properties-of-parafac-decomposition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><strong>Uniqueness</strong>: Unlike PCA (where different rotations of principal components can yield the same solution), the PARAFAC decomposition is unique under mild conditions. This uniqueness makes PARAFAC particularly useful for interpretability.</li>
<li><strong>Low-rank Approximation</strong>: PARAFAC provides a low-rank approximation of tensors, analogous to PCA for matrices.</li>
<li><strong>Interpretability</strong>: Each component can be interpreted as a factor along each mode, making it valuable in fields like chemometrics and neuroscience.</li>
</ul>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-regression.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
