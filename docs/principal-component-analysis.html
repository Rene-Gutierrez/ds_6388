<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Principal Component Analysis | DS 6388 Spring 2025</title>
  <meta name="description" content="5 Principal Component Analysis | DS 6388 Spring 2025" />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Principal Component Analysis | DS 6388 Spring 2025" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Principal Component Analysis | DS 6388 Spring 2025" />
  
  
  

<meta name="author" content="Rene Gutierrez University of Texas at El Paso" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-regression.html"/>
<link rel="next" href="factor-analysis.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> DS 6388: Multivariate Statistical Methods for High-dimensional Data</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#calendar"><i class="fa fa-check"></i><b>1.1</b> Calendar</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#importatnt-dates"><i class="fa fa-check"></i><b>1.1.1</b> Importatnt Dates</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#class-schedule"><i class="fa fa-check"></i><b>1.1.2</b> Class Schedule</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>2</b> Prerequisites</a>
<ul>
<li class="chapter" data-level="2.1" data-path="prerequisites.html"><a href="prerequisites.html#linear-algebra"><i class="fa fa-check"></i><b>2.1</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="prerequisites.html"><a href="prerequisites.html#linear-independence"><i class="fa fa-check"></i><b>2.1.1</b> Linear Independence</a></li>
<li class="chapter" data-level="2.1.2" data-path="prerequisites.html"><a href="prerequisites.html#column-space-of-a-matrix"><i class="fa fa-check"></i><b>2.1.2</b> Column Space of a Matrix</a></li>
<li class="chapter" data-level="2.1.3" data-path="prerequisites.html"><a href="prerequisites.html#rank-of-a-matrix"><i class="fa fa-check"></i><b>2.1.3</b> Rank of a Matrix</a></li>
<li class="chapter" data-level="2.1.4" data-path="prerequisites.html"><a href="prerequisites.html#full-rank-matrix"><i class="fa fa-check"></i><b>2.1.4</b> Full Rank Matrix</a></li>
<li class="chapter" data-level="2.1.5" data-path="prerequisites.html"><a href="prerequisites.html#inverse-matrix"><i class="fa fa-check"></i><b>2.1.5</b> Inverse Matrix</a></li>
<li class="chapter" data-level="2.1.6" data-path="prerequisites.html"><a href="prerequisites.html#positive-definite-matrix"><i class="fa fa-check"></i><b>2.1.6</b> Positive Definite Matrix</a></li>
<li class="chapter" data-level="2.1.7" data-path="prerequisites.html"><a href="prerequisites.html#singular-value-decomposition"><i class="fa fa-check"></i><b>2.1.7</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="2.1.8" data-path="prerequisites.html"><a href="prerequisites.html#eigendecomposition"><i class="fa fa-check"></i><b>2.1.8</b> Eigendecomposition</a></li>
<li class="chapter" data-level="2.1.9" data-path="prerequisites.html"><a href="prerequisites.html#idempotent-matrix"><i class="fa fa-check"></i><b>2.1.9</b> Idempotent Matrix</a></li>
<li class="chapter" data-level="2.1.10" data-path="prerequisites.html"><a href="prerequisites.html#determinant-of-a-matrix"><i class="fa fa-check"></i><b>2.1.10</b> Determinant of a Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="prerequisites.html"><a href="prerequisites.html#calculus"><i class="fa fa-check"></i><b>2.2</b> Calculus</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="prerequisites.html"><a href="prerequisites.html#gradient"><i class="fa fa-check"></i><b>2.2.1</b> Gradient</a></li>
<li class="chapter" data-level="2.2.2" data-path="prerequisites.html"><a href="prerequisites.html#hessian-matrix"><i class="fa fa-check"></i><b>2.2.2</b> Hessian Matrix</a></li>
<li class="chapter" data-level="2.2.3" data-path="prerequisites.html"><a href="prerequisites.html#applications-1"><i class="fa fa-check"></i><b>2.2.3</b> Applications:</a></li>
<li class="chapter" data-level="2.2.4" data-path="prerequisites.html"><a href="prerequisites.html#matrix-calculus"><i class="fa fa-check"></i><b>2.2.4</b> Matrix Calculus</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="prerequisites.html"><a href="prerequisites.html#probability"><i class="fa fa-check"></i><b>2.3</b> Probability</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="prerequisites.html"><a href="prerequisites.html#expected-value"><i class="fa fa-check"></i><b>2.3.1</b> Expected Value</a></li>
<li class="chapter" data-level="2.3.2" data-path="prerequisites.html"><a href="prerequisites.html#variance"><i class="fa fa-check"></i><b>2.3.2</b> Variance</a></li>
<li class="chapter" data-level="2.3.3" data-path="prerequisites.html"><a href="prerequisites.html#cross-covariance-matrix"><i class="fa fa-check"></i><b>2.3.3</b> Cross-Covariance Matrix</a></li>
<li class="chapter" data-level="2.3.4" data-path="prerequisites.html"><a href="prerequisites.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>2.3.4</b> Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="2.3.5" data-path="prerequisites.html"><a href="prerequisites.html#chi2-distribution"><i class="fa fa-check"></i><b>2.3.5</b> <span class="math inline">\(\chi^2\)</span> Distribution</a></li>
<li class="chapter" data-level="2.3.6" data-path="prerequisites.html"><a href="prerequisites.html#t-distribution"><i class="fa fa-check"></i><b>2.3.6</b> <span class="math inline">\(t\)</span> Distribution</a></li>
<li class="chapter" data-level="2.3.7" data-path="prerequisites.html"><a href="prerequisites.html#f-distribution"><i class="fa fa-check"></i><b>2.3.7</b> <span class="math inline">\(F\)</span> Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="prerequisites.html"><a href="prerequisites.html#statistics"><i class="fa fa-check"></i><b>2.4</b> Statistics</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="prerequisites.html"><a href="prerequisites.html#bias-of-an-estimator"><i class="fa fa-check"></i><b>2.4.1</b> Bias of an Estimator</a></li>
<li class="chapter" data-level="2.4.2" data-path="prerequisites.html"><a href="prerequisites.html#unbiased-estimator"><i class="fa fa-check"></i><b>2.4.2</b> Unbiased Estimator</a></li>
<li class="chapter" data-level="2.4.3" data-path="prerequisites.html"><a href="prerequisites.html#mean-square-error-of-an-estimator"><i class="fa fa-check"></i><b>2.4.3</b> Mean Square Error of an Estimator</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>3</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introduction.html"><a href="introduction.html#key-challenges"><i class="fa fa-check"></i><b>3.1</b> Key Challenges</a></li>
<li class="chapter" data-level="3.2" data-path="introduction.html"><a href="introduction.html#core-concepts"><i class="fa fa-check"></i><b>3.2</b> Core Concepts</a></li>
<li class="chapter" data-level="3.3" data-path="introduction.html"><a href="introduction.html#applications-4"><i class="fa fa-check"></i><b>3.3</b> Applications</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>4</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="linear-regression.html"><a href="linear-regression.html#machine-learning"><i class="fa fa-check"></i><b>4.1</b> Machine Learning</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="linear-regression.html"><a href="linear-regression.html#ordinary-least-squares"><i class="fa fa-check"></i><b>4.1.1</b> Ordinary Least Squares</a></li>
<li class="chapter" data-level="4.1.2" data-path="linear-regression.html"><a href="linear-regression.html#ridge-regression"><i class="fa fa-check"></i><b>4.1.2</b> Ridge Regression</a></li>
<li class="chapter" data-level="4.1.3" data-path="linear-regression.html"><a href="linear-regression.html#lasso-regression"><i class="fa fa-check"></i><b>4.1.3</b> Lasso Regression</a></li>
<li class="chapter" data-level="4.1.4" data-path="linear-regression.html"><a href="linear-regression.html#elastic-net"><i class="fa fa-check"></i><b>4.1.4</b> Elastic Net</a></li>
<li class="chapter" data-level="4.1.5" data-path="linear-regression.html"><a href="linear-regression.html#elastic-net-as-a-mixed-penalty"><i class="fa fa-check"></i><b>4.1.5</b> <strong>Elastic Net as a Mixed Penalty</strong></a></li>
<li class="chapter" data-level="4.1.6" data-path="linear-regression.html"><a href="linear-regression.html#other-options-of-regularization"><i class="fa fa-check"></i><b>4.1.6</b> Other Options of Regularization</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-linear-regression"><i class="fa fa-check"></i><b>4.2</b> Bayesian Linear Regression</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="linear-regression.html"><a href="linear-regression.html#basic-bayesian-linear-regression"><i class="fa fa-check"></i><b>4.2.1</b> Basic Bayesian Linear Regression</a></li>
<li class="chapter" data-level="4.2.2" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-lasso-regression"><i class="fa fa-check"></i><b>4.2.2</b> Bayesian Lasso Regression</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="linear-regression.html"><a href="linear-regression.html#computational-comparisson"><i class="fa fa-check"></i><b>4.3</b> Computational Comparisson</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="linear-regression.html"><a href="linear-regression.html#set-up"><i class="fa fa-check"></i><b>4.3.1</b> Set-Up</a></li>
<li class="chapter" data-level="4.3.2" data-path="linear-regression.html"><a href="linear-regression.html#simulation"><i class="fa fa-check"></i><b>4.3.2</b> Simulation</a></li>
<li class="chapter" data-level="4.3.3" data-path="linear-regression.html"><a href="linear-regression.html#ols"><i class="fa fa-check"></i><b>4.3.3</b> OLS</a></li>
<li class="chapter" data-level="4.3.4" data-path="linear-regression.html"><a href="linear-regression.html#ridge-regression-1"><i class="fa fa-check"></i><b>4.3.4</b> Ridge Regression</a></li>
<li class="chapter" data-level="4.3.5" data-path="linear-regression.html"><a href="linear-regression.html#lasso"><i class="fa fa-check"></i><b>4.3.5</b> Lasso</a></li>
<li class="chapter" data-level="4.3.6" data-path="linear-regression.html"><a href="linear-regression.html#basic-bayesian-regression"><i class="fa fa-check"></i><b>4.3.6</b> Basic Bayesian Regression</a></li>
<li class="chapter" data-level="4.3.7" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-lasso"><i class="fa fa-check"></i><b>4.3.7</b> Bayesian Lasso</a></li>
<li class="chapter" data-level="4.3.8" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-horseshoe-prior"><i class="fa fa-check"></i><b>4.3.8</b> Bayesian Horseshoe Prior</a></li>
<li class="chapter" data-level="4.3.9" data-path="linear-regression.html"><a href="linear-regression.html#results-comparisson"><i class="fa fa-check"></i><b>4.3.9</b> Results Comparisson</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="linear-regression.html"><a href="linear-regression.html#efficient-computation"><i class="fa fa-check"></i><b>4.4</b> Efficient Computation</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="linear-regression.html"><a href="linear-regression.html#efficient-computation-of-ridge-regression-using-the-woodbury-identity"><i class="fa fa-check"></i><b>4.4.1</b> Efficient Computation of Ridge Regression using the Woodbury Identity</a></li>
<li class="chapter" data-level="4.4.2" data-path="linear-regression.html"><a href="linear-regression.html#efficient-bayesian-sampling-for-gaussian-scale-mixture-priors"><i class="fa fa-check"></i><b>4.4.2</b> Efficient Bayesian Sampling for Gaussian Scale-Mixture Priors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>5</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="5.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#pca-as-a-low-rank-approximation-of-mathbfx"><i class="fa fa-check"></i><b>5.1</b> PCA as a Low-Rank Approximation of <span class="math inline">\(\mathbf{X}\)</span></a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#why-low-rank-approximation-matters"><i class="fa fa-check"></i><b>5.1.1</b> Why Low-Rank Approximation Matters</a></li>
<li class="chapter" data-level="5.1.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#the-singular-value-solution-is-the-best-low-rank-approximation-eckartyoungmirsky-theorem"><i class="fa fa-check"></i><b>5.1.2</b> The Singular Value Solution is the Best Low-Rank Approximation (Eckart–Young–Mirsky Theorem)</a></li>
<li class="chapter" data-level="5.1.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#eckartyoungmirsky-theorem-for-the-spectral-norm"><i class="fa fa-check"></i><b>5.1.3</b> Eckart–Young–Mirsky Theorem for the Spectral Norm</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#variance-maximization-in-pca"><i class="fa fa-check"></i><b>5.2</b> Variance Maximization in PCA</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#first-principal-cmponent"><i class="fa fa-check"></i><b>5.2.1</b> First Principal Cmponent</a></li>
<li class="chapter" data-level="5.2.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#second-principal-component"><i class="fa fa-check"></i><b>5.2.2</b> Second Principal Component</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#parafac-decomposition"><i class="fa fa-check"></i><b>5.3</b> PARAFAC decomposition</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#what-is-a-tensor"><i class="fa fa-check"></i><b>5.3.1</b> What is a Tensor?</a></li>
<li class="chapter" data-level="5.3.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#motivation-why-generalize-pca-to-tensors"><i class="fa fa-check"></i><b>5.3.2</b> Motivation: Why Generalize PCA to Tensors?</a></li>
<li class="chapter" data-level="5.3.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#parafac-decomposition-definition"><i class="fa fa-check"></i><b>5.3.3</b> PARAFAC Decomposition Definition</a></li>
<li class="chapter" data-level="5.3.4" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#how-is-parafac-related-to-pca"><i class="fa fa-check"></i><b>5.3.4</b> How is PARAFAC Related to PCA?</a></li>
<li class="chapter" data-level="5.3.5" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#key-properties-of-parafac-decomposition"><i class="fa fa-check"></i><b>5.3.5</b> Key Properties of PARAFAC Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#high-dimensional-pca"><i class="fa fa-check"></i><b>5.4</b> High-Dimensional PCA</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#high-dimensional-pca-p-gg-n"><i class="fa fa-check"></i><b>5.4.1</b> High-Dimensional PCA (<span class="math inline">\(p \gg n\)</span>)</a></li>
<li class="chapter" data-level="5.4.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#randomized-pca"><i class="fa fa-check"></i><b>5.4.2</b> Randomized PCA</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#principal-components-regression"><i class="fa fa-check"></i><b>5.5</b> Principal Components Regression</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#what-is-principal-components-regression"><i class="fa fa-check"></i><b>5.5.1</b> What is Principal Components Regression?</a></li>
<li class="chapter" data-level="5.5.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#why-use-pcr"><i class="fa fa-check"></i><b>5.5.2</b> Why Use PCR?</a></li>
<li class="chapter" data-level="5.5.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#mathematical-formulation"><i class="fa fa-check"></i><b>5.5.3</b> Mathematical Formulation</a></li>
<li class="chapter" data-level="5.5.4" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#choosing-the-number-of-components"><i class="fa fa-check"></i><b>5.5.4</b> Choosing the Number of Components</a></li>
<li class="chapter" data-level="5.5.5" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#advantages-of-pcr"><i class="fa fa-check"></i><b>5.5.5</b> Advantages of PCR</a></li>
<li class="chapter" data-level="5.5.6" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#disadvantages"><i class="fa fa-check"></i><b>5.5.6</b> Disadvantages</a></li>
<li class="chapter" data-level="5.5.7" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#randomized-pca-and-pcr"><i class="fa fa-check"></i><b>5.5.7</b> Randomized PCA and PCR</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="factor-analysis.html"><a href="factor-analysis.html"><i class="fa fa-check"></i><b>6</b> Factor Analysis</a>
<ul>
<li class="chapter" data-level="6.1" data-path="factor-analysis.html"><a href="factor-analysis.html#introduction-2"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="factor-analysis.html"><a href="factor-analysis.html#how-factor-analysis-works"><i class="fa fa-check"></i><b>6.1.1</b> How Factor Analysis Works</a></li>
<li class="chapter" data-level="6.1.2" data-path="factor-analysis.html"><a href="factor-analysis.html#why-is-factor-analysis-useful"><i class="fa fa-check"></i><b>6.1.2</b> Why is Factor Analysis Useful?</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="factor-analysis.html"><a href="factor-analysis.html#factor-analysis-model"><i class="fa fa-check"></i><b>6.2</b> Factor Analysis Model</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="factor-analysis.html"><a href="factor-analysis.html#model-specification-1"><i class="fa fa-check"></i><b>6.2.1</b> Model Specification</a></li>
<li class="chapter" data-level="6.2.2" data-path="factor-analysis.html"><a href="factor-analysis.html#assumptions"><i class="fa fa-check"></i><b>6.2.2</b> Assumptions</a></li>
<li class="chapter" data-level="6.2.3" data-path="factor-analysis.html"><a href="factor-analysis.html#variance-covariance-matrix"><i class="fa fa-check"></i><b>6.2.3</b> Variance-Covariance Matrix</a></li>
<li class="chapter" data-level="6.2.4" data-path="factor-analysis.html"><a href="factor-analysis.html#implications"><i class="fa fa-check"></i><b>6.2.4</b> Implications</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="factor-analysis.html"><a href="factor-analysis.html#estimation-in-factor-analysis"><i class="fa fa-check"></i><b>6.3</b> Estimation in Factor Analysis</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="factor-analysis.html"><a href="factor-analysis.html#principal-components-method"><i class="fa fa-check"></i><b>6.3.1</b> Principal Components Method</a></li>
<li class="chapter" data-level="6.3.2" data-path="factor-analysis.html"><a href="factor-analysis.html#principal-factor-method"><i class="fa fa-check"></i><b>6.3.2</b> Principal Factor Method</a></li>
<li class="chapter" data-level="6.3.3" data-path="factor-analysis.html"><a href="factor-analysis.html#maximum-likelihood-estimation-mle"><i class="fa fa-check"></i><b>6.3.3</b> Maximum Likelihood Estimation (MLE)</a></li>
<li class="chapter" data-level="6.3.4" data-path="factor-analysis.html"><a href="factor-analysis.html#bayesian-estimation-hierarchical-priors"><i class="fa fa-check"></i><b>6.3.4</b> Bayesian Estimation (Hierarchical Priors)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="factor-analysis.html"><a href="factor-analysis.html#estimating-factor-scores-in-factor-analysis"><i class="fa fa-check"></i><b>6.4</b> Estimating Factor Scores in Factor Analysis</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="factor-analysis.html"><a href="factor-analysis.html#ordinary-least-squares-ols-method"><i class="fa fa-check"></i><b>6.4.1</b> Ordinary Least Squares (OLS) Method</a></li>
<li class="chapter" data-level="6.4.2" data-path="factor-analysis.html"><a href="factor-analysis.html#weighted-least-squares-wls-method-bartletts-method"><i class="fa fa-check"></i><b>6.4.2</b> Weighted Least Squares (WLS) Method (Bartlett’s Method)</a></li>
<li class="chapter" data-level="6.4.3" data-path="factor-analysis.html"><a href="factor-analysis.html#regression-method-thompsons-estimator"><i class="fa fa-check"></i><b>6.4.3</b> Regression Method (Thompson’s Estimator)</a></li>
<li class="chapter" data-level="6.4.4" data-path="factor-analysis.html"><a href="factor-analysis.html#summary-of-factor-score-estimators"><i class="fa fa-check"></i><b>6.4.4</b> Summary of Factor Score Estimators</a></li>
<li class="chapter" data-level="6.4.5" data-path="factor-analysis.html"><a href="factor-analysis.html#key-takeaways"><i class="fa fa-check"></i><b>6.4.5</b> Key Takeaways</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="factor-analysis.html"><a href="factor-analysis.html#factor-analysis-example"><i class="fa fa-check"></i><b>6.5</b> Factor Analysis Example</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="factor-analysis.html"><a href="factor-analysis.html#load-required-libraries"><i class="fa fa-check"></i><b>6.5.1</b> Load Required Libraries</a></li>
<li class="chapter" data-level="6.5.2" data-path="factor-analysis.html"><a href="factor-analysis.html#generate-sample-data"><i class="fa fa-check"></i><b>6.5.2</b> Generate Sample Data</a></li>
<li class="chapter" data-level="6.5.3" data-path="factor-analysis.html"><a href="factor-analysis.html#estimation-of-factor-loadings-and-unique-variances"><i class="fa fa-check"></i><b>6.5.3</b> Estimation of Factor Loadings and Unique Variances</a></li>
<li class="chapter" data-level="6.5.4" data-path="factor-analysis.html"><a href="factor-analysis.html#estimate-factor-scores"><i class="fa fa-check"></i><b>6.5.4</b> Estimate Factor Scores</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html"><i class="fa fa-check"></i><b>7</b> Canonical Correlation Analysis (CCA)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#motivation-1"><i class="fa fa-check"></i><b>7.1</b> Motivation</a></li>
<li class="chapter" data-level="7.2" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#mathematical-formulation-1"><i class="fa fa-check"></i><b>7.2</b> Mathematical Formulation</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#key-properties-6"><i class="fa fa-check"></i><b>7.2.1</b> Key Properties</a></li>
<li class="chapter" data-level="7.2.2" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#implementation-in-r"><i class="fa fa-check"></i><b>7.2.2</b> Implementation in R</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#canonical-directions-estimation"><i class="fa fa-check"></i><b>7.3</b> Canonical Directions Estimation</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#problem-definition-with-random-variables"><i class="fa fa-check"></i><b>7.3.1</b> Problem Definition with Random Variables</a></li>
<li class="chapter" data-level="7.3.2" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#canonical-correlation-maximization-problem"><i class="fa fa-check"></i><b>7.3.2</b> Canonical Correlation Maximization Problem</a></li>
<li class="chapter" data-level="7.3.3" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#solving-for-the-canonical-directions-using-lagrange-multipliers"><i class="fa fa-check"></i><b>7.3.3</b> Solving for the Canonical Directions Using Lagrange Multipliers</a></li>
<li class="chapter" data-level="7.3.4" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#showing-that-the-matrices-have-the-same-eigenvalues"><i class="fa fa-check"></i><b>7.3.4</b> Showing That the Matrices Have the Same Eigenvalues</a></li>
<li class="chapter" data-level="7.3.5" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#showing-that-the-largest-eigenvalue-maximizes-the-objective-function"><i class="fa fa-check"></i><b>7.3.5</b> Showing That the Largest Eigenvalue Maximizes the Objective Function</a></li>
<li class="chapter" data-level="7.3.6" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#conclusion-3"><i class="fa fa-check"></i><b>7.3.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#hd-cca"><i class="fa fa-check"></i><b>7.4</b> HD CCA</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#regularized-canonical-correlation-analysis-ridge-cca"><i class="fa fa-check"></i><b>7.4.1</b> Regularized Canonical Correlation Analysis (Ridge CCA)</a></li>
<li class="chapter" data-level="7.4.2" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#sparse-canonical-correlation-analysis-sparse-cca"><i class="fa fa-check"></i><b>7.4.2</b> Sparse Canonical Correlation Analysis (Sparse CCA)**</a></li>
<li class="chapter" data-level="7.4.3" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#low-rank-approximation-cca-randomized-svd-approach"><i class="fa fa-check"></i><b>7.4.3</b> Low-Rank Approximation CCA (Randomized SVD Approach)</a></li>
<li class="chapter" data-level="7.4.4" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#factor-model-based-cca"><i class="fa fa-check"></i><b>7.4.4</b> Factor Model-Based CCA</a></li>
<li class="chapter" data-level="7.4.5" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#comparison-of-methods-for-high-dimensional-cca"><i class="fa fa-check"></i><b>7.4.5</b> Comparison of Methods for High-Dimensional CCA</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#conclusion-which-method-to-use"><i class="fa fa-check"></i><b>7.5</b> <strong>Conclusion: Which Method to Use?</strong></a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">DS 6388 Spring 2025</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="principal-component-analysis" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">5</span> Principal Component Analysis<a href="principal-component-analysis.html#principal-component-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Principal Component Analysis (PCA) is a widely used dimensionality reduction technique in statistics and machine learning. While it is often introduced as a method for <strong>finding orthogonal directions of maximum variance</strong>, another fundamental perspective is that <strong>PCA provides the best low-rank approximation of the data matrix <span class="math inline">\(\mathbf{X}\)</span></strong>.</p>
<p>In many real-world datasets, the observed variables exhibit significant redundancy due to correlations between features. This means that the <strong>true intrinsic dimensionality</strong> of the data is often much lower than the number of measured variables. PCA exploits this redundancy by representing the data using a <strong>lower-dimensional subspace</strong> while preserving as much of the original information as possible.</p>
<hr />
<div id="pca-as-a-low-rank-approximation-of-mathbfx" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> PCA as a Low-Rank Approximation of <span class="math inline">\(\mathbf{X}\)</span><a href="principal-component-analysis.html#pca-as-a-low-rank-approximation-of-mathbfx" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let <span class="math inline">\(\mathbf{X}\)</span> be an <span class="math inline">\(n \times p\)</span> data matrix, where <span class="math inline">\(n\)</span> is the number of observations and <span class="math inline">\(p\)</span> is the number of variables. The goal of PCA is to find a low-rank representation of <span class="math inline">\(\mathbf{X}\)</span> that captures its most important structure. This is achieved through the <strong>Singular Value Decomposition (SVD)</strong>:</p>
<p><span class="math display">\[
\mathbf{X} = \sum_{i=1}^{r} \sigma_i \mathbf{u}_i \mathbf{v}_i&#39;
\]</span></p>
<p>where:
- <span class="math inline">\(\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_r &gt; 0\)</span> are the <strong>singular values</strong> of <span class="math inline">\(\mathbf{X}\)</span>,
- <span class="math inline">\(\mathbf{u}_i\)</span> and <span class="math inline">\(\mathbf{v}_i\)</span> are the corresponding <strong>left and right singular vectors</strong>,
- <span class="math inline">\(r = \text{rank}(\mathbf{X})\)</span>.</p>
<p>PCA provides a <strong>rank-<span class="math inline">\(k\)</span> approximation</strong> by retaining only the <strong>top <span class="math inline">\(k\)</span> singular values and singular vectors</strong>, leading to:</p>
<p><span class="math display">\[
\mathbf{X}_k = \sum_{i=1}^{k} \sigma_i \mathbf{u}_i \mathbf{v}_i&#39;.
\]</span></p>
<p>By the <strong>Eckart–Young–Mirsky theorem</strong>, this is the best approximation to <span class="math inline">\(\mathbf{X}\)</span> under the <strong>Frobenius norm</strong>, meaning it minimizes the reconstruction error:</p>
<p><span class="math display">\[
\|\mathbf{X} - \mathbf{X}_k\|_F^2 = \sum_{i=k+1}^{r} \sigma_i^2.
\]</span></p>
<hr />
<div id="why-low-rank-approximation-matters" class="section level3 hasAnchor" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> Why Low-Rank Approximation Matters<a href="principal-component-analysis.html#why-low-rank-approximation-matters" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li><strong>Dimensionality Reduction</strong>
<ul>
<li>The original data matrix <span class="math inline">\(\mathbf{X}\)</span> is often high-dimensional and contains redundant information.<br />
</li>
<li>PCA allows us to approximate <span class="math inline">\(\mathbf{X}\)</span> with a much smaller representation while preserving most of the variance.</li>
</ul></li>
<li><strong>Noise Reduction</strong>
<ul>
<li>Higher-order singular values <span class="math inline">\(\sigma_{k+1}, \dots, \sigma_r\)</span> are often associated with noise rather than meaningful structure.<br />
</li>
<li>Truncating these components leads to a <strong>denoised version of the data</strong>.</li>
</ul></li>
<li><strong>Compression and Storage Efficiency</strong>
<ul>
<li>Instead of storing <span class="math inline">\(n \times p\)</span> raw data values, we only need to store the <strong>top <span class="math inline">\(k\)</span> singular values and vectors</strong>, which is much more memory-efficient.</li>
</ul></li>
<li><strong>Feature Extraction</strong>
<ul>
<li>The <strong>principal components</strong> (columns of <span class="math inline">\(\mathbf{V}_k\)</span>) define a new <strong>basis</strong> for the data that is more informative and often interpretable.<br />
</li>
<li>These features can be used for <strong>classification, clustering, and visualization</strong>.</li>
</ul></li>
</ol>
<hr />
</div>
<div id="the-singular-value-solution-is-the-best-low-rank-approximation-eckartyoungmirsky-theorem" class="section level3 hasAnchor" number="5.1.2">
<h3><span class="header-section-number">5.1.2</span> The Singular Value Solution is the Best Low-Rank Approximation (Eckart–Young–Mirsky Theorem)<a href="principal-component-analysis.html#the-singular-value-solution-is-the-best-low-rank-approximation-eckartyoungmirsky-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We want to prove that the <strong>truncated Singular Value Decomposition (SVD)</strong> provides the <strong>best rank-<span class="math inline">\(k\)</span> approximation</strong> to a matrix <span class="math inline">\(\mathbf{X}\)</span> in the <strong>Frobenius norm</strong> and <strong>spectral norm</strong>. This result is known as the <strong>Eckart–Young–Mirsky theorem</strong>.</p>
<hr />
<div id="problem-statement-eckartyoungmirsky-theorem" class="section level4 hasAnchor" number="5.1.2.1">
<h4><span class="header-section-number">5.1.2.1</span> Problem Statement (Eckart–Young–Mirsky Theorem)<a href="principal-component-analysis.html#problem-statement-eckartyoungmirsky-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let <span class="math inline">\(\mathbf{X}\)</span> be an <span class="math inline">\(n \times p\)</span> matrix with <strong>full SVD</strong>:</p>
<p><span class="math display">\[
\mathbf{X} = \sum_{i=1}^{r} \sigma_i \mathbf{u}_i \mathbf{v}_i&#39;
\]</span></p>
<p>where:
- <span class="math inline">\(\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_r &gt; 0\)</span> are the <strong>singular values</strong> (ordered decreasingly),
- <span class="math inline">\(\mathbf{u}_i\)</span> are the left singular vectors (columns of <span class="math inline">\(\mathbf{U}\)</span>, an <span class="math inline">\(n \times n\)</span> orthonormal matrix),
- <span class="math inline">\(\mathbf{v}_i\)</span> are the right singular vectors (columns of <span class="math inline">\(\mathbf{V}\)</span>, a <span class="math inline">\(p \times p\)</span> orthonormal matrix),
- <span class="math inline">\(r = \text{rank}(\mathbf{X})\)</span>.</p>
<p>We seek a <strong>rank-<span class="math inline">\(k\)</span> matrix</strong> <span class="math inline">\(\mathbf{Y}\)</span> that best approximates <span class="math inline">\(\mathbf{X}\)</span> by minimizing the <strong>Frobenius norm error</strong>:</p>
<p><span class="math display">\[
\min_{\text{rank}(\mathbf{Y}) = k} \|\mathbf{X} - \mathbf{Y}\|_F.
\]</span></p>
<p>The theorem states that the best rank-<span class="math inline">\(k\)</span> approximation is given by the <strong>truncated SVD</strong>:</p>
<p><span class="math display">\[
\mathbf{X}_k = \sum_{i=1}^{k} \sigma_i \mathbf{u}_i \mathbf{v}_i&#39;.
\]</span></p>
<p>We now prove this claim.</p>
<hr />
</div>
<div id="proof-eckartyoungmirsky-theorem" class="section level4 hasAnchor" number="5.1.2.2">
<h4><span class="header-section-number">5.1.2.2</span> Proof (Eckart–Young–Mirsky Theorem)<a href="principal-component-analysis.html#proof-eckartyoungmirsky-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We will show this in four steps.</p>
<ol style="list-style-type: decimal">
<li>Step 1: The <strong>Frobenius norm</strong> of <span class="math inline">\(\mathbf{X}\)</span> is:</li>
</ol>
<p><span class="math display">\[
\|\mathbf{X}\|_F^2 = \sum_{i=1}^{r} \sigma_i^2.
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Step 2: Any <strong>rank-<span class="math inline">\(k\)</span> approximation</strong> <span class="math inline">\(\mathbf{Y}\)</span> can be written in terms of <strong>some</strong> linear combination of singular vectors:</li>
</ol>
<p><span class="math display">\[
\mathbf{Y} = \sum_{i=1}^{k} a_i \mathbf{u}_i \mathbf{v}_i&#39;.
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Step 3: The residual error</li>
</ol>
<p><span class="math display">\[
\|\mathbf{X} - \mathbf{Y}\|_F^2 = \sum_{i=1}^{k} (\sigma_i - a_i)^2 + \sum_{i=k+1}^{r} \sigma_i^2
\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li>Step 4: The optimal choice is <strong><span class="math inline">\(a_i = \sigma_i\)</span></strong> for the first <span class="math inline">\(k\)</span> terms and <strong><span class="math inline">\(a_i = 0\)</span> for the rest</strong>.</li>
</ol>
<p>For Step 1, we want to show that the <strong>Frobenius norm</strong> of a matrix <span class="math inline">\(\mathbf{X}\)</span> is equal to the sum of the squared singular values:</p>
<p><span class="math display">\[
\|\mathbf{X}\|_F^2 = \sum_{i=1}^{r} \sigma_i^2.
\]</span></p>
<p>The <strong>Frobenius norm</strong> of an <span class="math inline">\(n \times p\)</span> matrix <span class="math inline">\(\mathbf{X}\)</span> is defined as:</p>
<p><span class="math display">\[
\|\mathbf{X}\|_F = \sqrt{\sum_{i=1}^{n} \sum_{j=1}^{p} x_{ij}^2}.
\]</span></p>
<p>Squaring both sides:</p>
<p><span class="math display">\[
\|\mathbf{X}\|_F^2 = \sum_{i=1}^{n} \sum_{j=1}^{p} x_{ij}^2.
\]</span></p>
<p>An alternative way to express the Frobenius norm is using the <strong>trace function</strong>:</p>
<p><span class="math display">\[
\|\mathbf{X}\|_F^2 = \text{Tr}(\mathbf{X}&#39; \mathbf{X}).
\]</span></p>
<p>where <span class="math inline">\(\text{Tr}(\mathbf{A})\)</span> denotes the <strong>trace</strong> (sum of the diagonal elements) of a square matrix <span class="math inline">\(\mathbf{A}\)</span>.</p>
<p>To see this note that</p>
<p><span class="math display">\[
\|\mathbf{X}\|_F^2 = \sum_{i=1}^{n} \sum_{j=1}^{p} x_{ij}^2 = \sum_{j=1}^{p} \sum_{i=1}^{n} x_{ij}^2 = \sum_{j=1}^{p}  {\boldsymbol x} _j&#39;  {\boldsymbol x} _j = \text{Tr}(\mathbf{X}&#39; \mathbf{X}).
\]</span></p>
<p>From the <strong>SVD decomposition</strong>, we write:</p>
<p><span class="math display">\[
\mathbf{X} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}&#39;
\]</span></p>
<p>where:
- <span class="math inline">\(\mathbf{U}\)</span> is an <span class="math inline">\(n \times n\)</span> orthonormal matrix of <strong>left singular vectors</strong>,
- <span class="math inline">\(\mathbf{V}\)</span> is a <span class="math inline">\(p \times p\)</span> orthonormal matrix of <strong>right singular vectors</strong>,
- <span class="math inline">\(\mathbf{\Sigma}\)</span> is an <span class="math inline">\(n \times p\)</span> diagonal matrix of <strong>singular values</strong>:</p>
<p><span class="math display">\[
\mathbf{\Sigma} =
\begin{bmatrix}
\sigma_1 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \sigma_2 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; 0 &amp; \sigma_3 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; \sigma_r \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots
\end{bmatrix}.
\]</span></p>
<p>The rank of <span class="math inline">\(\mathbf{X}\)</span> is <span class="math inline">\(r\)</span>, meaning it has <strong><span class="math inline">\(r\)</span> nonzero singular values</strong>.</p>
<p>Now, compute <span class="math inline">\(\mathbf{X}&#39; \mathbf{X}\)</span>:</p>
<p><span class="math display">\[
\mathbf{X}&#39; \mathbf{X} = (\mathbf{U} \mathbf{\Sigma} \mathbf{V}&#39;)&#39; (\mathbf{U} \mathbf{\Sigma} \mathbf{V}&#39;).
\]</span></p>
<p>Using the transpose property <span class="math inline">\((\mathbf{A} \mathbf{B})&#39; = \mathbf{B}&#39; \mathbf{A}&#39;\)</span>:</p>
<p><span class="math display">\[
\mathbf{X}&#39; \mathbf{X} = \mathbf{V} \mathbf{\Sigma}&#39; \mathbf{U}&#39; \mathbf{U} \mathbf{\Sigma} \mathbf{V}&#39;.
\]</span></p>
<p>Since <span class="math inline">\(\mathbf{U}\)</span> is orthonormal (<span class="math inline">\(\mathbf{U}&#39; \mathbf{U} = \mathbf{I}\)</span>), this simplifies to:</p>
<p><span class="math display">\[
\mathbf{X}&#39; \mathbf{X} = \mathbf{V} \mathbf{\Sigma}&#39; \mathbf{\Sigma} \mathbf{V}&#39;.
\]</span></p>
<p>Since <span class="math inline">\(\mathbf{\Sigma}&#39; \mathbf{\Sigma}\)</span> is a diagonal matrix with squared singular values <span class="math inline">\(\sigma_i^2\)</span>, we get:</p>
<p><span class="math display">\[
\mathbf{X}&#39; \mathbf{X} = \mathbf{V}
\begin{bmatrix}
\sigma_1^2 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \sigma_2^2 &amp; \cdots &amp; 0 \\
0 &amp; 0 &amp; \ddots &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; \sigma_r^2
\end{bmatrix}
\mathbf{V}&#39;.
\]</span></p>
<p>Taking the <strong>trace</strong> on both sides:</p>
<p><span class="math display">\[
\text{Tr}(\mathbf{X}&#39; \mathbf{X}) = \text{Tr}(\mathbf{V} \mathbf{\Sigma}&#39; \mathbf{\Sigma} \mathbf{V}&#39;).
\]</span></p>
<p>Since the trace is invariant under cyclic permutations:</p>
<p><span class="math display">\[
\text{Tr}(\mathbf{X}&#39; \mathbf{X}) = \text{Tr}( \mathbf{V}&#39; \mathbf{V} \mathbf{\Sigma}&#39; \mathbf{\Sigma}) = \text{Tr}(\mathbf{\Sigma}&#39; \mathbf{\Sigma}).
\]</span></p>
<p>Since <span class="math inline">\(\mathbf{\Sigma}&#39; \mathbf{\Sigma}\)</span> is diagonal, its trace is simply the sum of the diagonal elements:</p>
<p><span class="math display">\[
\text{Tr}(\mathbf{\Sigma}&#39; \mathbf{\Sigma}) = \sum_{i=1}^{r} \sigma_i^2.
\]</span></p>
<p>Since we previously established that:</p>
<p><span class="math display">\[
\|\mathbf{X}\|_F^2 = \text{Tr}(\mathbf{X}&#39; \mathbf{X}),
\]</span></p>
<p>we conclude:</p>
<p><span class="math display">\[
\|\mathbf{X}\|_F^2 = \sum_{i=1}^{r} \sigma_i^2.
\]</span></p>
<p>For Step 2, we claim that any rank-<span class="math inline">\(k\)</span> matrix <span class="math inline">\(\mathbf{Y}\)</span> can be expressed as:</p>
<p><span class="math display">\[
\mathbf{Y} = \sum_{i=1}^{k} a_i \mathbf{u}_i \mathbf{v}_i&#39;.
\]</span></p>
<p>This follows from the fact that the outer products of the singular vectors,</p>
<p><span class="math display">\[
\mathbf{u}_i \mathbf{v}_i&#39;
\]</span></p>
<p>form an <strong>orthonormal basis</strong> for matrices in <span class="math inline">\(\mathbb{R}^{n \times p}\)</span>, meaning that any rank-<span class="math inline">\(k\)</span> matrix can be expressed as a linear combination of at most <span class="math inline">\(k\)</span> of these elements.</p>
<p>To see this, consider the inner product between two <strong>singular vector outer products</strong>, defined as:</p>
<p><span class="math display">\[
\langle \mathbf{u}_i \mathbf{v}_i&#39;, \mathbf{u}_j \mathbf{v}_j&#39; \rangle_F.
\]</span></p>
<p>By the <strong>Frobenius inner product</strong>, the inner product between two matrices <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{B}\)</span> is:</p>
<p><span class="math display">\[
\langle \mathbf{A}, \mathbf{B} \rangle_F = \text{Tr}(\mathbf{A}&#39; \mathbf{B}).
\]</span></p>
<p>Applying this to the singular vectors:</p>
<p><span class="math display">\[
\langle \mathbf{u}_i \mathbf{v}_i&#39;, \mathbf{u}_j \mathbf{v}_j&#39; \rangle_F = \text{Tr} \big( (\mathbf{u}_i \mathbf{v}_i&#39;)&#39; (\mathbf{u}_j \mathbf{v}_j&#39;) \big).
\]</span></p>
<p>Using the transpose property <span class="math inline">\((\mathbf{A} \mathbf{B})&#39; = \mathbf{B}&#39; \mathbf{A}&#39;\)</span>, we get:</p>
<p><span class="math display">\[
\langle \mathbf{u}_i \mathbf{v}_i&#39;, \mathbf{u}_j \mathbf{v}_j&#39; \rangle_F = \text{Tr} \big( \mathbf{v}_i \mathbf{u}_i&#39; \mathbf{u}_j \mathbf{v}_j&#39; \big).
\]</span></p>
<p>Since <span class="math inline">\(\mathbf{U}\)</span> and <span class="math inline">\(\mathbf{V}\)</span> are <strong>orthonormal</strong>, we use:</p>
<p><span class="math display">\[
\mathbf{u}_i&#39; \mathbf{u}_j = \delta_{ij}, \quad \mathbf{v}_i&#39; \mathbf{v}_j = \delta_{ij}.
\]</span></p>
<p>where <span class="math inline">\(\delta_{ij} = 1\)</span>, if <span class="math inline">\(i=j\)</span> and <span class="math inline">\(\delta_{ij} = 0\)</span>, if <span class="math inline">\(i \neq j\)</span>.</p>
<p>Thus,</p>
<p><span class="math display">\[
\langle \mathbf{u}_i \mathbf{v}_i&#39;, \mathbf{u}_j \mathbf{v}_j&#39; \rangle_F = \delta_{ij}.
\]</span></p>
<p>This proves that <span class="math inline">\(\{ \mathbf{u}_i \mathbf{v}_i&#39; \}_{i=1}^{k}\)</span> are <strong>orthonormal under the Frobenius inner product</strong>, therefore linearly independent and forming an spanning bases for a rank <span class="math inline">\(k\)</span> matrix.</p>
<p>For Step 3, mote that the <strong>error matrix</strong> is:</p>
<p><span class="math display">\[
\mathbf{X} - \mathbf{Y} = \sum_{i=1}^{r} \sigma_i \mathbf{u}_i \mathbf{v}_i&#39; - \sum_{i=1}^{k} a_i \mathbf{u}_i \mathbf{v}_i&#39;.
\]</span></p>
<p>Rewriting this sum:</p>
<p><span class="math display">\[
\mathbf{X} - \mathbf{Y} = \sum_{i=1}^{k} (\sigma_i - a_i) \mathbf{u}_i \mathbf{v}_i&#39; + \sum_{i=k+1}^{r} \sigma_i \mathbf{u}_i \mathbf{v}_i&#39;.
\]</span></p>
<p>Since the <strong>singular vector outer products</strong> <span class="math inline">\(\mathbf{u}_i \mathbf{v}_i&#39;\)</span> form an <strong>orthonormal basis</strong> under the Frobenius norm, the squared Frobenius norm of a sum of such terms is the <strong>sum of the squared coefficients</strong>:</p>
<p><span class="math display">\[
\|\mathbf{X} - \mathbf{Y}\|_F^2 = \sum_{i=1}^{k} (\sigma_i - a_i)^2 + \sum_{i=k+1}^{r} \sigma_i^2.
\]</span></p>
<p>Finally, in Step 4, we need to minimize the approximation error:</p>
<p><span class="math display">\[
\sum_{i=1}^{k} (\sigma_i - a_i)^2 + \sum_{i=k+1}^{r} \sigma_i^2,
\]</span></p>
<p>the best choice is clearly:</p>
<p><span class="math display">\[
a_i = \sigma_i, \quad \text{for } i = 1, \dots, k.
\]</span></p>
<p>Thus, the <strong>best rank-<span class="math inline">\(k\)</span> approximation</strong> is:</p>
<p><span class="math display">\[
\mathbf{X}_k = \sum_{i=1}^{k} \sigma_i \mathbf{u}_i \mathbf{v}_i&#39;.
\]</span></p>
<p>This minimizes the error:</p>
<p><span class="math display">\[
\|\mathbf{X} - \mathbf{X}_k\|_F^2 = \sum_{i=k+1}^{r} \sigma_i^2.
\]</span></p>
<p>which confirms the <strong>Eckart–Young–Mirsky theorem</strong>.</p>
<hr />
</div>
<div id="uniqueness-of-the-solution" class="section level4 hasAnchor" number="5.1.2.3">
<h4><span class="header-section-number">5.1.2.3</span> Uniqueness of the Solution<a href="principal-component-analysis.html#uniqueness-of-the-solution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <strong>Eckart–Young–Mirsky theorem</strong> guarantees that the <strong>best rank-<span class="math inline">\(k\)</span> approximation</strong> of a matrix <span class="math inline">\(\mathbf{X}\)</span> in terms of the <strong>Frobenius norm</strong> is given by the <strong>truncated SVD</strong>. However, the <strong>uniqueness</strong> of this solution depends on the <strong>singular values</strong>.</p>
<hr />
<div id="key-condition-for-uniqueness" class="section level5 hasAnchor" number="5.1.2.3.1">
<h5><span class="header-section-number">5.1.2.3.1</span> Key Condition for Uniqueness<a href="principal-component-analysis.html#key-condition-for-uniqueness" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>The best rank-<span class="math inline">\(k\)</span> approximation is <strong>unique</strong> if and only if the <span class="math inline">\(k\)</span>-th singular value <span class="math inline">\(\sigma_k\)</span> is <strong>strictly greater</strong> than the <span class="math inline">\((k+1)\)</span>-th singular value <span class="math inline">\(\sigma_{k+1}\)</span>.</li>
<li>In other words:
<span class="math display">\[
\sigma_k &gt; \sigma_{k+1} \implies \text{Unique solution.}
\]</span></li>
</ul>
<hr />
</div>
<div id="when-is-the-solution-not-unique" class="section level5 hasAnchor" number="5.1.2.3.2">
<h5><span class="header-section-number">5.1.2.3.2</span> When is the Solution Not Unique?<a href="principal-component-analysis.html#when-is-the-solution-not-unique" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>If there is a <strong>tie</strong> or <strong>degeneracy</strong> in the singular values:
<span class="math display">\[
\sigma_k = \sigma_{k+1},
\]</span>
then the best rank-<span class="math inline">\(k\)</span> approximation is <strong>not unique</strong>.</li>
<li>Multiple linear combinations of the corresponding singular vectors can yield the same Frobenius norm error, leading to multiple optimal solutions.</li>
</ul>
<hr />
</div>
<div id="intuition" class="section level5 hasAnchor" number="5.1.2.3.3">
<h5><span class="header-section-number">5.1.2.3.3</span> Intuition<a href="principal-component-analysis.html#intuition" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li><strong>Unique solution</strong>: Singular values decrease strictly, and each singular vector contributes distinctly to the approximation.</li>
<li><strong>Non-unique solution</strong>: Equal singular values create a <strong>subspace</strong> of possible solutions, as any linear combination of singular vectors associated with the repeated singular value results in the same approximation quality.</li>
</ul>
<hr />
</div>
<div id="example-9" class="section level5 hasAnchor" number="5.1.2.3.4">
<h5><span class="header-section-number">5.1.2.3.4</span> Example<a href="principal-component-analysis.html#example-9" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>If <span class="math inline">\(\mathbf{X}\)</span> has singular values <span class="math inline">\(\sigma_1 = 5, \sigma_2 = 4, \sigma_3 = 4, \sigma_4 = 2\)</span>, then:
- The <strong>best rank-1 approximation</strong> is unique because <span class="math inline">\(\sigma_1 &gt; \sigma_2\)</span>.
- The <strong>best rank-2 approximation</strong> is not unique because <span class="math inline">\(\sigma_2 = \sigma_3\)</span>.
- The <strong>best rank-3 approximation</strong> is unique because <span class="math inline">\(\sigma_3 &gt; \sigma_4\)</span>.</p>
<hr />
</div>
<div id="summary" class="section level5 hasAnchor" number="5.1.2.3.5">
<h5><span class="header-section-number">5.1.2.3.5</span> Summary<a href="principal-component-analysis.html#summary" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li><strong>Unique solution</strong>: <span class="math inline">\(\sigma_k &gt; \sigma_{k+1}\)</span>.</li>
<li><strong>Not unique solution</strong>: <span class="math inline">\(\sigma_k = \sigma_{k+1}\)</span>.</li>
</ul>
<p>Would you like me to show an example or provide more details? 😊</p>
<hr />
</div>
</div>
</div>
<div id="eckartyoungmirsky-theorem-for-the-spectral-norm" class="section level3 hasAnchor" number="5.1.3">
<h3><span class="header-section-number">5.1.3</span> Eckart–Young–Mirsky Theorem for the Spectral Norm<a href="principal-component-analysis.html#eckartyoungmirsky-theorem-for-the-spectral-norm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>Eckart–Young–Mirsky theorem</strong> is a fundamental result in matrix approximation, stating that the <strong>best rank-k approximation</strong> of a matrix in terms of the <strong>spectral norm</strong> is obtained through its <strong>singular value decomposition (SVD)</strong>. However, unlike in the Frobenius norm case, the solution is not always unique in the spectral norm.</p>
<hr />
<div id="spectral-norm-theorem-statement" class="section level4 hasAnchor" number="5.1.3.1">
<h4><span class="header-section-number">5.1.3.1</span> Spectral Norm Theorem Statement<a href="principal-component-analysis.html#spectral-norm-theorem-statement" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For any matrix <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times p}\)</span> with singular values <span class="math inline">\(\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r &gt; 0\)</span>, the rank-k matrix <span class="math inline">\(\mathbf{X}_k\)</span> that minimizes the spectral norm error <span class="math inline">\(\|\mathbf{X} - \mathbf{Y}\|_2\)</span> is given by:</p>
<p><span class="math display">\[
\mathbf{X}_k = \sum_{i=1}^k \sigma_i \mathbf{u}_i \mathbf{v}_i&#39;,
\]</span></p>
<p>where <span class="math inline">\(\mathbf{u}_i\)</span> and <span class="math inline">\(\mathbf{v}_i\)</span> are the left and right singular vectors corresponding to <span class="math inline">\(\sigma_i\)</span>.</p>
<p>The minimal spectral norm error is:</p>
<p><span class="math display">\[
\|\mathbf{X} - \mathbf{X}_k\|_2 = \sigma_{k+1}.
\]</span></p>
<hr />
</div>
<div id="spectral-norm-proof-sketch" class="section level4 hasAnchor" number="5.1.3.2">
<h4><span class="header-section-number">5.1.3.2</span> Spectral Norm Proof Sketch<a href="principal-component-analysis.html#spectral-norm-proof-sketch" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The spectral norm of a matrix is its largest singular value. Given any rank-k matrix <span class="math inline">\(\mathbf{Y}\)</span> written as before <span class="math inline">\(\mathbf{Y} = \sum_{i=1}^{k} a_i \mathbf{u}_i \mathbf{v}_i&#39;\)</span>, its singular values are <span class="math inline">\(a_1, a_2, \dots, a_k\)</span>. The error matrix <span class="math inline">\(\mathbf{X} - \mathbf{Y}\)</span> has singular values:</p>
<p><span class="math display">\[
\{ |\sigma_1 - a_1|, |\sigma_2 - a_2|, \dots, |\sigma_k - a_k|, \sigma_{k+1}, \dots, \sigma_r \}.
\]</span></p>
<p>To minimize the spectral norm of the error, one must ensure that:</p>
<p><span class="math display">\[
\max \{ |\sigma_1 - a_1|, |\sigma_2 - a_2|, \dots, \sigma_{k+1} \}
\]</span></p>
<p>is as small as possible. A way to achieve the minimal error <span class="math inline">\(\sigma_{k+1}\)</span> is to set <span class="math inline">\(a_i = \sigma_i\)</span> for <span class="math inline">\(i=1, \dots, k\)</span>, as with the spectral norm, but we can construct different rank-k matrices that also achieve this value.</p>
<hr />
</div>
<div id="non-uniqueness-for-the-spectral-norm" class="section level4 hasAnchor" number="5.1.3.3">
<h4><span class="header-section-number">5.1.3.3</span> Non-Uniqueness for the Spectral Norm<a href="principal-component-analysis.html#non-uniqueness-for-the-spectral-norm" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Unlike in the Frobenius norm case, where the truncated SVD solution is unique, the spectral norm allows for <strong>multiple solutions</strong>. If we require:</p>
<p><span class="math display">\[
|\sigma_i - a_i| \leq \sigma_{k+1}, \quad \forall i=1, \dots, k,
\]</span></p>
<p>any such choice of <span class="math inline">\(a_i\)</span> will yield the same spectral norm error <span class="math inline">\(\sigma_{k+1}\)</span>. This provides a class of rank-k approximations that achieve the minimal spectral norm error.</p>
<hr />
<div id="how-to-find-alternative-solutions" class="section level5 hasAnchor" number="5.1.3.3.1">
<h5><span class="header-section-number">5.1.3.3.1</span> How to Find Alternative Solutions<a href="principal-component-analysis.html#how-to-find-alternative-solutions" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>To find alternative rank-k approximations with the same spectral norm error, one can:</p>
<ol style="list-style-type: decimal">
<li><strong>Perturb Singular Values</strong>: Adjust <span class="math inline">\(a_i\)</span> such that <span class="math inline">\(|\sigma_i - a_i| \leq \sigma_{k+1}\)</span>, ensuring no singular value difference exceeds <span class="math inline">\(\sigma_{k+1}\)</span>.</li>
<li><strong>Linear Combinations of Singular Vectors</strong>: Combine singular vectors corresponding to equal singular values to create different approximations.</li>
</ol>
<hr />
</div>
</div>
<div id="conclusion-for-the-spectral-norm" class="section level4 hasAnchor" number="5.1.3.4">
<h4><span class="header-section-number">5.1.3.4</span> Conclusion for the Spectral Norm<a href="principal-component-analysis.html#conclusion-for-the-spectral-norm" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <strong>Eckart–Young–Mirsky theorem</strong> guarantees the minimal spectral norm error with the truncated SVD solution. However, in cases of spectral norm approximation, the solution is <strong>not unique</strong>. By carefully choosing singular values and vectors within the specified bounds, one can construct <strong>alternative solutions</strong> that achieve the same minimal error.</p>
<p>This flexibility in spectral norm approximation highlights the subtle difference between approximations in spectral and Frobenius norms, making it an important consideration in applications like low-rank matrix approximation, data compression, and principal component analysis.</p>
</div>
</div>
</div>
<div id="variance-maximization-in-pca" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Variance Maximization in PCA<a href="principal-component-analysis.html#variance-maximization-in-pca" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Principal Component Analysis (PCA) is traditionally introduced as a method that finds directions (principal components) along which the variance of the data is maximized.</p>
<p>Given a data matrix <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times p}\)</span> with centered columns, PCA seeks an orthonormal set of vectors <span class="math inline">\(\{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_p\}\)</span> such that:</p>
<ul>
<li>The first principal component <span class="math inline">\(\mathbf{v}_1\)</span> maximizes the variance:
<span class="math display">\[
\mathbf{v}_1 = \arg \max_{\|\mathbf{v}\|=1} \text{Var}(\mathbf{X}\mathbf{v}) = \arg \max_{\|\mathbf{v}\|=1} \|\mathbf{X}\mathbf{v}\|_2^2.
\]</span></li>
<li>Subsequent principal components are chosen orthogonal to the previous ones and also maximize the variance.</li>
</ul>
<p>Each principal component corresponds to a singular vector from the Singular Value Decomposition (SVD) of <span class="math inline">\(\mathbf{X}\)</span>:
<span class="math display">\[
\mathbf{X} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}&#39;,
\]</span>
where <span class="math inline">\(\mathbf{\Sigma}\)</span> contains the singular values <span class="math inline">\(\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_r &gt; 0\)</span>.</p>
<p>The first <span class="math inline">\(k\)</span> principal components are the first <span class="math inline">\(k\)</span> columns of <span class="math inline">\(\mathbf{V}\)</span>:
<span class="math display">\[
\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k,
\]</span>
with the corresponding principal component scores given by:
<span class="math display">\[
\mathbf{X} \mathbf{v}_i = \sigma_i \mathbf{u}_i, \quad i = 1, 2, \dots, k.
\]</span></p>
<hr />
<div id="first-principal-cmponent" class="section level3 hasAnchor" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> First Principal Cmponent<a href="principal-component-analysis.html#first-principal-cmponent" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We aim to find the direction <span class="math inline">\(\mathbf{v}_1 \in \mathbb{R}^p\)</span> (a unit vector, <span class="math inline">\(\| \mathbf{v}_1 \| = 1\)</span>) that maximizes the <strong>variance</strong> of the data when projected onto <span class="math inline">\(\mathbf{v}_1\)</span>.</p>
<p>The dataset is represented by <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times p}\)</span> (with centered columns), and the projection of <span class="math inline">\(\mathbf{X}\)</span> onto <span class="math inline">\(\mathbf{v}_1\)</span> is given by <span class="math inline">\(\mathbf{Xv}_1\)</span>.</p>
<hr />
<div id="variance-of-the-projection" class="section level4 hasAnchor" number="5.2.1.1">
<h4><span class="header-section-number">5.2.1.1</span> Variance of the Projection<a href="principal-component-analysis.html#variance-of-the-projection" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The variance of the projected data <span class="math inline">\(\mathbf{Xv}_1\)</span> is:</p>
<p><span class="math display">\[
\text{Var}(\mathbf{Xv}_1) = \frac{1}{n} \| \mathbf{Xv}_1 \|_2^2 = \frac{1}{n} (\mathbf{Xv}_1)&#39;(\mathbf{Xv}_1).
\]</span></p>
<p>Expanding this:</p>
<p><span class="math display">\[
\text{Var}(\mathbf{Xv}_1) = \frac{1}{n} \mathbf{v}_1&#39; \mathbf{X}&#39; \mathbf{X} \mathbf{v}_1.
\]</span></p>
<p>Since <span class="math inline">\(\frac{1}{n} \mathbf{X}&#39; \mathbf{X}\)</span> is the <strong>sample covariance matrix</strong> <span class="math inline">\(\mathbf{S}\)</span>, we rewrite the objective as:</p>
<p><span class="math display">\[
\text{maximize } \mathbf{v}_1&#39; \mathbf{S} \mathbf{v}_1 \quad \text{subject to} \quad \| \mathbf{v}_1 \| = 1.
\]</span></p>
<hr />
</div>
<div id="optimization-problem-rayleigh-quotient" class="section level4 hasAnchor" number="5.2.1.2">
<h4><span class="header-section-number">5.2.1.2</span> Optimization Problem (Rayleigh Quotient)<a href="principal-component-analysis.html#optimization-problem-rayleigh-quotient" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This is a constrained optimization problem that can be solved using the method of <strong>Lagrange multipliers</strong>:</p>
<p><span class="math display">\[
\mathcal{L}(\mathbf{v}_1, \lambda) = \mathbf{v}_1&#39; \mathbf{S} \mathbf{v}_1 - \lambda (\mathbf{v}_1&#39; \mathbf{v}_1 - 1).
\]</span></p>
<p>Taking the derivative with respect to <span class="math inline">\(\mathbf{v}_1\)</span> and setting it to zero gives:</p>
<p><span class="math display">\[
\frac{\partial \mathcal{L}}{\partial \mathbf{v}_1} = 2 \mathbf{S} \mathbf{v}_1 - 2 \lambda \mathbf{v}_1 = 0.
\]</span></p>
<p>Thus:</p>
<p><span class="math display">\[
\mathbf{S} \mathbf{v}_1 = \lambda \mathbf{v}_1.
\]</span></p>
<hr />
</div>
<div id="eigenvalue-problem" class="section level4 hasAnchor" number="5.2.1.3">
<h4><span class="header-section-number">5.2.1.3</span> Eigenvalue Problem<a href="principal-component-analysis.html#eigenvalue-problem" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This is an <strong>eigenvalue problem</strong> where <span class="math inline">\(\lambda\)</span> is an eigenvalue of <span class="math inline">\(\mathbf{S}\)</span> and <span class="math inline">\(\mathbf{v}_1\)</span> is the corresponding eigenvector. The direction <span class="math inline">\(\mathbf{v}_1\)</span> that maximizes the variance is the <strong>eigenvector corresponding to the largest eigenvalue</strong> <span class="math inline">\(\lambda_1\)</span>.</p>
<p>Hence, the <strong>first principal component</strong> <span class="math inline">\(\mathbf{v}_1\)</span> is the eigenvector of <span class="math inline">\(\mathbf{S}\)</span> associated with the largest eigenvalue <span class="math inline">\(\lambda_1\)</span>, and the <strong>maximum variance</strong> is <span class="math inline">\(\lambda_1\)</span>.</p>
<hr />
</div>
<div id="connection-to-svd" class="section level4 hasAnchor" number="5.2.1.4">
<h4><span class="header-section-number">5.2.1.4</span> Connection to SVD<a href="principal-component-analysis.html#connection-to-svd" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Recall that the <strong>SVD</strong> of <span class="math inline">\(\mathbf{X}\)</span> is:</p>
<p><span class="math display">\[
\mathbf{X} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}&#39;.
\]</span></p>
<p>The sample covariance matrix <span class="math inline">\(\mathbf{S} = \frac{1}{n} \mathbf{X}&#39; \mathbf{X}\)</span> can be written as:</p>
<p><span class="math display">\[
\mathbf{S} = \frac{1}{n} \mathbf{V} \mathbf{\Sigma}&#39; \mathbf{U}&#39; \mathbf{U} \mathbf{\Sigma} \mathbf{V}&#39; = \frac{1}{n} \mathbf{V} \mathbf{\Sigma}^2 \mathbf{V}&#39;.
\]</span></p>
<p>The eigenvectors of <span class="math inline">\(\mathbf{S}\)</span> are the columns of <span class="math inline">\(\mathbf{V}\)</span>, and the eigenvalues are the squared singular values <span class="math inline">\(\sigma_i^2\)</span> divided by <span class="math inline">\(n\)</span>.</p>
<p>Thus:
- The <strong>first principal component direction</strong> <span class="math inline">\(\mathbf{v}_1\)</span> is the first column of <span class="math inline">\(\mathbf{V}\)</span> from the SVD.
- The <strong>variance explained</strong> by the first principal component is <span class="math inline">\(\frac{\sigma_1^2}{n}\)</span>.</p>
<hr />
</div>
</div>
<div id="second-principal-component" class="section level3 hasAnchor" number="5.2.2">
<h3><span class="header-section-number">5.2.2</span> Second Principal Component<a href="principal-component-analysis.html#second-principal-component" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>second principal component</strong> <span class="math inline">\(\mathbf{v}_2\)</span> is the direction that:
- Maximizes the variance of the projected data <span class="math inline">\(\mathbf{Xv}_2\)</span>,
- Subject to being <strong>orthogonal</strong> to the first principal component <span class="math inline">\(\mathbf{v}_1\)</span>.</p>
<hr />
<div id="optimization-problem-1" class="section level4 hasAnchor" number="5.2.2.1">
<h4><span class="header-section-number">5.2.2.1</span> Optimization Problem<a href="principal-component-analysis.html#optimization-problem-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We want to maximize:
<span class="math display">\[
\text{Var}(\mathbf{Xv}_2) = \mathbf{v}_2&#39; \mathbf{S} \mathbf{v}_2
\]</span>
subject to:
<span class="math display">\[
\|\mathbf{v}_2\| = 1 \quad \text{and} \quad \mathbf{v}_2&#39; \mathbf{v}_1 = 0.
\]</span></p>
<hr />
</div>
<div id="solution-using-lagrange-multipliers" class="section level4 hasAnchor" number="5.2.2.2">
<h4><span class="header-section-number">5.2.2.2</span> Solution Using Lagrange Multipliers<a href="principal-component-analysis.html#solution-using-lagrange-multipliers" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The constraint <span class="math inline">\(\mathbf{v}_2&#39; \mathbf{v}_1 = 0\)</span> ensures orthogonality to the first principal component.</p>
<p>Solving this optimization problem leads to:
<span class="math display">\[
\mathbf{S} \mathbf{v}_2 = \lambda_2 \mathbf{v}_2
\]</span>
where <span class="math inline">\(\mathbf{v}_2\)</span> is the <strong>eigenvector associated with the second largest eigenvalue</strong> <span class="math inline">\(\lambda_2\)</span> of <span class="math inline">\(\mathbf{S}\)</span>.</p>
<hr />
</div>
<div id="intuition-1" class="section level4 hasAnchor" number="5.2.2.3">
<h4><span class="header-section-number">5.2.2.3</span> Intuition<a href="principal-component-analysis.html#intuition-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Since the first principal component <span class="math inline">\(\mathbf{v}_1\)</span> has already captured the maximum variance, the second principal component <span class="math inline">\(\mathbf{v}_2\)</span> captures the next largest amount of variance while being orthogonal to <span class="math inline">\(\mathbf{v}_1\)</span>. We can continue in this way to find the next Principal Components.</p>
</div>
</div>
</div>
<div id="parafac-decomposition" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> PARAFAC decomposition<a href="principal-component-analysis.html#parafac-decomposition" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <strong>PARAFAC decomposition</strong> (also known as CANDECOMP/PARAFAC or CP decomposition) is a natural extension of the concept of <strong>Principal Component Analysis (PCA)</strong> from matrices to <strong>higher-order tensors</strong>.</p>
<hr />
<div id="what-is-a-tensor" class="section level3 hasAnchor" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> What is a Tensor?<a href="principal-component-analysis.html#what-is-a-tensor" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A <strong>tensor</strong> is a multi-dimensional array, extending the concept of vectors (1D arrays) and matrices (2D arrays) to higher dimensions:
- A <strong>vector</strong> is a tensor of order 1.
- A <strong>matrix</strong> is a tensor of order 2.
- A <strong>tensor</strong> of order <span class="math inline">\(N\)</span> has <span class="math inline">\(N\)</span> dimensions (also called modes).</p>
<p>For example:
- An image is a matrix (2D tensor).
- A color video can be represented as a 3D tensor (height × width × time).</p>
<hr />
</div>
<div id="motivation-why-generalize-pca-to-tensors" class="section level3 hasAnchor" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Motivation: Why Generalize PCA to Tensors?<a href="principal-component-analysis.html#motivation-why-generalize-pca-to-tensors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>PCA works well with matrices, but many real-world data are naturally represented as tensors:
- <strong>Psychology</strong>: Data from multiple individuals across multiple tests and times.
- <strong>Signal Processing</strong>: Multi-channel EEG data recorded over time.
- <strong>Chemometrics</strong>: Spectral data collected over different conditions.</p>
<p>PCA provides a low-rank approximation for matrices by decomposing them into principal components. The PARAFAC decomposition extends this idea to tensors, finding components along each dimension simultaneously.</p>
<hr />
</div>
<div id="parafac-decomposition-definition" class="section level3 hasAnchor" number="5.3.3">
<h3><span class="header-section-number">5.3.3</span> PARAFAC Decomposition Definition<a href="principal-component-analysis.html#parafac-decomposition-definition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Given a tensor <span class="math inline">\(\mathcal{X} \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N}\)</span>, the <strong>PARAFAC decomposition</strong> expresses <span class="math inline">\(\mathcal{X}\)</span> as a sum of <span class="math inline">\(R\)</span> rank-1 tensors:</p>
<p><span class="math display">\[
\mathcal{X} \approx \sum_{r=1}^{R} \mathbf{a}_r^{(1)} \circ \mathbf{a}_r^{(2)} \circ \cdots \circ \mathbf{a}_r^{(N)},
\]</span></p>
<p>where:
- <span class="math inline">\(R\)</span> is the <strong>rank</strong> of the decomposition.
- <span class="math inline">\(\mathbf{a}_r^{(n)} \in \mathbb{R}^{I_n}\)</span> are vectors along the <span class="math inline">\(n\)</span>-th mode.
- <span class="math inline">\(\circ\)</span> denotes the <strong>outer product</strong>.
- Each rank-1 tensor <span class="math inline">\(\mathbf{a}_r^{(1)} \circ \mathbf{a}_r^{(2)} \circ \cdots \circ \mathbf{a}_r^{(N)}\)</span> is analogous to a <strong>principal component</strong> in PCA.</p>
<hr />
</div>
<div id="how-is-parafac-related-to-pca" class="section level3 hasAnchor" number="5.3.4">
<h3><span class="header-section-number">5.3.4</span> How is PARAFAC Related to PCA?<a href="principal-component-analysis.html#how-is-parafac-related-to-pca" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p><strong>PCA</strong> decomposes a matrix <span class="math inline">\(\mathbf{X}\)</span> into a sum of rank-1 matrices:
<span class="math display">\[
\mathbf{X} \approx \sum_{r=1}^{R} \sigma_r \mathbf{u}_r \mathbf{v}_r&#39;,
\]</span>
where <span class="math inline">\(\mathbf{u}_r\)</span> and <span class="math inline">\(\mathbf{v}_r\)</span> are left and right singular vectors, and <span class="math inline">\(\sigma_r\)</span> are singular values.</p></li>
<li><p><strong>PARAFAC</strong> generalizes this concept by decomposing a tensor <span class="math inline">\(\mathcal{X}\)</span> into a sum of rank-1 tensors. Just like PCA finds directions that capture the most variance in a matrix, PARAFAC finds <strong>multi-dimensional factors</strong> that capture the most structure in a tensor.</p></li>
</ul>
<hr />
</div>
<div id="key-properties-of-parafac-decomposition" class="section level3 hasAnchor" number="5.3.5">
<h3><span class="header-section-number">5.3.5</span> Key Properties of PARAFAC Decomposition<a href="principal-component-analysis.html#key-properties-of-parafac-decomposition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><strong>Uniqueness</strong>: Unlike PCA (where different rotations of principal components can yield the same solution), the PARAFAC decomposition is unique under mild conditions. This uniqueness makes PARAFAC particularly useful for interpretability.</li>
<li><strong>Low-rank Approximation</strong>: PARAFAC provides a low-rank approximation of tensors, analogous to PCA for matrices.</li>
<li><strong>Interpretability</strong>: Each component can be interpreted as a factor along each mode, making it valuable in fields like chemometrics and neuroscience.</li>
</ul>
</div>
</div>
<div id="high-dimensional-pca" class="section level2 hasAnchor" number="5.4">
<h2><span class="header-section-number">5.4</span> High-Dimensional PCA<a href="principal-component-analysis.html#high-dimensional-pca" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When the number of features or variables is <span class="math inline">\(p\)</span>, computing the eigenvalue decomposition or SVD can be prohebitely expensive. We can have some alternatives depending on the</p>
<div id="high-dimensional-pca-p-gg-n" class="section level3 hasAnchor" number="5.4.1">
<h3><span class="header-section-number">5.4.1</span> High-Dimensional PCA (<span class="math inline">\(p \gg n\)</span>)<a href="principal-component-analysis.html#high-dimensional-pca-p-gg-n" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When the number of features <span class="math inline">\(p\)</span> is much larger than the number of samples <span class="math inline">\(n\)</span> (a common scenario in high-dimensional data analysis), directly computing the PCA decomposition from the <span class="math inline">\(p \times p\)</span> covariance matrix <span class="math inline">\(\mathbf{X}&#39; \mathbf{X}\)</span> is computationally expensive.</p>
<p>To address this, we can use the matrix <span class="math inline">\(\mathbf{X} \mathbf{X}&#39;\)</span> instead, which is an <span class="math inline">\(n \times n\)</span> matrix, making the computation much more efficient.</p>
<hr />
<div id="key-idea" class="section level4 hasAnchor" number="5.4.1.1">
<h4><span class="header-section-number">5.4.1.1</span> Key Idea<a href="principal-component-analysis.html#key-idea" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Instead of directly solving the eigenvalue problem:
<span class="math display">\[
\mathbf{X}&#39; \mathbf{X} \mathbf{v}_i = \lambda_i \mathbf{v}_i,
\]</span>
we solve the equivalent problem:
<span class="math display">\[
\mathbf{X} \mathbf{X}&#39; \mathbf{u}_i = \lambda_i \mathbf{u}_i,
\]</span>
where <span class="math inline">\(\mathbf{u}_i\)</span> are the left singular vectors of <span class="math inline">\(\mathbf{X}\)</span>.</p>
<hr />
</div>
<div id="steps-for-efficient-computation" class="section level4 hasAnchor" number="5.4.1.2">
<h4><span class="header-section-number">5.4.1.2</span> Steps for Efficient Computation<a href="principal-component-analysis.html#steps-for-efficient-computation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li><strong>Form the <span class="math inline">\(n \times n\)</span> matrix</strong> <span class="math inline">\(\mathbf{X} \mathbf{X}&#39;\)</span> instead of <span class="math inline">\(\mathbf{X}&#39; \mathbf{X}\)</span>.</li>
<li><strong>Compute the eigenvalues and eigenvectors</strong> of <span class="math inline">\(\mathbf{X} \mathbf{X}&#39;\)</span>:
<ul>
<li><span class="math inline">\(\mathbf{X} \mathbf{X}&#39; \mathbf{u}_i = \sigma_i^2 \mathbf{u}_i.\)</span></li>
</ul></li>
<li><strong>Obtain singular values</strong> <span class="math inline">\(\sigma_i\)</span> as the square roots of the eigenvalues.</li>
<li><strong>Compute the principal directions</strong> <span class="math inline">\(\mathbf{v}_i\)</span> from the <span class="math inline">\(\mathbf{u}_i\)</span> vectors:
<span class="math display">\[
\mathbf{v}_i = \frac{\mathbf{X}&#39; \mathbf{u}_i}{\sigma_i}.
\]</span></li>
<li><strong>Compute the PCA scores</strong> directly as:
<span class="math display">\[
\mathbf{X} \mathbf{v}_i = \sigma_i \mathbf{u}_i.
\]</span></li>
</ol>
<hr />
</div>
<div id="why-does-this-work" class="section level4 hasAnchor" number="5.4.1.3">
<h4><span class="header-section-number">5.4.1.3</span> Why Does This Work?<a href="principal-component-analysis.html#why-does-this-work" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>The matrix <span class="math inline">\(\mathbf{X} \mathbf{X}&#39;\)</span> is much smaller when <span class="math inline">\(n \ll p\)</span>, making eigen-decomposition computationally cheaper.</li>
<li>Once we have the left singular vectors <span class="math inline">\(\mathbf{u}_i\)</span> from <span class="math inline">\(\mathbf{X} \mathbf{X}&#39;\)</span>, we can easily find the right singular vectors <span class="math inline">\(\mathbf{v}_i\)</span> without ever forming the large <span class="math inline">\(p \times p\)</span> matrix.</li>
</ul>
<p>This approach is crucial for handling high-dimensional data efficiently, ensuring that PCA remains computationally feasible even when <span class="math inline">\(p \gg n\)</span>.</p>
<p>Here’s an explanation with added comments to your provided R Markdown code, keeping the code unchanged:</p>
<hr />
</div>
<div id="efficient-computation-pca-p-gg-n" class="section level4 hasAnchor" number="5.4.1.4">
<h4><span class="header-section-number">5.4.1.4</span> Efficient Computation PCA <span class="math inline">\(p \gg n\)</span><a href="principal-component-analysis.html#efficient-computation-pca-p-gg-n" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="pca-methods" class="section level5 hasAnchor" number="5.4.1.4.1">
<h5><span class="header-section-number">5.4.1.4.1</span> PCA Methods<a href="principal-component-analysis.html#pca-methods" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="principal-component-analysis.html#cb86-1" tabindex="-1"></a><span class="co"># PCA using X&#39;X (traditional method)</span></span>
<span id="cb86-2"><a href="principal-component-analysis.html#cb86-2" tabindex="-1"></a>pca_traditional <span class="ot">&lt;-</span> <span class="cf">function</span>(X) {</span>
<span id="cb86-3"><a href="principal-component-analysis.html#cb86-3" tabindex="-1"></a>  n      <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb86-4"><a href="principal-component-analysis.html#cb86-4" tabindex="-1"></a>  S      <span class="ot">&lt;-</span> <span class="fu">crossprod</span>(X)                 <span class="co"># X&#39;X</span></span>
<span id="cb86-5"><a href="principal-component-analysis.html#cb86-5" tabindex="-1"></a>  eig    <span class="ot">&lt;-</span> <span class="fu">eigen</span>(S, <span class="at">symmetric =</span> <span class="cn">TRUE</span>)   <span class="co"># Eigen decomposition</span></span>
<span id="cb86-6"><a href="principal-component-analysis.html#cb86-6" tabindex="-1"></a>  priCom <span class="ot">&lt;-</span> eig<span class="sc">$</span>vectors                  <span class="co"># PC</span></span>
<span id="cb86-7"><a href="principal-component-analysis.html#cb86-7" tabindex="-1"></a>  varExp <span class="ot">&lt;-</span> eig<span class="sc">$</span>values <span class="sc">/</span> <span class="fu">sum</span>(eig<span class="sc">$</span>values) <span class="co"># Explain Variance by PC</span></span>
<span id="cb86-8"><a href="principal-component-analysis.html#cb86-8" tabindex="-1"></a>  pcaSco <span class="ot">&lt;-</span> X <span class="sc">%*%</span> eig<span class="sc">$</span>vectors            <span class="co"># PCA scores</span></span>
<span id="cb86-9"><a href="principal-component-analysis.html#cb86-9" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">priCom =</span> priCom,</span>
<span id="cb86-10"><a href="principal-component-analysis.html#cb86-10" tabindex="-1"></a>              <span class="at">pcaSco =</span> pcaSco,</span>
<span id="cb86-11"><a href="principal-component-analysis.html#cb86-11" tabindex="-1"></a>              <span class="at">varExp =</span> varExp))</span>
<span id="cb86-12"><a href="principal-component-analysis.html#cb86-12" tabindex="-1"></a>}</span>
<span id="cb86-13"><a href="principal-component-analysis.html#cb86-13" tabindex="-1"></a></span>
<span id="cb86-14"><a href="principal-component-analysis.html#cb86-14" tabindex="-1"></a><span class="co"># PCA using X X&#39; (efficient method)</span></span>
<span id="cb86-15"><a href="principal-component-analysis.html#cb86-15" tabindex="-1"></a>pca_efficient <span class="ot">&lt;-</span> <span class="cf">function</span>(X) {</span>
<span id="cb86-16"><a href="principal-component-analysis.html#cb86-16" tabindex="-1"></a>  S               <span class="ot">&lt;-</span> <span class="fu">tcrossprod</span>(X)                                <span class="co"># X X&#39;</span></span>
<span id="cb86-17"><a href="principal-component-analysis.html#cb86-17" tabindex="-1"></a>  eig             <span class="ot">&lt;-</span> <span class="fu">eigen</span>(S, <span class="at">symmetric =</span> <span class="cn">TRUE</span>)                   <span class="co"># Eigen decomposition</span></span>
<span id="cb86-18"><a href="principal-component-analysis.html#cb86-18" tabindex="-1"></a>  singular_values <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(eig<span class="sc">$</span>values)</span>
<span id="cb86-19"><a href="principal-component-analysis.html#cb86-19" tabindex="-1"></a>  priCom          <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">t</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> eig<span class="sc">$</span>vectors) <span class="sc">/</span> singular_values) <span class="co"># Compute V = X&#39;U / Sigma</span></span>
<span id="cb86-20"><a href="principal-component-analysis.html#cb86-20" tabindex="-1"></a>  pcaSco          <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">t</span>(eig<span class="sc">$</span>vectors) <span class="sc">*</span> singular_values)          <span class="co"># PCA scores</span></span>
<span id="cb86-21"><a href="principal-component-analysis.html#cb86-21" tabindex="-1"></a>  varExp          <span class="ot">&lt;-</span> eig<span class="sc">$</span>values <span class="sc">/</span> <span class="fu">sum</span>(eig<span class="sc">$</span>values)                 <span class="co"># Explain Variance by PC</span></span>
<span id="cb86-22"><a href="principal-component-analysis.html#cb86-22" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">priCom =</span> priCom,</span>
<span id="cb86-23"><a href="principal-component-analysis.html#cb86-23" tabindex="-1"></a>              <span class="at">pcaSco =</span> pcaSco,</span>
<span id="cb86-24"><a href="principal-component-analysis.html#cb86-24" tabindex="-1"></a>              <span class="at">varExp =</span> varExp))</span>
<span id="cb86-25"><a href="principal-component-analysis.html#cb86-25" tabindex="-1"></a>}</span></code></pre></div>
<p><em>Explanation</em>:</p>
<ul>
<li>The <strong>traditional method</strong> computes PCA from <span class="math inline">\(X&#39;X\)</span>, which is large when <span class="math inline">\(p \gg n\)</span>.</li>
<li>The <strong>efficient method</strong> computes PCA from <span class="math inline">\(X X&#39;\)</span>, a smaller matrix when <span class="math inline">\(p \gg n\)</span>, then derives the principal components and scores from that decomposition.</li>
</ul>
<hr />
</div>
<div id="simulate-a-data-set" class="section level5 hasAnchor" number="5.4.1.4.2">
<h5><span class="header-section-number">5.4.1.4.2</span> Simulate a Data Set<a href="principal-component-analysis.html#simulate-a-data-set" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="principal-component-analysis.html#cb87-1" tabindex="-1"></a><span class="co"># Simulate high-dimensional data</span></span>
<span id="cb87-2"><a href="principal-component-analysis.html#cb87-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb87-3"><a href="principal-component-analysis.html#cb87-3" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span>    <span class="co"># Number of samples</span></span>
<span id="cb87-4"><a href="principal-component-analysis.html#cb87-4" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">1000</span>   <span class="co"># Number of features</span></span>
<span id="cb87-5"><a href="principal-component-analysis.html#cb87-5" tabindex="-1"></a></span>
<span id="cb87-6"><a href="principal-component-analysis.html#cb87-6" tabindex="-1"></a><span class="co"># Design Matrix</span></span>
<span id="cb87-7"><a href="principal-component-analysis.html#cb87-7" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">data =</span> <span class="cn">NA</span>,</span>
<span id="cb87-8"><a href="principal-component-analysis.html#cb87-8" tabindex="-1"></a>            <span class="at">nrow =</span> n,</span>
<span id="cb87-9"><a href="principal-component-analysis.html#cb87-9" tabindex="-1"></a>            <span class="at">ncol =</span> <span class="dv">0</span>)</span>
<span id="cb87-10"><a href="principal-component-analysis.html#cb87-10" tabindex="-1"></a></span>
<span id="cb87-11"><a href="principal-component-analysis.html#cb87-11" tabindex="-1"></a><span class="co"># Block-Correlation Matrix in Blocks</span></span>
<span id="cb87-12"><a href="principal-component-analysis.html#cb87-12" tabindex="-1"></a>sizBlo <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb87-13"><a href="principal-component-analysis.html#cb87-13" tabindex="-1"></a>disCor <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb87-14"><a href="principal-component-analysis.html#cb87-14" tabindex="-1"></a>C <span class="ot">&lt;-</span> <span class="fu">exp</span>(<span class="sc">-</span> disCor <span class="sc">*</span> <span class="fu">abs</span>(<span class="fu">rep</span>(<span class="dv">1</span>, sizBlo) <span class="sc">%*%</span> <span class="fu">t</span>(<span class="dv">1</span><span class="sc">:</span>sizBlo) <span class="sc">-</span> (<span class="dv">1</span><span class="sc">:</span>sizBlo) <span class="sc">%*%</span> <span class="fu">t</span>(<span class="fu">rep</span>(<span class="dv">1</span>, sizBlo))))</span>
<span id="cb87-15"><a href="principal-component-analysis.html#cb87-15" tabindex="-1"></a></span>
<span id="cb87-16"><a href="principal-component-analysis.html#cb87-16" tabindex="-1"></a><span class="co"># Simulates the Design Matrix</span></span>
<span id="cb87-17"><a href="principal-component-analysis.html#cb87-17" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(p <span class="sc">/</span> sizBlo)){</span>
<span id="cb87-18"><a href="principal-component-analysis.html#cb87-18" tabindex="-1"></a>  X <span class="ot">&lt;-</span>  <span class="fu">cbind</span>(X, mvtnorm<span class="sc">::</span><span class="fu">rmvnorm</span>(<span class="at">n =</span> n, <span class="at">sigma =</span> C))</span>
<span id="cb87-19"><a href="principal-component-analysis.html#cb87-19" tabindex="-1"></a>}</span></code></pre></div>
<p><em>Explanation</em>:</p>
<ul>
<li>Simulates a dataset with <span class="math inline">\(n = 100\)</span> samples and <span class="math inline">\(p = 1000\)</span> features.</li>
<li>Correlation is introduced within blocks of features using an exponential decay structure with <span class="math inline">\(\rho = 0.5\)</span>.</li>
</ul>
<hr />
</div>
<div id="compare-results" class="section level5 hasAnchor" number="5.4.1.4.3">
<h5><span class="header-section-number">5.4.1.4.3</span> Compare Results<a href="principal-component-analysis.html#compare-results" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="principal-component-analysis.html#cb88-1" tabindex="-1"></a><span class="co"># Performs PCA by Both Methods</span></span>
<span id="cb88-2"><a href="principal-component-analysis.html#cb88-2" tabindex="-1"></a>outTra <span class="ot">&lt;-</span> <span class="fu">pca_traditional</span>(<span class="at">X =</span> X)</span>
<span id="cb88-3"><a href="principal-component-analysis.html#cb88-3" tabindex="-1"></a>outEff <span class="ot">&lt;-</span> <span class="fu">pca_efficient</span>(<span class="at">X =</span> X)</span>
<span id="cb88-4"><a href="principal-component-analysis.html#cb88-4" tabindex="-1"></a></span>
<span id="cb88-5"><a href="principal-component-analysis.html#cb88-5" tabindex="-1"></a><span class="co"># Compare First Two Principal Components</span></span>
<span id="cb88-6"><a href="principal-component-analysis.html#cb88-6" tabindex="-1"></a><span class="fu">cbind</span>(outTra<span class="sc">$</span>priCom[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>], outEff<span class="sc">$</span>priCom[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>])</span></code></pre></div>
<pre><code>##               [,1]         [,2]         [,3]         [,4]
##  [1,] -0.001202379  0.004997560  0.001202379  0.004997560
##  [2,]  0.014206244  0.032971374 -0.014206244  0.032971374
##  [3,]  0.017112378  0.030209540 -0.017112378  0.030209540
##  [4,]  0.008336267  0.029518877 -0.008336267  0.029518877
##  [5,]  0.007510180  0.041815584 -0.007510180  0.041815584
##  [6,] -0.013239150 -0.009654624  0.013239150 -0.009654624
##  [7,]  0.008449956  0.042200920 -0.008449956  0.042200920
##  [8,]  0.028501647  0.026011567 -0.028501647  0.026011567
##  [9,]  0.008413915  0.004696628 -0.008413915  0.004696628
## [10,]  0.018911051 -0.012567287 -0.018911051 -0.012567287</code></pre>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="principal-component-analysis.html#cb90-1" tabindex="-1"></a><span class="co"># Compares the First Two Principal Component Scores</span></span>
<span id="cb90-2"><a href="principal-component-analysis.html#cb90-2" tabindex="-1"></a><span class="fu">cbind</span>(outTra<span class="sc">$</span>pcaSco[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>], outEff<span class="sc">$</span>pcaSco[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>])</span></code></pre></div>
<pre><code>##             [,1]       [,2]       [,3]       [,4]
##  [1,] -6.5413193 -4.0702069  6.5413193 -4.0702069
##  [2,] -8.5679536  0.3470996  8.5679536  0.3470996
##  [3,] -2.7203524  2.0908395  2.7203524  2.0908395
##  [4,]  2.7429942  3.8261368 -2.7429942  3.8261368
##  [5,] -0.1077707 -6.3139474  0.1077707 -6.3139474
##  [6,]  0.2253885  0.2471944 -0.2253885  0.2471944
##  [7,] -1.3734419 -4.6083631  1.3734419 -4.6083631
##  [8,]  3.2775010 -8.2332778 -3.2775010 -8.2332778
##  [9,] -3.3366402  1.1154150  3.3366402  1.1154150
## [10,]  8.1907932 -0.2565279 -8.1907932 -0.2565279</code></pre>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="principal-component-analysis.html#cb92-1" tabindex="-1"></a><span class="co"># Plots the Variance Explained</span></span>
<span id="cb92-2"><a href="principal-component-analysis.html#cb92-2" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar   =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">0</span>))</span>
<span id="cb92-3"><a href="principal-component-analysis.html#cb92-3" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb92-4"><a href="principal-component-analysis.html#cb92-4" tabindex="-1"></a><span class="fu">plot</span>(outTra<span class="sc">$</span>varExp[<span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>], <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb92-5"><a href="principal-component-analysis.html#cb92-5" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar   =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">0</span>))</span>
<span id="cb92-6"><a href="principal-component-analysis.html#cb92-6" tabindex="-1"></a><span class="fu">plot</span>(outTra<span class="sc">$</span>varExp[<span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>])</span></code></pre></div>
<p><img src="_main_files/figure-html/pca-results-comparison-1.png" width="672" /></p>
<p><em>Explanation</em>:</p>
<ul>
<li>Compares the results from both PCA methods to ensure they are identical.</li>
<li>Displays the first two principal components and scores side by side.</li>
<li>Plots the variance explained by the top 50 components for both methods.</li>
</ul>
<hr />
</div>
<div id="computational-efficiency-comparison" class="section level5 hasAnchor" number="5.4.1.4.4">
<h5><span class="header-section-number">5.4.1.4.4</span> Computational Efficiency Comparison<a href="principal-component-analysis.html#computational-efficiency-comparison" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="principal-component-analysis.html#cb93-1" tabindex="-1"></a><span class="co"># Load necessary library</span></span>
<span id="cb93-2"><a href="principal-component-analysis.html#cb93-2" tabindex="-1"></a><span class="fu">library</span>(microbenchmark)</span>
<span id="cb93-3"><a href="principal-component-analysis.html#cb93-3" tabindex="-1"></a></span>
<span id="cb93-4"><a href="principal-component-analysis.html#cb93-4" tabindex="-1"></a><span class="co"># Benchmark both methods</span></span>
<span id="cb93-5"><a href="principal-component-analysis.html#cb93-5" tabindex="-1"></a>benchmark_results <span class="ot">&lt;-</span> <span class="fu">microbenchmark</span>(</span>
<span id="cb93-6"><a href="principal-component-analysis.html#cb93-6" tabindex="-1"></a>  <span class="at">Traditional =</span> <span class="fu">pca_traditional</span>(X),</span>
<span id="cb93-7"><a href="principal-component-analysis.html#cb93-7" tabindex="-1"></a>  <span class="at">Efficient   =</span> <span class="fu">pca_efficient</span>(X),</span>
<span id="cb93-8"><a href="principal-component-analysis.html#cb93-8" tabindex="-1"></a>  <span class="at">times       =</span> <span class="dv">5</span></span>
<span id="cb93-9"><a href="principal-component-analysis.html#cb93-9" tabindex="-1"></a>)</span>
<span id="cb93-10"><a href="principal-component-analysis.html#cb93-10" tabindex="-1"></a></span>
<span id="cb93-11"><a href="principal-component-analysis.html#cb93-11" tabindex="-1"></a><span class="co"># Print the results</span></span>
<span id="cb93-12"><a href="principal-component-analysis.html#cb93-12" tabindex="-1"></a><span class="fu">print</span>(benchmark_results)</span></code></pre></div>
<pre><code>## Unit: milliseconds
##         expr      min       lq      mean   median       uq      max neval
##  Traditional 475.6833 480.4067 482.95812 485.1269 485.9995 487.5742     5
##    Efficient   4.7050   4.7711   6.06798   5.0668   5.3364  10.4606     5</code></pre>
<p><em>Explanation</em>:</p>
<ul>
<li>Uses <code>microbenchmark</code> to measure and compare the runtime of both PCA methods.</li>
<li>Demonstrates the computational advantage of the efficient method when <span class="math inline">\(p \gg n\)</span>.</li>
</ul>
<hr />
</div>
<div id="larger-example" class="section level5 hasAnchor" number="5.4.1.4.5">
<h5><span class="header-section-number">5.4.1.4.5</span> Larger Example<a href="principal-component-analysis.html#larger-example" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="principal-component-analysis.html#cb95-1" tabindex="-1"></a><span class="co"># Simulate high-dimensional data</span></span>
<span id="cb95-2"><a href="principal-component-analysis.html#cb95-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb95-3"><a href="principal-component-analysis.html#cb95-3" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span>    <span class="co"># Number of samples</span></span>
<span id="cb95-4"><a href="principal-component-analysis.html#cb95-4" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">2000</span>   <span class="co"># Number of features</span></span>
<span id="cb95-5"><a href="principal-component-analysis.html#cb95-5" tabindex="-1"></a></span>
<span id="cb95-6"><a href="principal-component-analysis.html#cb95-6" tabindex="-1"></a><span class="co"># Design Matrix</span></span>
<span id="cb95-7"><a href="principal-component-analysis.html#cb95-7" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">data =</span> <span class="cn">NA</span>,</span>
<span id="cb95-8"><a href="principal-component-analysis.html#cb95-8" tabindex="-1"></a>            <span class="at">nrow =</span> n,</span>
<span id="cb95-9"><a href="principal-component-analysis.html#cb95-9" tabindex="-1"></a>            <span class="at">ncol =</span> <span class="dv">0</span>)</span>
<span id="cb95-10"><a href="principal-component-analysis.html#cb95-10" tabindex="-1"></a></span>
<span id="cb95-11"><a href="principal-component-analysis.html#cb95-11" tabindex="-1"></a><span class="co"># Block-Correlation Matrix in Blocks</span></span>
<span id="cb95-12"><a href="principal-component-analysis.html#cb95-12" tabindex="-1"></a>sizBlo <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb95-13"><a href="principal-component-analysis.html#cb95-13" tabindex="-1"></a>disCor <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb95-14"><a href="principal-component-analysis.html#cb95-14" tabindex="-1"></a>C <span class="ot">&lt;-</span> <span class="fu">exp</span>(<span class="sc">-</span> disCor <span class="sc">*</span> <span class="fu">abs</span>(<span class="fu">rep</span>(<span class="dv">1</span>, sizBlo) <span class="sc">%*%</span> <span class="fu">t</span>(<span class="dv">1</span><span class="sc">:</span>sizBlo) <span class="sc">-</span> (<span class="dv">1</span><span class="sc">:</span>sizBlo) <span class="sc">%*%</span> <span class="fu">t</span>(<span class="fu">rep</span>(<span class="dv">1</span>, sizBlo))))</span>
<span id="cb95-15"><a href="principal-component-analysis.html#cb95-15" tabindex="-1"></a></span>
<span id="cb95-16"><a href="principal-component-analysis.html#cb95-16" tabindex="-1"></a><span class="co"># Simulates the Design Matrix</span></span>
<span id="cb95-17"><a href="principal-component-analysis.html#cb95-17" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(p <span class="sc">/</span> sizBlo)){</span>
<span id="cb95-18"><a href="principal-component-analysis.html#cb95-18" tabindex="-1"></a>  X <span class="ot">&lt;-</span>  <span class="fu">cbind</span>(X, mvtnorm<span class="sc">::</span><span class="fu">rmvnorm</span>(<span class="at">n =</span> n, <span class="at">sigma =</span> C))</span>
<span id="cb95-19"><a href="principal-component-analysis.html#cb95-19" tabindex="-1"></a>}</span>
<span id="cb95-20"><a href="principal-component-analysis.html#cb95-20" tabindex="-1"></a></span>
<span id="cb95-21"><a href="principal-component-analysis.html#cb95-21" tabindex="-1"></a><span class="co"># Benchmark both methods</span></span>
<span id="cb95-22"><a href="principal-component-analysis.html#cb95-22" tabindex="-1"></a>benchmark_results <span class="ot">&lt;-</span> <span class="fu">microbenchmark</span>(</span>
<span id="cb95-23"><a href="principal-component-analysis.html#cb95-23" tabindex="-1"></a>  <span class="at">Traditional =</span> <span class="fu">pca_traditional</span>(X),</span>
<span id="cb95-24"><a href="principal-component-analysis.html#cb95-24" tabindex="-1"></a>  <span class="at">Efficient   =</span> <span class="fu">pca_efficient</span>(X),</span>
<span id="cb95-25"><a href="principal-component-analysis.html#cb95-25" tabindex="-1"></a>  <span class="at">times       =</span> <span class="dv">5</span></span>
<span id="cb95-26"><a href="principal-component-analysis.html#cb95-26" tabindex="-1"></a>)</span>
<span id="cb95-27"><a href="principal-component-analysis.html#cb95-27" tabindex="-1"></a></span>
<span id="cb95-28"><a href="principal-component-analysis.html#cb95-28" tabindex="-1"></a><span class="co"># Print the results</span></span>
<span id="cb95-29"><a href="principal-component-analysis.html#cb95-29" tabindex="-1"></a><span class="fu">print</span>(benchmark_results)</span></code></pre></div>
<pre><code>## Unit: milliseconds
##         expr       min        lq      mean    median        uq       max neval
##  Traditional 3892.6445 3925.5093 4008.5234 3992.7742 4035.0165 4196.6727     5
##    Efficient    9.0702    9.1546    9.5051    9.3315    9.9286   10.0406     5</code></pre>
<p><em>Explanation</em>:</p>
<ul>
<li>Repeats the PCA and timing comparison on a larger dataset with <span class="math inline">\(p = 2000\)</span> features to demonstrate the increasing efficiency of the <span class="math inline">\(X X&#39;\)</span>-based method as <span class="math inline">\(p\)</span> grows.</li>
</ul>
<hr />
</div>
</div>
</div>
<div id="randomized-pca" class="section level3 hasAnchor" number="5.4.2">
<h3><span class="header-section-number">5.4.2</span> Randomized PCA<a href="principal-component-analysis.html#randomized-pca" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The use of <strong>random matrices</strong> in the computation of <strong>PCA</strong> has gained significant attention due to its potential to make high-dimensional computations more efficient. This is particularly important when dealing with large datasets where traditional PCA methods become computationally expensive.</p>
<hr />
<div id="random-matrices-in-pca" class="section level4 hasAnchor" number="5.4.2.1">
<h4><span class="header-section-number">5.4.2.1</span> Random matrices in PCA<a href="principal-component-analysis.html#random-matrices-in-pca" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>When computing PCA, especially in high-dimensional settings, the major computational bottleneck is often the <strong>singular value decomposition (SVD)</strong> of large matrices. <strong>Random matrix methods</strong> offer an efficient approximation by projecting the original high-dimensional data into a lower-dimensional subspace, while preserving most of its structure.</p>
<hr />
</div>
<div id="randomized-pca-rpca" class="section level4 hasAnchor" number="5.4.2.2">
<h4><span class="header-section-number">5.4.2.2</span> Randomized PCA (RPCA)<a href="principal-component-analysis.html#randomized-pca-rpca" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Randomized PCA</strong> leverages random matrices to approximate the principal components. The key steps are:</p>
<ol style="list-style-type: decimal">
<li><strong>Random Projection</strong>:
<ul>
<li>Instead of working directly with the full data matrix <span class="math inline">\(\mathbf{X}\)</span>, we multiply it by a random matrix <span class="math inline">\(\mathbf{\Omega}\)</span> to form a lower-dimensional matrix <span class="math inline">\(\mathbf{Y}\)</span>:
<span class="math display">\[
\mathbf{Y} = \mathbf{X} \mathbf{\Omega}
\]</span></li>
<li>Here, <span class="math inline">\(\mathbf{\Omega}\)</span> is typically a Gaussian random matrix, where each entry is drawn from a normal distribution <span class="math inline">\(N(0, 1)\)</span>.</li>
</ul></li>
<li><strong>Compute PCA in Lower Dimension</strong>:
<ul>
<li>Perform PCA (or SVD) on <span class="math inline">\(\mathbf{Y}\)</span> instead of <span class="math inline">\(\mathbf{X}\)</span>, which is much cheaper computationally.</li>
<li>Since <span class="math inline">\(\mathbf{Y}\)</span> has fewer columns, the complexity is greatly reduced.</li>
</ul></li>
<li><strong>Recover Approximate Components</strong>:
<ul>
<li>The principal components of <span class="math inline">\(\mathbf{X}\)</span> are approximated using the computed components of <span class="math inline">\(\mathbf{Y}\)</span>.</li>
</ul></li>
</ol>
<hr />
</div>
<div id="why-use-random-matrices" class="section level4 hasAnchor" number="5.4.2.3">
<h4><span class="header-section-number">5.4.2.3</span> Why use random matrices?<a href="principal-component-analysis.html#why-use-random-matrices" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>Efficiency</strong>: Reduces the dimensionality before performing SVD, thus significantly speeding up the computation.</li>
<li><strong>Scalability</strong>: Ideal for large datasets where traditional PCA is computationally intensive.</li>
<li><strong>Memory-friendly</strong>: Works well in environments with limited memory by avoiding operations on the full data matrix.</li>
</ul>
<hr />
</div>
<div id="theoretical-foundations-from-random-matrix-theory" class="section level4 hasAnchor" number="5.4.2.4">
<h4><span class="header-section-number">5.4.2.4</span> Theoretical foundations from random matrix theory<a href="principal-component-analysis.html#theoretical-foundations-from-random-matrix-theory" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Random matrix theory</strong> provides tools to understand how well the random projections preserve the original structure of the data. Key results such as the <strong>Johnson-Lindenstrauss lemma</strong> ensure that the distances between points are approximately preserved under random projections. This is crucial because PCA relies on variance and distances within the dataset.</p>
<hr />
</div>
<div id="when-to-use-random-matrix-methods-in-pca" class="section level4 hasAnchor" number="5.4.2.5">
<h4><span class="header-section-number">5.4.2.5</span> When to use random matrix methods in PCA?<a href="principal-component-analysis.html#when-to-use-random-matrix-methods-in-pca" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>When the dataset is <strong>too large</strong> to store or process in full.</li>
<li>When <strong>approximate principal components</strong> are acceptable.</li>
<li>In <strong>streaming data</strong> scenarios where full PCA computations at each step are infeasible.</li>
<li>For <strong>distributed computing</strong> environments where random projections can be easily parallelized.</li>
</ul>
<hr />
</div>
<div id="limitations" class="section level4 hasAnchor" number="5.4.2.6">
<h4><span class="header-section-number">5.4.2.6</span> Limitations<a href="principal-component-analysis.html#limitations" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Approximate results: Although close, they might not match exactly with the traditional PCA results.</li>
<li>Hyperparameter tuning: The dimension of the random projection space must be chosen carefully to balance accuracy and computational cost.</li>
</ul>
<hr />
</div>
<div id="conclusion-2" class="section level4 hasAnchor" number="5.4.2.7">
<h4><span class="header-section-number">5.4.2.7</span> Conclusion<a href="principal-component-analysis.html#conclusion-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The use of random matrices in PCA allows for:
- Faster computations,
- Lower memory usage,
- Scalable algorithms for large datasets.</p>
<p>This technique is particularly powerful in <strong>high-dimensional statistics</strong>, <strong>machine learning</strong>, and <strong>big data</strong> applications.</p>
<p>Yes! Using <span class="math inline">\(\mathbf{Y} = \mathbf{\Omega} \mathbf{X}\)</span> is also a common approach in <strong>randomized PCA</strong> and can offer computational advantages depending on the dimensions of the data matrix.</p>
<hr />
<div id="when-to-use-mathbfy-mathbfomega-mathbfx" class="section level5 hasAnchor" number="5.4.2.7.1">
<h5><span class="header-section-number">5.4.2.7.1</span> When to use <span class="math inline">\(\mathbf{Y} = \mathbf{\Omega} \mathbf{X}\)</span><a href="principal-component-analysis.html#when-to-use-mathbfy-mathbfomega-mathbfx" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Let <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times p}\)</span> where:
- <span class="math inline">\(n\)</span> is the number of samples,
- <span class="math inline">\(p\)</span> is the number of features.</p>
<p>If <span class="math inline">\(n \gg p\)</span> (many samples, few features), using:
<span class="math display">\[
\mathbf{Y} = \mathbf{X} \mathbf{\Omega}
\]</span>
is typically preferred because it reduces the number of features.</p>
<p>However, if <span class="math inline">\(p \gg n\)</span> (many features, few samples), using:
<span class="math display">\[
\mathbf{Y} = \mathbf{\Omega} \mathbf{X}
\]</span>
is advantageous because:
- It reduces the number of samples,
- The random matrix <span class="math inline">\(\mathbf{\Omega}\)</span> is applied to the rows instead of the columns,
- This reduces the size of the matrix on which the singular value decomposition (SVD) is performed.</p>
<hr />
</div>
<div id="advantages-of-mathbfy-mathbfomega-mathbfx" class="section level5 hasAnchor" number="5.4.2.7.2">
<h5><span class="header-section-number">5.4.2.7.2</span> Advantages of <span class="math inline">\(\mathbf{Y} = \mathbf{\Omega} \mathbf{X}\)</span><a href="principal-component-analysis.html#advantages-of-mathbfy-mathbfomega-mathbfx" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li><strong>More efficient when <span class="math inline">\(p \gg n\)</span></strong> because the random projection reduces the dimension from <span class="math inline">\(p\)</span> features to a smaller dimension.</li>
<li><strong>Computational savings</strong>: Performing SVD on <span class="math inline">\(\mathbf{Y}\)</span> (which is smaller than <span class="math inline">\(\mathbf{X}\)</span>) is much faster.</li>
<li><strong>Memory efficiency</strong>: Storing <span class="math inline">\(\mathbf{Y}\)</span> requires less memory than <span class="math inline">\(\mathbf{X}\)</span>.</li>
</ul>
<hr />
</div>
<div id="how-it-works" class="section level5 hasAnchor" number="5.4.2.7.3">
<h5><span class="header-section-number">5.4.2.7.3</span> How it works<a href="principal-component-analysis.html#how-it-works" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ol style="list-style-type: decimal">
<li><strong>Random matrix multiplication</strong>:
<ul>
<li><span class="math inline">\(\mathbf{\Omega} \in \mathbb{R}^{r \times n}\)</span> is a random Gaussian matrix (or any well-chosen random matrix) with <span class="math inline">\(r &lt; n\)</span>.</li>
<li>Multiplying <span class="math inline">\(\mathbf{\Omega}\)</span> with <span class="math inline">\(\mathbf{X}\)</span> results in <span class="math inline">\(\mathbf{Y} \in \mathbb{R}^{r \times p}\)</span>.</li>
</ul></li>
<li><strong>Reduced dimensionality</strong>:
<ul>
<li>Now, <span class="math inline">\(\mathbf{Y}\)</span> has fewer rows, making it more efficient to compute the SVD.</li>
</ul></li>
<li><strong>Approximate PCA</strong>:
<ul>
<li>Perform SVD on <span class="math inline">\(\mathbf{Y}\)</span> instead of <span class="math inline">\(\mathbf{X}\)</span>, obtaining:
<span class="math display">\[
\mathbf{Y} = \mathbf{U}_Y \mathbf{\Sigma}_Y \mathbf{V}_Y&#39;
\]</span></li>
<li>The principal components of <span class="math inline">\(\mathbf{X}\)</span> are then approximated from <span class="math inline">\(\mathbf{U}_Y\)</span>, <span class="math inline">\(\mathbf{\Sigma}_Y\)</span>, and <span class="math inline">\(\mathbf{V}_Y\)</span>.</li>
</ul></li>
</ol>
<hr />
</div>
<div id="choosing-mathbfy-mathbfomega-mathbfx-vs-mathbfy-mathbfx-mathbfomega" class="section level5 hasAnchor" number="5.4.2.7.4">
<h5><span class="header-section-number">5.4.2.7.4</span> Choosing <span class="math inline">\(\mathbf{Y} = \mathbf{\Omega} \mathbf{X}\)</span> vs <span class="math inline">\(\mathbf{Y} = \mathbf{X} \mathbf{\Omega}\)</span><a href="principal-component-analysis.html#choosing-mathbfy-mathbfomega-mathbfx-vs-mathbfy-mathbfx-mathbfomega" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>Use <span class="math inline">\(\mathbf{X} \mathbf{\Omega}\)</span> if you want to reduce the <strong>feature dimension</strong> <span class="math inline">\(p\)</span>.</li>
<li>Use <span class="math inline">\(\mathbf{\Omega} \mathbf{X}\)</span> if you want to reduce the <strong>sample dimension</strong> <span class="math inline">\(n\)</span>.</li>
</ul>
<hr />
</div>
<div id="applications-5" class="section level5 hasAnchor" number="5.4.2.7.5">
<h5><span class="header-section-number">5.4.2.7.5</span> Applications<a href="principal-component-analysis.html#applications-5" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li><strong>High-dimensional biology data</strong> (e.g., genomics with <span class="math inline">\(p \gg n\)</span>),</li>
<li><strong>Text data</strong> where the feature space (words) is huge compared to the number of documents,</li>
<li><strong>Large image datasets</strong> where each image is represented as a high-dimensional vector.</li>
</ul>
</div>
</div>
</div>
</div>
<div id="principal-components-regression" class="section level2 hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> Principal Components Regression<a href="principal-component-analysis.html#principal-components-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Principal components regression (PCR) is a regression technique that leverages <strong>principal component analysis (PCA)</strong> to handle high-dimensional data, especially when predictors are highly correlated.</p>
<div id="what-is-principal-components-regression" class="section level3 hasAnchor" number="5.5.1">
<h3><span class="header-section-number">5.5.1</span> What is Principal Components Regression?<a href="principal-component-analysis.html#what-is-principal-components-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>PCR combines PCA and linear regression:</p>
<ol style="list-style-type: decimal">
<li><strong>PCA Step</strong>: Reduce the dimensionality of the predictor matrix <span class="math inline">\(\mathbf{X}\)</span> by transforming it into its principal components.</li>
<li><strong>Regression Step</strong>: Perform linear regression using the selected principal components as predictors instead of the original variables.</li>
</ol>
<hr />
</div>
<div id="why-use-pcr" class="section level3 hasAnchor" number="5.5.2">
<h3><span class="header-section-number">5.5.2</span> Why Use PCR?<a href="principal-component-analysis.html#why-use-pcr" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><strong>Multicollinearity</strong>: PCR is particularly useful when predictors are highly correlated, as PCA removes collinearity.</li>
<li><strong>Dimensionality Reduction</strong>: It reduces the number of predictors by selecting only the most important principal components.</li>
<li><strong>Stability</strong>: Improves the stability of the regression coefficients by avoiding overfitting.</li>
</ul>
<hr />
</div>
<div id="mathematical-formulation" class="section level3 hasAnchor" number="5.5.3">
<h3><span class="header-section-number">5.5.3</span> Mathematical Formulation<a href="principal-component-analysis.html#mathematical-formulation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Given:
- <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times p}\)</span> (design matrix),
- <span class="math inline">\(\mathbf{y} \in \mathbb{R}^n\)</span> (response vector),</p>
<div id="step-1-perform-pca-on-mathbfx" class="section level4 hasAnchor" number="5.5.3.1">
<h4><span class="header-section-number">5.5.3.1</span> Step 1: Perform PCA on <span class="math inline">\(\mathbf{X}\)</span><a href="principal-component-analysis.html#step-1-perform-pca-on-mathbfx" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The PCA decomposition is:
<span class="math display">\[
\mathbf{X} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}&#39;
\]</span>
- <span class="math inline">\(\mathbf{V} \in \mathbb{R}^{p \times p}\)</span> contains the principal directions.
- The principal components are given by <span class="math inline">\(\mathbf{Z} = \mathbf{X} \mathbf{V}\)</span>.</p>
</div>
<div id="step-2-select-k-principal-components" class="section level4 hasAnchor" number="5.5.3.2">
<h4><span class="header-section-number">5.5.3.2</span> Step 2: Select <span class="math inline">\(k\)</span> Principal Components<a href="principal-component-analysis.html#step-2-select-k-principal-components" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Choose the top <span class="math inline">\(k\)</span> principal components <span class="math inline">\(\mathbf{Z}_k\)</span> that explain most of the variance in <span class="math inline">\(\mathbf{X}\)</span>.</p>
</div>
<div id="step-3-perform-linear-regression" class="section level4 hasAnchor" number="5.5.3.3">
<h4><span class="header-section-number">5.5.3.3</span> Step 3: Perform Linear Regression<a href="principal-component-analysis.html#step-3-perform-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Fit a linear regression model:
<span class="math display">\[
\mathbf{y} = \mathbf{Z}_k \mathbf{\beta}_k + \mathbf{e},
\]</span>
where <span class="math inline">\(\mathbf{\beta}_k\)</span> are the regression coefficients and <span class="math inline">\(\mathbf{e}\)</span> is the error term.</p>
</div>
<div id="step-4-transform-coefficients-back" class="section level4 hasAnchor" number="5.5.3.4">
<h4><span class="header-section-number">5.5.3.4</span> Step 4: Transform Coefficients Back<a href="principal-component-analysis.html#step-4-transform-coefficients-back" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The original regression coefficients <span class="math inline">\(\mathbf{\hat{\beta}}\)</span> are obtained by:
<span class="math display">\[
\mathbf{\hat{\beta}} = \mathbf{V}_k \mathbf{\hat{\beta}}_k.
\]</span></p>
<hr />
</div>
</div>
<div id="choosing-the-number-of-components" class="section level3 hasAnchor" number="5.5.4">
<h3><span class="header-section-number">5.5.4</span> Choosing the Number of Components<a href="principal-component-analysis.html#choosing-the-number-of-components" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The number of principal components <span class="math inline">\(k\)</span> is often chosen using:
- <strong>Cross-validation</strong>: Minimizing the prediction error,
- <strong>Explained variance</strong>: Selecting enough components to capture a large proportion of the variance.</p>
<hr />
</div>
<div id="advantages-of-pcr" class="section level3 hasAnchor" number="5.5.5">
<h3><span class="header-section-number">5.5.5</span> Advantages of PCR<a href="principal-component-analysis.html#advantages-of-pcr" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Handles <strong>multicollinearity</strong> well.</li>
<li>Reduces <strong>overfitting</strong> by eliminating noise and less important predictors.</li>
<li>Works well in <strong>high-dimensional settings</strong> where <span class="math inline">\(p &gt; n\)</span>.</li>
</ul>
</div>
<div id="disadvantages" class="section level3 hasAnchor" number="5.5.6">
<h3><span class="header-section-number">5.5.6</span> Disadvantages<a href="principal-component-analysis.html#disadvantages" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>PCA is <strong>unsupervised</strong>: It does not consider the response variable when selecting components, potentially excluding important predictors.</li>
<li>Interpretation of principal components can be challenging compared to original features.</li>
</ul>
<hr />
</div>
<div id="randomized-pca-and-pcr" class="section level3 hasAnchor" number="5.5.7">
<h3><span class="header-section-number">5.5.7</span> Randomized PCA and PCR<a href="principal-component-analysis.html#randomized-pca-and-pcr" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Randomized PCA offers several advantages when used in the context of <strong>Principal Components Regression (PCR)</strong>, particularly for <strong>high-dimensional data</strong> and <strong>large datasets</strong>.</p>
<div id="advantages-of-randomized-pca-in-pcr" class="section level4 hasAnchor" number="5.5.7.1">
<h4><span class="header-section-number">5.5.7.1</span> Advantages of Randomized PCA in PCR<a href="principal-component-analysis.html#advantages-of-randomized-pca-in-pcr" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p><strong>Computational Efficiency</strong>:
In PCR, PCA is performed on the design matrix <span class="math inline">\(\mathbf{X}\)</span> to obtain principal components. For large datasets, especially when <span class="math inline">\(p \gg n\)</span>, the cost of computing the full SVD is high.</p>
<ul>
<li><strong>Randomized PCA</strong> approximates the principal components by projecting <span class="math inline">\(\mathbf{X}\)</span> onto a lower-dimensional subspace using a random matrix, significantly reducing computation time without a major loss in accuracy.</li>
</ul></li>
<li><p><strong>Scalability</strong>:
Randomized PCA scales well with extremely large datasets, making it feasible to perform PCR on modern, high-dimensional data without running into memory or time constraints.</p></li>
<li><p><strong>Memory Efficiency</strong>:
Traditional PCA requires storing and manipulating large matrices. Randomized PCA reduces the effective dimension early, requiring less memory, which is especially beneficial for PCR when <span class="math inline">\(p\)</span> is very large.</p></li>
<li><p><strong>Handling High-Dimensional Data</strong>:
PCR is often used when there are many predictors (features). Randomized PCA efficiently reduces the dimension while preserving the structure, making it an ideal preprocessing step before regression.</p></li>
<li><p><strong>Fast Approximate Solutions</strong>:
In many practical applications of PCR, an <strong>approximate solution</strong> is often sufficient. Randomized PCA provides a close approximation to the leading principal components with much less computational effort, which is particularly useful when iterative model fitting is required.</p></li>
<li><p><strong>Random Projections Preserve Distances</strong>:
Results from <strong>random matrix theory</strong> (like the Johnson-Lindenstrauss lemma) ensure that random projections approximately preserve the geometric structure of the data. This means that the key relationships between variables are retained even after dimensionality reduction, ensuring that the regression model does not lose critical information.</p></li>
</ul>
</div>
<div id="why-is-this-important-in-pcr" class="section level4 hasAnchor" number="5.5.7.2">
<h4><span class="header-section-number">5.5.7.2</span> Why is this Important in PCR?<a href="principal-component-analysis.html#why-is-this-important-in-pcr" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>PCR involves <strong>two expensive operations</strong>: PCA and then regression on the principal components.</li>
<li><strong>Randomized PCA accelerates the first step</strong>, allowing rapid extraction of the most important components.</li>
<li>The resulting principal components are then used in the regression step, maintaining good predictive performance while reducing computational cost.</li>
</ul>
</div>
<div id="example-use-case" class="section level4 hasAnchor" number="5.5.7.3">
<h4><span class="header-section-number">5.5.7.3</span> Example Use Case<a href="principal-component-analysis.html#example-use-case" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>If you’re working with a dataset where:
- <span class="math inline">\(p = 100,000\)</span> features,
- <span class="math inline">\(n = 1,000\)</span> samples,
performing standard PCA can be unfeasible. Randomized PCA quickly approximates the top <span class="math inline">\(k\)</span> components, enabling the PCR model to be trained efficiently without losing significant predictive power.</p>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="factor-analysis.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
