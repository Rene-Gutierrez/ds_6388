[["index.html", "DS 6388 Spring 2025 1 DS 6388: Multivariate Statistical Methods for High-dimensional Data 1.1 Calendar", " DS 6388 Spring 2025 Rene Gutierrez University of Texas at El Paso 1 DS 6388: Multivariate Statistical Methods for High-dimensional Data This is the website for DS 6388. It contains relevant information for the course and lecture notes. 1.1 Calendar 1.1.1 Importatnt Dates February 10: Linear Algebra Extra Credit Test 1.1.2 Class Schedule Class 1: Introduction to Multivariate High-Dimensional Methods Class 2: HD Linear Regression: Machine Learning Class 3: HD Linear Regression: Bayesian Methods Class 4: Computational Implementation of HD Linear Regression "],["prerequisites.html", "2 Prerequisites 2.1 Linear Algebra 2.2 Calculus 2.3 Probability 2.4 Statistics", " 2 Prerequisites Before diving into the course, it’s important to have a solid understanding of the following foundational concepts. These are categorized into four key topics: Linear Algebra Probability Statistics Calculus 2.1 Linear Algebra You should be familiar with the following linear algebra concepts: Linear Independence Column Space of a Matrix Rank of a Matrix Full Rank Matrix Inverse Matrix Positive Definite Matrix Singular Value Decomposition Eigendecomposition Idempotent Matrix Determinant of a Matrix 2.1.1 Linear Independence Linear independence is a fundamental concept in linear algebra that describes a set of vectors where no vector can be written as a linear combination of the others. In other words, the vectors are not “redundant,” meaning none of the vectors depends on any other in the set. 2.1.1.1 Definition: A set of vectors \\(\\{ \\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n \\}\\) in a vector space is linearly independent if the only solution to the equation: \\[ c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_n \\mathbf{v}_n = \\mathbf{0} \\] is when all the scalar coefficients \\(c_1, c_2, \\ldots, c_n\\) are zero, i.e., \\(c_1 = c_2 = \\cdots = c_n = 0\\). If any of the coefficients can be non-zero while still satisfying this equation, then the vectors are linearly dependent. 2.1.1.2 Example: Consider two vectors in \\(\\mathbb{R}^2\\): \\[ \\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\] These vectors are linearly independent because there is no way to express one as a multiple of the other. The only solution to: \\[ c_1 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} + c_2 \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\] is \\(c_1 = 0\\) and \\(c_2 = 0\\). In contrast, if: \\[ \\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} \\] These vectors are linearly dependent, because \\(\\mathbf{v}_2 = 2 \\mathbf{v}_1\\). Therefore, you can express \\(mathbf{v}_2\\) as a linear combination of \\(\\mathbf{v}_1\\). 2.1.1.3 Key Points: Linearly independent vectors carry distinct information and cannot be derived from each other. Linearly dependent vectors are redundant because one or more can be expressed as a combination of others. In a set of linearly independent vectors, removing any vector would reduce the span of the vector space they cover. 2.1.1.4 Importance: Linear independence is crucial in determining the rank of a matrix. In systems of equations, linear independence of the rows or columns determines if the system has a unique solution. In vector spaces, the dimension of the space is the maximum number of linearly independent vectors. 2.1.2 Column Space of a Matrix The column space of a matrix is the set of all possible linear combinations of its columns. If you have a matrix \\(\\mathbf{A}\\) with \\(n\\) rows and \\(p\\) columns, the column space of \\(\\mathbf{A}\\), denoted as Col(\\(\\mathbf{A}\\)), consists of all vectors in \\(\\mathbb{R}^n\\) that can be expressed as a linear combination of the columns of \\(\\mathbf{A}\\). 2.1.2.1 Definition: Given a matrix \\(\\mathbf{A}\\) with columns \\(\\mathbf{a}_1, \\mathbf{a}_2, \\dots, \\mathbf{a}_p\\), the column space of \\(\\mathbf{A}\\) is defined as: \\[ \\text{Col}(\\mathbf{A}) = \\left\\{ \\mathbf{y} \\in \\mathbb{R}^n \\mid \\mathbf{y} = \\mathbf{A} \\mathbf{c} \\text{ for some } \\mathbf{c} \\in \\mathbb{R}^p \\right\\} \\] This means the column space is the span of the columns of \\(\\mathbf{A}\\), or equivalently, all vectors that can be written as \\(\\mathbf{y} = c_1 \\mathbf{a}_1 + c_2 \\mathbf{a}_2 + \\dots + c_p \\mathbf{a}_p\\), where \\(c_1, c_2, \\dots, c_p\\) are scalars. 2.1.2.2 Properties: The column space of \\(\\mathbf{A}\\) is a subspace of \\(\\mathbb{R}^n\\). The dimension of the column space of \\(\\mathbf{A}\\) is called the rank of the matrix and corresponds to the number of linearly independent columns in \\(\\mathbf{A}\\). The column space provides valuable information about the linear independence and span of the columns of a matrix. 2.1.2.3 Geometric Interpretation: In geometric terms, the column space represents the set of all possible vectors that can be “reached” by linearly combining the columns of the matrix. For example: - For a matrix with 2 columns in \\(\\mathbb{R}^3\\), the column space will be a plane in \\(\\mathbb{R}^3\\) if the columns are linearly independent. - For a matrix with 3 columns in \\(\\mathbb{R}^2\\), the column space will span all of \\(\\mathbb{R}^2\\) (if the columns are linearly independent) or a line (if they are dependent). 2.1.3 Rank of a Matrix The rank of a matrix is the dimension of its column space, which is the number of linearly independent columns in the matrix. Alternatively, it is also the dimension of the row space, which is the number of linearly independent rows. 2.1.3.1 Definition: For a matrix \\(\\mathbf{A}\\), the rank is defined as: \\[ \\text{rank}(\\mathbf{A}) = \\dim(\\text{Col}(\\mathbf{A})) = \\dim(\\text{Row}(\\mathbf{A})) \\] This is the maximum number of linearly independent rows or columns in the matrix. In other words, it tells you how many of the matrix’s columns (or rows) are not redundant and cannot be written as a linear combination of the others. 2.1.3.2 Key Points: The rank of a matrix \\(\\mathbf{A}\\) is denoted as rank(\\(\\mathbf{A}\\)). It measures the number of independent directions in the column space or row space. Full rank: A matrix is said to have full rank if its rank is equal to the smaller of the number of rows or columns. For an \\(m \\times n\\) matrix: If \\(\\text{rank}(\\mathbf{A}) = m\\) (number of rows), it has full row rank. If \\(\\text{rank}(\\mathbf{A}) = n\\) (number of columns), it has full column rank. Rank-deficient: If the rank of the matrix is less than the smaller of the number of rows or columns, the matrix is called rank-deficient, meaning that some of its rows or columns are linearly dependent. \\(\\text{rank}( {\\boldsymbol A} ) = \\text{rank}( {\\boldsymbol A} &#39;) = \\text{rank}( {\\boldsymbol A} &#39; {\\boldsymbol A} ) = \\text{rank}( {\\boldsymbol A} {\\boldsymbol A} &#39;)\\) 2.1.3.3 Example: Consider the matrix: \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{bmatrix} \\] The rank of \\(\\mathbf{A}\\) is 2 because two of the rows (or columns) are linearly independent, but the third row (or column) is a linear combination of the others. 2.1.3.4 Properties: The rank of a matrix is always less than or equal to the minimum of the number of rows and columns: \\[ \\text{rank}(\\mathbf{A}) \\leq \\min(m, n) \\] The rank of a matrix is equal to the number of non-zero singular values in its singular value decomposition (SVD). In square matrices, the rank gives insight into whether the matrix is invertible. A square matrix is invertible if and only if it has full rank. 2.1.4 Full Rank Matrix A full rank matrix is a matrix in which the rank is equal to the largest possible value for that matrix, meaning: For an \\(m \\times n\\) matrix \\(A\\), the rank is the maximum number of linearly independent rows or columns. If the rank is equal to \\(m\\) (the number of rows), the matrix has full row rank. If the rank is equal to \\(n\\) (the number of columns), the matrix has full column rank. 2.1.4.1 For a square matrix (\\(m = n\\)): A square matrix is full rank if its rank is equal to its dimension, i.e., if the matrix is invertible. In this case, \\(\\text{rank}( {\\boldsymbol A} ) = n\\), meaning all rows and columns are linearly independent, and the matrix has an inverse. 2.1.4.2 For a rectangular matrix (\\(m \\neq n\\)): A matrix is full rank if the rank equals the smaller of the number of rows or columns. For an \\(m \\times n\\) matrix, the rank is at most \\(\\min(m, n)\\). If the matrix has full row rank, all rows are linearly independent. If the matrix has full column rank, all columns are linearly independent. 2.1.4.3 Example: Consider the matrix: \\[ {\\boldsymbol A} = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix} \\] This is a \\(2 \\times 3\\) matrix. Since its two rows are linearly independent, it has full row rank, with rank = 2 (the number of rows). However, it does not have full column rank because it has only two independent rows for three columns. 2.1.4.4 Key Properties: A full rank matrix has no redundant rows or columns (no row or column can be written as a linear combination of others). A square matrix with full rank is invertible (non-singular). For a rectangular matrix, full rank implies the matrix has maximal independent information in terms of its rows or columns. 2.1.4.5 Importance: Full rank matrices are crucial in solving systems of linear equations. A system \\( {\\boldsymbol A} \\mathbf{x} = \\mathbf{b}\\) has a unique solution if \\( {\\boldsymbol A} \\) is a square, full rank matrix. In linear algebra and machine learning, the rank provides insight into the dimensionality and the independence of the data or transformation matrix. 2.1.5 Inverse Matrix An inverse matrix of a square matrix \\( {\\boldsymbol A} \\), denoted as \\( {\\boldsymbol A} ^{-1}\\), is a matrix that, when multiplied by \\( {\\boldsymbol A} \\), results in the identity matrix \\(I\\). This relationship is expressed as: \\[ {\\boldsymbol A} {\\boldsymbol A} ^{-1} = {\\boldsymbol A} ^{-1} {\\boldsymbol A} = {\\boldsymbol I} \\] where \\( {\\boldsymbol I} \\) is the identity matrix, and its diagonal elements are 1, with all off-diagonal elements being 0. 2.1.5.1 Conditions for a Matrix to Have an Inverse: The matrix \\( {\\boldsymbol A} \\) must be square, meaning it has the same number of rows and columns. The matrix \\( {\\boldsymbol A} \\) must be non-singular, meaning its determinant is non-zero (\\(| {\\boldsymbol A} | \\neq 0\\)). 2.1.5.2 Properties of the Inverse Matrix: Uniqueness: If a matrix has an inverse, it is unique. Inverse of a Product: The inverse of the product of two matrices \\( {\\boldsymbol A} \\) and \\( {\\boldsymbol B} \\) is given by \\(( {\\boldsymbol A} {\\boldsymbol B} )^{-1} = {\\boldsymbol B} ^{-1} {\\boldsymbol A} ^{-1}\\). Inverse of the Inverse: \\(( {\\boldsymbol A} ^{-1})^{-1} = {\\boldsymbol A} \\). Transpose of the Inverse: \\(( {\\boldsymbol A} ^{-1})&#39; = ( {\\boldsymbol A} &#39;)^{-1}\\). 2.1.5.3 Special Case 2 by 2 Matrix For a \\(2 \\times 2\\) matrix: \\[ {\\boldsymbol A} = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix} \\] The inverse of \\( {\\boldsymbol A} \\) (if \\(| {\\boldsymbol A} |=\\det( {\\boldsymbol A} ) \\neq 0\\)) is: \\[ A^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix} d &amp; -b \\\\ -c &amp; a \\end{bmatrix} \\] where \\(ad - bc\\) is the determinant of the matrix \\( {\\boldsymbol A} \\). 2.1.5.4 Special Case 2 by 2 Block Matrix The inverse of a \\(2 \\times 2\\) block matrix can be expressed under certain conditions. Let’s consider a block matrix \\(\\mathbf{M}\\) of the form: \\[ \\mathbf{M} = \\begin{bmatrix} \\mathbf{A} &amp; \\mathbf{B} \\\\ \\mathbf{C} &amp; \\mathbf{D} \\end{bmatrix} \\] where: - \\(\\mathbf{A}\\) and \\(\\mathbf{D}\\) are themselves square matrices, and \\(\\mathbf{B}\\) and \\(\\mathbf{C}\\) are matrices (not necessarily square). Then the inverse of \\(\\mathbf{M}\\) is given by: \\[ \\mathbf{M}^{-1} = \\begin{bmatrix} \\mathbf{A}^{-1} + \\mathbf{A}^{-1} \\mathbf{B} \\mathbf{S}^{-1} \\mathbf{C} \\mathbf{A}^{-1} &amp; -\\mathbf{A}^{-1} \\mathbf{B} \\mathbf{S}^{-1} \\\\ -\\mathbf{S}^{-1} \\mathbf{C} \\mathbf{A}^{-1} &amp; \\mathbf{S}^{-1} \\end{bmatrix} \\] where \\(\\mathbf{S}\\) is the Schur complement of \\(\\mathbf{A}\\) and is defined as: \\[ \\mathbf{S} = \\mathbf{D} - \\mathbf{C} \\mathbf{A}^{-1} \\mathbf{B} \\] 2.1.5.4.1 Conditions for the Inverse to Exist: \\(\\mathbf{A}\\) must be invertible, The Schur complement \\(\\mathbf{S}\\) must also be invertible. 2.1.5.4.2 Explanation of the Terms: \\(\\mathbf{A}^{-1}\\): The inverse of matrix \\(\\mathbf{A}\\), \\(\\mathbf{S}^{-1}\\): The inverse of the Schur complement \\(\\mathbf{S}\\), which can be interpreted as the “effective” part of matrix \\(\\mathbf{D}\\) once the contribution of \\(\\mathbf{A}\\) has been removed. This formula generalizes the concept of inverting a matrix when it’s partitioned into blocks. 2.1.5.5 Sherman-Morrison Formula The Sherman-Morrison formula provides a way to compute the inverse of a matrix after it has been updated by a rank-one modification. Specifically, it addresses the situation where a matrix A has been updated by the outer product of two vectors u and v. The formula is: \\[ (\\mathbf{A} + \\mathbf{u} \\mathbf{v}^T)^{-1} = \\mathbf{A}^{-1} - \\frac{\\mathbf{A}^{-1} \\mathbf{u} \\mathbf{v}^T \\mathbf{A}^{-1}}{1 + \\mathbf{v}^T \\mathbf{A}^{-1} \\mathbf{u}} \\] 2.1.5.5.1 Requirements: A must be an invertible matrix. The scalar \\(1 + \\mathbf{v}^T \\mathbf{A}^{-1} \\mathbf{u}\\) must not be zero. 2.1.5.5.2 Explanation of the terms: A is an invertible \\(n \\times n\\) matrix. u and v are \\(n \\times 1\\) column vectors. The outer product \\(\\mathbf{u} \\mathbf{v}^T\\) is an \\(n \\times n\\) rank-one matrix. The term \\(1 + \\mathbf{v}^T \\mathbf{A}^{-1} \\mathbf{u}\\) is a scalar. This formula is useful in situations where you need to efficiently update the inverse of a matrix after a low-rank modification, rather than recomputing the inverse from scratch. 2.1.6 Positive Definite Matrix A positive definite matrix is a symmetric matrix \\( {\\boldsymbol A} \\) where, for any non-zero vector \\(\\mathbf{x}\\), the following condition holds: \\[ \\mathbf{x}&#39; {\\boldsymbol A} \\mathbf{x} &gt; 0 \\] 2.1.6.1 Key Properties: Symmetry: The matrix \\( {\\boldsymbol A} \\) must be symmetric, meaning \\( {\\boldsymbol A} = {\\boldsymbol A} &#39;\\). Positive quadratic form: For any non-zero vector \\(\\mathbf{x}\\), the quadratic form \\(\\mathbf{x}&#39; {\\boldsymbol A} \\mathbf{x}\\) must yield a positive value. 2.1.6.2 Characteristics of a Positive Definite Matrix: All the eigenvalues of a positive definite matrix are positive. The determinants of the leading principal minors (submatrices) of the matrix are positive. The diagonal elements of a positive definite matrix are positive. \\( {\\boldsymbol A} \\) is invertible. \\( {\\boldsymbol A} ^{-1}\\) is also positive definite matrix. 2.1.6.3 Example: The matrix: \\[ {\\boldsymbol A} = \\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 2 \\end{bmatrix} \\] is positive definite, because for any non-zero vector \\(\\mathbf{x}\\), \\(\\mathbf{x}&#39; {\\boldsymbol A} \\mathbf{x} &gt; 0\\). For instance, if \\(\\mathbf{x} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\), then: \\[ \\mathbf{x}&#39; {\\boldsymbol A} \\mathbf{x} = \\begin{bmatrix} 1 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = 6 &gt; 0 \\] 2.1.7 Singular Value Decomposition Singular Value Decomposition (SVD) is a fundamental matrix factorization technique used in linear algebra to break down a matrix into three distinct components. It provides valuable insight into the structure of a matrix and is widely used in applications like data compression, signal processing, and dimensionality reduction. 2.1.7.1 Definition: For any real (or complex) matrix \\(\\mathbf{A}\\) of size \\(m \\times n\\), the Singular Value Decomposition is given by: \\[ \\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}&#39; \\] where: - \\(\\mathbf{U}\\) is an \\(m \\times m\\) orthogonal matrix, whose columns are called the left singular vectors. - \\(\\mathbf{\\Sigma}\\) is an \\(m \\times n\\) diagonal matrix, where the diagonal entries are the singular values of \\(\\mathbf{A}\\). The singular values are always non-negative and arranged in decreasing order. - \\(\\mathbf{V}\\) is an \\(n \\times n\\) orthogonal matrix, whose columns are called the right singular vectors. 2.1.7.2 Interpretation of the Components: \\(\\mathbf{U}\\) represents the orthonormal basis for the column space of \\(\\mathbf{A}\\). \\(\\mathbf{V}\\) represents the orthonormal basis for the row space of \\(\\mathbf{A}\\). \\(\\mathbf{\\Sigma}\\) contains the singular values, which provide information about the importance or magnitude of the corresponding singular vectors. Large singular values indicate directions with significant data spread, while small or zero singular values correspond to directions with little or no data variation. 2.1.7.3 Geometric Interpretation: SVD can be viewed geometrically as a transformation where: 1. \\(\\mathbf{V}\\) applies a rotation or reflection in the input space. 2. \\(\\mathbf{\\Sigma}\\) stretches or compresses the data along certain axes. 3. \\(\\mathbf{U}\\) applies a final rotation or reflection in the output space. 2.1.7.4 Key Points: Rank: The number of non-zero singular values in \\(\\mathbf{\\Sigma}\\) equals the rank of the matrix \\(\\mathbf{A}\\). Dimensionality Reduction: By truncating small singular values in \\(\\mathbf{\\Sigma}\\), we can approximate \\(\\mathbf{A}\\) with a lower-rank matrix, which is useful in compressing data while retaining most of its structure. Condition Number: The ratio of the largest to the smallest non-zero singular value gives the condition number of the matrix, which indicates how sensitive a matrix is to numerical errors or perturbations. 2.1.7.5 Example: For a matrix \\(\\mathbf{A}\\) of size \\(3 \\times 2\\), the SVD would look like: \\[ \\mathbf{A} = \\mathbf{U} \\begin{bmatrix} \\sigma_1 &amp; 0 \\\\ 0 &amp; \\sigma_2 \\\\ 0 &amp; 0 \\end{bmatrix} \\mathbf{V}&#39; \\] where \\(\\sigma_1\\) and \\(\\sigma_2\\) are the singular values, and \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are orthogonal matrices. 2.1.7.6 Applications of SVD: Dimensionality Reduction: SVD is widely used in Principal Component Analysis (PCA) for reducing the dimensionality of large datasets. Low-Rank Approximations: In data compression, SVD helps to approximate matrices with fewer dimensions while maintaining the core structure. Solving Linear Systems: In cases where a matrix is close to singular, SVD can be used to solve linear systems more stably. Latent Semantic Analysis (LSA): In natural language processing, SVD is used to reduce the dimensionality of word-document matrices to capture latent relationships between words and documents. 2.1.8 Eigendecomposition Eigendecomposition is a matrix factorization technique used in linear algebra, where a square matrix is decomposed into its eigenvalues and eigenvectors. It is applicable to square matrices and provides deep insight into the matrix’s structure, particularly in understanding transformations, systems of linear equations, and differential equations. 2.1.8.1 Definition: Given a square matrix \\(\\mathbf{A}\\) of size \\(n \\times n\\), eigendecomposition is a factorization of the matrix into the following form: \\[ \\mathbf{A} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^{-1} \\] where: - \\(\\mathbf{V}\\) is the matrix of eigenvectors of \\(\\mathbf{A}\\), and each column of \\(\\mathbf{V}\\) is an eigenvector. - \\(\\mathbf{\\Lambda}\\) is a diagonal matrix of eigenvalues of \\(\\mathbf{A}\\), with each diagonal element corresponding to an eigenvalue of \\(\\mathbf{A}\\). - \\(\\mathbf{V}^{-1}\\) is the inverse of the matrix of eigenvectors. 2.1.8.2 Eigenvalues and Eigenvectors: Eigenvalue (\\(\\lambda\\)): A scalar \\(\\lambda\\) is an eigenvalue of \\(\\mathbf{A}\\) if there exists a non-zero vector \\(\\mathbf{v}\\) such that: \\[ \\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v} \\] In this case, \\(\\mathbf{v}\\) is called the eigenvector corresponding to the eigenvalue \\(\\lambda\\). Eigenvector: A non-zero vector \\(\\mathbf{v}\\) that remains parallel to itself (i.e., only scaled) when multiplied by \\(\\mathbf{A}\\) is called an eigenvector. 2.1.8.3 Conditions for Eigendecomposition: A matrix \\(\\mathbf{A}\\) is diagonalizable (i.e., it can be factored into \\(\\mathbf{A} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^{-1}\\)) if and only if it has \\(n\\) linearly independent eigenvectors. Not all matrices are diagonalizable. However, if \\(\\mathbf{A}\\) has \\(n\\) distinct eigenvalues, then it is guaranteed to be diagonalizable. Symmetric matrices are always diagonalizable. 2.1.8.4 Geometric Interpretation: Eigendecomposition reveals the directions (eigenvectors) along which the matrix transformation \\(\\mathbf{A}\\) acts as a simple scaling by the eigenvalues. Geometrically: - Eigenvectors point in directions that remain invariant under the transformation by \\(\\mathbf{A}\\). - The corresponding eigenvalues tell us how much the matrix stretches or compresses vectors in the direction of those eigenvectors. 2.1.8.5 Example: For a matrix \\(\\mathbf{A}\\): \\[ \\mathbf{A} = \\begin{bmatrix} 4 &amp; 1 \\\\ 2 &amp; 3 \\end{bmatrix} \\] The eigenvalues \\(\\lambda_1 = 5\\) and \\(\\lambda_2 = 2\\) can be found by solving the characteristic equation \\(\\det(\\mathbf{A} - \\lambda \\mathbf{I}) = 0\\). Corresponding eigenvectors can then be computed, allowing the matrix to be diagonalized as: \\[ \\mathbf{A} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^{-1} \\] where \\(\\mathbf{\\Lambda} = \\text{diag}(5, 2)\\) and \\(\\mathbf{V}\\) is the matrix of eigenvectors. 2.1.8.6 Applications of Eigendecomposition: Diagonalization: Eigendecomposition allows matrices to be diagonalized, simplifying many computations (such as raising matrices to powers). Principal Component Analysis (PCA): In data science, eigendecomposition is used in PCA to find directions of maximum variance in data. Solving Differential Equations: Eigenvalues and eigenvectors are useful in solving systems of linear differential equations. Quantum Mechanics: In physics, eigenvalues and eigenvectors describe the measurable properties (like energy levels) of systems. In summary, eigendecomposition is a powerful tool in linear algebra that provides insight into how a matrix transforms space, offering valuable properties through its eigenvalues and eigenvectors. 2.1.9 Idempotent Matrix An idempotent matrix is a matrix that, when multiplied by itself, yields the same matrix. In other words, a matrix \\(\\mathbf{M}\\) is idempotent if it satisfies the condition: \\[ \\mathbf{M}^2 = \\mathbf{M} \\] 2.1.9.1 Key Properties of Idempotent Matrices: Eigenvalues: The eigenvalues of an idempotent matrix are either 0 or 1. This is because for an eigenvector \\(\\mathbf{v}\\) with eigenvalue \\(\\lambda\\), the equation \\(\\mathbf{M}^2 \\mathbf{v} = \\mathbf{M} \\mathbf{v}\\) simplifies to \\(\\lambda^2 \\mathbf{v} = \\lambda \\mathbf{v}\\), meaning \\(\\lambda(\\lambda - 1) = 0\\), so \\(\\lambda = 0\\) or \\(\\lambda = 1\\). Rank: The rank of an idempotent matrix \\(\\mathbf{M}\\) is equal to the trace of the matrix (the sum of the diagonal elements), which is also the number of eigenvalues equal to 1. Projection Interpretation: Idempotent matrices often represent projection matrices in linear algebra. A projection matrix projects vectors onto a subspace, and applying the projection multiple times doesn’t change the result beyond the first application, which is why it satisfies \\(\\mathbf{M}^2 = \\mathbf{M}\\). 2.1.9.2 Examples: Identity Matrix: The identity matrix \\(\\mathbf{I}\\) is idempotent because: \\[ \\mathbf{I}^2 = \\mathbf{I} \\] Zero Matrix: The zero matrix \\(\\mathbf{0}\\) is also idempotent because: \\[ \\mathbf{0}^2 = \\mathbf{0} \\] Projection Matrix: Consider a projection matrix onto the x-axis in 2D: \\[ \\mathbf{P} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix} \\] This matrix is idempotent since: \\[ \\mathbf{P}^2 = \\mathbf{P} \\] 2.1.9.3 Use in Statistics: Idempotent matrices are commonly used in statistics, particularly in the context of regression analysis. For example, the hat matrix \\(\\mathbf{H}\\) in linear regression, which transforms the observed values into the predicted values, is idempotent: \\[ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\] where \\(\\mathbf{X}\\) is the design matrix. In summary, idempotent matrices have unique properties and are frequently encountered in linear algebra, projections, and statistical applications. 2.1.10 Determinant of a Matrix The determinant of a matrix is a scalar value that provides important information about the properties of a square matrix. It is a fundamental concept in linear algebra, often used to determine whether a matrix is invertible, as well as to describe geometric transformations and volume scaling. 2.1.10.1 Key Charachteristics of the Determinant: Square Matrices Only: The determinant is only defined for square matrices (i.e., matrices with the same number of rows and columns). Invertibility: A matrix is invertible (i.e., it has an inverse) if and only if its determinant is non-zero. If the determinant is zero, the matrix is singular and does not have an inverse. Geometric Interpretation: The determinant represents the scaling factor of the linear transformation described by the matrix. For example, in two or three dimensions, the determinant tells you how much the matrix scales area or volume: A determinant of 1 means the matrix preserves the area (in 2D) or volume (in 3D). A determinant greater than 1 means the matrix scales the area or volume by that factor. A negative determinant indicates that the transformation also includes a reflection. Significance of Zero Determinant: If the determinant is zero, it means that the matrix maps some vectors to a lower-dimensional space. For instance, in 2D, it might map points onto a line, collapsing the area to zero. 2.1.10.2 Definition: For a 2x2 matrix: \\[ \\mathbf{A} = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix} \\] The determinant of \\(\\mathbf{A}\\) is: \\[ \\det(\\mathbf{A}) = ad - bc \\] This formula gives the area scaling factor for the linear transformation represented by the matrix \\(\\mathbf{A}\\). For a 3x3 matrix: \\[ \\mathbf{B} = \\begin{bmatrix} a &amp; b &amp; c \\\\ d &amp; e &amp; f \\\\ g &amp; h &amp; i \\end{bmatrix} \\] The determinant of \\(\\mathbf{B}\\) is: \\[ \\det(\\mathbf{B}) = a(ei - fh) - b(di - fg) + c(dh - eg) \\] This formula can be extended to higher dimensions using recursive expansion (called cofactor expansion or Laplace expansion). 2.1.10.3 Applications of the Determinant: Solving Linear Systems: The determinant is used in Cramer’s Rule, a method for solving systems of linear equations. If the determinant of the coefficient matrix is non-zero, the system has a unique solution. Eigenvalues and Eigenvectors: The determinant plays a key role in computing the eigenvalues of a matrix. The determinant of a matrix \\(\\mathbf{A} - \\lambda \\mathbf{I}\\), where \\(\\lambda\\) is a scalar and \\(\\mathbf{I}\\) is the identity matrix, gives the characteristic equation whose solutions are the eigenvalues. Volume and Area Calculations: In geometry, the determinant helps calculate the area (in 2D) or volume (in 3D) of a region after applying a linear transformation. Singular Value Decomposition (SVD) and Principal Component Analysis (PCA): The determinant is important in these techniques for understanding data structures, transformations, and dimensionality reduction. 2.1.10.4 Properties of the Determinant For a general block matrix of the form: \\[ \\mathbf{M} = \\begin{bmatrix} \\mathbf{A} &amp; \\mathbf{B} \\\\ \\mathbf{C} &amp; \\mathbf{D} \\end{bmatrix} \\] where \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), \\(\\mathbf{C}\\), and \\(\\mathbf{D}\\) are submatrices, the determinant formula is more complex. If \\(\\mathbf{D}\\) is invertible, we can use the Schur complement to compute the determinant: \\[ \\det(\\mathbf{M}) = \\det(\\mathbf{D}) \\det(\\mathbf{A} - \\mathbf{B} \\mathbf{D}^{-1} \\mathbf{C}) \\] Here, \\(\\mathbf{A} - \\mathbf{B} \\mathbf{D}^{-1} \\mathbf{C}\\) is called the Schur complement of \\(\\mathbf{D}\\) in \\(\\mathbf{M}\\). For any invertible matrix \\( {\\boldsymbol X} \\in {\\mathbb{R}} ^{p \\times p}\\) and matrices \\( {\\boldsymbol A} \\in {\\mathbb{R}} ^{p\\times q}\\) and \\( {\\boldsymbol B} \\in {\\mathbb{R}} ^{q\\times p}\\) we have, that: \\[\\det( {\\boldsymbol X} + {\\boldsymbol A} {\\boldsymbol B} ) = det( {\\boldsymbol X} )\\det( {\\boldsymbol I} _q + {\\boldsymbol B} {\\boldsymbol X} ^{-1} {\\boldsymbol A} ) \\] 2.2 Calculus Key calculus topics include: Gradient Hessian Matrix Calculus Optimization 2.2.1 Gradient The gradient of a function is a vector that contains the partial derivatives of the function with respect to each of its variables. It points in the direction of the steepest ascent of the function, and its magnitude indicates the rate of change in that direction. For a scalar function \\(f(x_1, x_2, \\ldots, x_n)\\), where \\(x_1, x_2, \\ldots, x_n\\) are the variables, the gradient is defined as: \\[ \\nabla f = \\frac{d}{d \\mathbf{x}} f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix} \\] 2.2.1.1 Key Points: Direction: The gradient points in the direction of the greatest increase of the function. Magnitude: The magnitude of the gradient represents how fast the function increases in that direction. Zero Gradient: If \\(\\nabla f = 0\\), it indicates that the function has a critical point, which could be a local minimum, maximum, or saddle point. 2.2.1.2 Example: For a function \\(f(x, y) = x^2 + y^2\\), the gradient is: \\[ \\nabla f = \\begin{bmatrix} \\frac{\\partial}{\\partial x} (x^2 + y^2) \\\\ \\frac{\\partial}{\\partial y} (x^2 + y^2) \\end{bmatrix} = \\begin{bmatrix} 2x \\\\ 2y \\end{bmatrix} \\] This shows that the gradient points outward from the origin, and its magnitude increases as \\(x\\) and \\(y\\) increase. 2.2.1.3 Applications: In optimization, the gradient is used to find the minimum or maximum of a function (e.g., in gradient descent, a common optimization algorithm). In vector calculus, the gradient is used to describe the slope or rate of change of scalar fields (such as temperature, pressure, or altitude in physical applications). 2.2.2 Hessian Matrix The Hessian matrix is a square matrix of second-order partial derivatives of a scalar-valued function. It describes the local curvature of a multivariable function and is used to assess the nature of critical points (i.e., whether they are minima, maxima, or saddle points). For a scalar function \\(f(x_1, x_2, \\ldots, x_n)\\), the Hessian matrix \\( {\\boldsymbol H} \\) is defined as: \\[ {\\boldsymbol H} (f) = \\frac{d}{d \\mathbf{x} d {\\boldsymbol x} &#39;} f(\\mathbf{x}) =\\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_2^2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix} \\] 2.2.2.1 Key Properties: The Hessian is symmetric if the second-order partial derivatives are continuous (by Clairaut’s theorem, also called Schwarz’s theorem). It provides important information about the local behavior of the function, particularly around critical points where the gradient is zero. Eigenvalues of the Hessian matrix determine the type of critical points: If all eigenvalues are positive, the function has a local minimum. If all eigenvalues are negative, the function has a local maximum. If some eigenvalues are positive and others are negative, the function has a saddle point. 2.2.2.2 Example: For a function \\(f(x, y) = x^2 + xy + y^2\\), the Hessian matrix is: \\[ {\\boldsymbol H} (f) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x^2} &amp; \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x} &amp; \\frac{\\partial^2 f}{\\partial y^2} \\end{bmatrix} = \\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 2 \\end{bmatrix} \\] 2.2.3 Applications: In optimization, the Hessian is used to assess the convexity or concavity of a function, which helps in identifying the nature of critical points. In machine learning, it is used to optimize loss functions and can be part of second-order optimization methods like Newton’s method. In economics and engineering, the Hessian helps in analyzing systems involving multiple variables and understanding how they interact with each other. 2.2.4 Matrix Calculus You need to know the following matrix calculus operations: \\[ \\frac{d}{d \\mathbf{x}} \\left( {\\boldsymbol c} &#39; {\\boldsymbol x} \\right) \\] \\[ \\frac{d}{d \\mathbf{x}} \\left( {\\boldsymbol x} &#39; {\\boldsymbol A} {\\boldsymbol x} \\right) \\] \\[ \\frac{d}{d \\mathbf{x} d {\\boldsymbol x} &#39;} \\left( {\\boldsymbol x} &#39; {\\boldsymbol A} {\\boldsymbol x} \\right) \\] Let \\(\\mathbf{c}\\) be a constant vector and \\(\\mathbf{x}\\) be a variable vector, both of size \\(n \\times 1\\). We want to compute the derivative of the product: \\[ f(\\mathbf{x}) = \\mathbf{c}&#39; \\mathbf{x} \\] Where: \\[ \\mathbf{c}&#39; \\mathbf{x} = \\sum_{i=1}^{n} c_i x_i \\] To differentiate \\(f(\\mathbf{x}) = \\mathbf{c}&#39; \\mathbf{x}\\) with respect to the variable vector \\(\\mathbf{x}\\), we take the derivative of each component separately: \\[ \\nabla f = \\frac{d}{d \\mathbf{x}} f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1} (\\mathbf{c}&#39; \\mathbf{x}) \\\\ \\frac{\\partial }{\\partial x_2} (\\mathbf{c}&#39; \\mathbf{x}) \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n} (\\mathbf{c}&#39; \\mathbf{x}) \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1} \\left(\\sum_{i=1}^{n} c_i x_i\\right) \\\\ \\frac{\\partial }{\\partial x_2} \\left(\\sum_{i=1}^{n} c_i x_i\\right) \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n} \\left(\\sum_{i=1}^{n} c_i x_i\\right) \\end{bmatrix} \\] Since \\(\\mathbf{c}\\) is a constant vector, the derivative of each term \\(c_i x_i\\) is simply \\(c_i\\), that is: \\[ \\frac{d}{d x_j} \\left(\\sum_{i=1}^{n} c_i x_i\\right) = c_j \\] Thus, the derivative of the entire sum is the vector: \\[ \\frac{d}{d \\mathbf{x}} \\left( \\mathbf{c}&#39; \\mathbf{x} \\right) = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix} = \\mathbf{c} \\] Now, let’s go through the derivative of the quadratic form \\(f(\\mathbf{x}) = \\mathbf{x}&#39; {\\boldsymbol A} \\mathbf{x}\\), where: \\(\\mathbf{x}\\) is a variable vector of size \\(n \\times 1\\), \\( {\\boldsymbol A} \\) is a constant, symmetric matrix of size \\(n \\times n\\). \\[ f(\\mathbf{x}) = \\mathbf{x}&#39; {\\boldsymbol A} \\mathbf{x} \\] First, expand the quadratic form: \\[ f(\\mathbf{x}) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j \\] Then \\[ \\nabla f = \\frac{d}{d \\mathbf{x}} f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1} (\\mathbf{x}&#39; {\\boldsymbol A} \\mathbf{x}) \\\\ \\frac{\\partial }{\\partial x_2} (\\mathbf{x}&#39; {\\boldsymbol A} \\mathbf{x}) \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n} (\\mathbf{x}&#39; {\\boldsymbol A} \\mathbf{x}) \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1} \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j\\right) \\\\ \\frac{\\partial }{\\partial x_2} \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j\\right) \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n} \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j\\right) \\end{bmatrix} \\] For each component \\(x_k\\) in the vector \\(\\mathbf{x}\\), the derivative of \\(f(\\mathbf{x})\\) is: \\[ \\frac{\\partial}{\\partial x_k} f(\\mathbf{x}) = \\frac{\\partial}{\\partial x_k} \\left( \\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j \\right) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\frac{\\partial}{\\partial x_k} x_i a_{ij} x_j \\] Each term \\(x_i a_{ij} x_j\\) has two components that depend on \\( {\\boldsymbol x} \\): If \\(i = j = k\\), the derivative with respect to \\(x_k\\) is: \\[ \\frac{\\partial}{\\partial x_k} (x_i a_{ij} x_j) = 2 a_{kk} x_k \\] If \\(i \\neq j\\) and \\(i = k\\), the derivative with respect to \\(x_k\\) is: \\[ \\frac{\\partial}{\\partial x_k} (x_i a_{ij} x_j) = a_{kj} x_j \\] - Similarly, if \\(i \\neq j\\) and \\(j = k\\), the derivative with respect to \\(x_k\\) is: \\[ \\frac{\\partial}{\\partial x_k} (x_i a_{ij} x_j) = a_{ik} x_i \\] - Finally, if \\(i \\neq k\\) and \\(j \\neq k\\), then: \\[ \\frac{\\partial}{\\partial x_k} (x_i a_{ij} x_j) = 0 \\] Then \\[ \\frac{\\partial}{\\partial x_k} f(\\mathbf{x}) = 2 a_{kk} x_k + \\sum_{i \\neq k} a_{ik} x_i + \\sum_{j \\neq k} a_{kj} x_j \\] Now since \\( {\\boldsymbol A} \\) is symmetric (\\(a_{ij} = a_{ji}\\)), then: \\[\\begin{align*} \\frac{\\partial}{\\partial x_k} f(\\mathbf{x}) &amp;= 2 a_{kk} x_k + \\sum_{i \\neq k} a_{ik} x_i + \\sum_{i \\neq k} a_{ik} x_i \\\\ &amp;= 2 a_{kk} x_k + 2\\sum_{i \\neq k} a_{ik} x_i \\\\ &amp;= 2 \\left(\\sum_{i \\neq k} a_{ik} x_i + a_{kk}x_k \\right) \\\\ &amp;= 2 \\left(\\sum_{i = 1}^n a_{ki} x_i\\right) \\end{align*}\\] Then: \\[ \\nabla f = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1} \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j\\right) \\\\ \\frac{\\partial }{\\partial x_2} \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j\\right) \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n} \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j\\right) \\end{bmatrix} = \\begin{bmatrix} 2 \\sum_{i = 1}^n a_{1i} x_i \\\\ 2 \\sum_{i = 1}^n a_{2i} x_i \\\\ \\vdots \\\\ 2 \\sum_{i = 1}^n a_{ni} x_i \\end{bmatrix} = 2 {\\boldsymbol A} {\\boldsymbol x} \\] Finally for the second derivative we have that: In general, the Hessian matrix of a scalar function \\(f(\\mathbf{x})\\), where \\(\\mathbf{x} \\in \\mathbb{R}^n\\) is a vector of variables, is a matrix that contains all the second-order partial derivatives of the function. It is defined as: \\[ {\\boldsymbol H} (f) = \\frac{d^2 f}{d {\\boldsymbol x} d {\\boldsymbol x} &#39;} = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_2^2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1}\\left(\\frac{d f}{d {\\boldsymbol x} }\\right)&#39; \\\\ \\frac{\\partial }{\\partial x_2}\\left(\\frac{d f}{d {\\boldsymbol x} }\\right)&#39; \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n}\\left(\\frac{d f}{d {\\boldsymbol x} }\\right)&#39; \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1}\\left(2 {\\boldsymbol A} {\\boldsymbol x} \\right)&#39; \\\\ \\frac{\\partial }{\\partial x_2}\\left(2 {\\boldsymbol A} {\\boldsymbol x} \\right)&#39; \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n}\\left(2 {\\boldsymbol A} {\\boldsymbol x} \\right)&#39; \\end{bmatrix} \\] Now \\[ \\frac{\\partial }{\\partial x_k}\\left(2 {\\boldsymbol A} {\\boldsymbol x} \\right) = 2\\frac{\\partial }{\\partial x_k}\\begin{bmatrix} \\sum_{i = 1}^n a_{1i} x_i \\\\ \\sum_{i = 1}^n a_{2i} x_i \\\\ \\vdots \\\\ \\sum_{i = 1}^n a_{ni} x_i \\end{bmatrix} = 2 \\begin{bmatrix} \\frac{\\partial }{\\partial x_k} \\left(\\sum_{i = 1}^n a_{1i} x_i \\right)\\\\ \\frac{\\partial }{\\partial x_k} \\left(\\sum_{i = 1}^n a_{2i} x_i \\right)\\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_k} \\left(\\sum_{i = 1}^n a_{ni} x_i \\right) \\end{bmatrix} = 2 \\begin{bmatrix} a_{k1} \\\\ a_{k2} \\\\ \\vdots \\\\ a_{kn} \\end{bmatrix} \\] Then \\[ {\\boldsymbol H} (f) = \\frac{d^2 f}{d {\\boldsymbol x} d {\\boldsymbol x} &#39;} = \\begin{bmatrix} 2\\begin{bmatrix} a_{11} \\\\ a_{12} \\\\ \\vdots \\\\ a_{1n} \\end{bmatrix}&#39; \\\\ 2\\begin{bmatrix} a_{21} \\\\ a_{22} \\\\ \\vdots \\\\ a_{2n} \\end{bmatrix}&#39; \\\\ \\vdots \\\\ 2\\begin{bmatrix} a_{n1} \\\\ a_{n2} \\\\ \\vdots \\\\ a_{nn} \\end{bmatrix}&#39; \\end{bmatrix} = 2\\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{nn} \\end{bmatrix} = 2 {\\boldsymbol A} \\] 2.3 Probability Key probability concepts to understand include: Expected Value Variance Cross-Covariance Matrix Multivariate Normal Distribution 2.3.1 Expected Value The expected value (or mean) of a random vector is a fundamental concept in multivariate statistics. Just as the expected value of a random variable provides a measure of the “center” or “average” of the distribution, the expected value of a random vector captures the central location of a multidimensional distribution. 2.3.1.1 Definition Let \\(\\mathbf{X}\\) be a random vector in \\(\\mathbb{R}^n\\), represented as: \\[ \\mathbf{X} = \\begin{bmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{bmatrix}, \\] where \\(X_1, X_2, \\dots, X_n\\) are its components. The expected value of \\(\\mathbf{X}\\), denoted by \\(\\mathbb{E}[\\mathbf{X}]\\), is defined as the vector of the expected values of each component: \\[ \\mathbb{E}[\\mathbf{X}] = \\begin{bmatrix} \\mathbb{E}[X_1] \\\\ \\mathbb{E}[X_2] \\\\ \\vdots \\\\ \\mathbb{E}[X_n] \\end{bmatrix}. \\] This vector \\(\\mathbb{E}[\\mathbf{X}]\\) is also called the mean vector of \\(\\mathbf{X}\\). 2.3.1.2 Key Properties of the Expected Value of a Random Vector Linearity: For any scalar \\(a\\) and random vectors \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\), \\[ \\mathbb{E}[a\\mathbf{X} + \\mathbf{Y}] = a\\mathbb{E}[\\mathbf{X}] + \\mathbb{E}[\\mathbf{Y}]. \\] Expectation of a Constant Vector: If \\(\\mathbf{c}\\) is a constant vector in \\(\\mathbb{R}^n\\), then the expectation is simply the vector itself: \\[ \\mathbb{E}[\\mathbf{c}] = \\mathbf{c}. \\] Expectation of a Linear Transformation: If \\(\\mathbf{A}\\) is an \\(m \\times n\\) constant matrix, then for a random vector \\(\\mathbf{X}\\) in \\(\\mathbb{R}^n\\), \\[ \\mathbb{E}[\\mathbf{A} \\mathbf{X}] = \\mathbf{A} \\mathbb{E}[\\mathbf{X}]. \\] 2.3.2 Variance 2.3.2.1 Definition Let \\(\\mathbf{X}\\) be a random vector in \\(\\mathbb{R}^n\\), represented as: \\[ \\mathbf{X} = \\begin{bmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{bmatrix}, \\] where each \\(X_i\\) is a random variable. The variance-covariance matrix of \\(\\mathbf{X}\\), denoted by \\( {\\mathbb{V}} (\\mathbf{X})\\) or \\(\\boldsymbol{\\Sigma}\\), is defined as: \\[ {\\mathbb{V}} (\\mathbf{X}) = \\mathbb{E}[(\\mathbf{X} - \\mathbb{E}[\\mathbf{X}])(\\mathbf{X} - \\mathbb{E}[\\mathbf{X}])&#39;]. \\] The \\((i, j)\\) entry of \\( {\\mathbb{V}} (\\mathbf{X})\\) is given by \\( {\\mathbb{C}} (X_i, X_j)\\), representing the covariance between components \\(X_i\\) and \\(X_j\\). If \\(\\mathbf{X}\\) has \\(n\\) components, \\( {\\mathbb{V}} (\\mathbf{X})\\) will be an \\(n \\times n\\) symmetric matrix where: - Diagonal entries represent the variances of each component, i.e., \\( {\\mathbb{V}} (X_i) = {\\mathbb{C}} (X_i, X_i)\\). - Off-diagonal entries represent the covariances between different components, i.e., \\( {\\mathbb{C}} (X_i, X_j)\\) for \\(i \\neq j\\). 2.3.2.2 Key Properties of the Variance-Covariance Matrix Symmetry: The variance-covariance matrix is symmetric, meaning: \\[ {\\mathbb{V}} (\\mathbf{X}) = {\\mathbb{V}} (\\mathbf{X})^T. \\] Non-negativity: The variance-covariance matrix is positive semi-definite, which implies: \\[ \\mathbf{z}^T {\\mathbb{V}} (\\mathbf{X}) \\mathbf{z} \\geq 0 \\quad \\text{for any vector } \\mathbf{z} \\in \\mathbb{R}^n. \\] This property indicates that variances (the diagonal entries) are always non-negative. Variance of a Linear Transformation: If \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix, then the variance of the transformed random vector \\(\\mathbf{A} \\mathbf{X}\\) is given by: \\[ {\\mathbb{V}} (\\mathbf{A} \\mathbf{X}) = \\mathbf{A} \\, {\\mathbb{V}} (\\mathbf{X}) \\, \\mathbf{A}^T. \\] Variance of Independent Random Variables: If the components of \\(\\mathbf{X}\\) are mutually independent, the off-diagonal entries of \\( {\\mathbb{V}} (\\mathbf{X})\\) (the covariances) are zero, resulting in a diagonal covariance matrix. Variance of the Sum of Two Arbitrary Random Vectors: If \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) are two arbitrary random vectors in \\(\\mathbb{R}^n\\), the variance of their sum is given by: \\[ {\\mathbb{V}} (\\mathbf{X} + \\mathbf{Y}) = {\\mathbb{V}} (\\mathbf{X}) + {\\mathbb{V}} (\\mathbf{Y}) + 2 \\, {\\mathbb{C}} (\\mathbf{X}, \\mathbf{Y}), \\] where \\( {\\mathbb{C}} (\\mathbf{X}, \\mathbf{Y})\\) is the cross-covariance matrix between \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\): \\[ {\\mathbb{C}} (\\mathbf{X}, \\mathbf{Y}) = \\mathbb{E}\\left[(\\mathbf{X} - \\mathbb{E}[\\mathbf{X}])(\\mathbf{Y} - \\mathbb{E}[\\mathbf{Y}])&#39;\\right]. \\] Variance of the Sum of Two Independent Random Vectors: If \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) are two independent random vectors in \\(\\mathbb{R}^n\\), then the variance of their sum is given by the sum of their individual variances: \\[ {\\mathbb{V}} (\\mathbf{X} + \\mathbf{Y}) = {\\mathbb{V}} (\\mathbf{X}) + {\\mathbb{V}} (\\mathbf{Y}). \\] Since \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) are independent, their covariance \\( {\\mathbb{C}} (\\mathbf{X}, \\mathbf{Y})\\) is zero, simplifying the variance of the sum. Trace of the Variance: A related measure of the variance of a random vector \\(\\mathbf{X}\\), is given by \\[ \\text{tr}( {\\mathbb{V}} [\\mathbf{X}]) = \\mathbb{E}[(\\mathbf{X} - \\mathbb{E}[\\mathbf{X}])&#39;(\\mathbf{X} - \\mathbb{E}[\\mathbf{X}])] \\] 2.3.3 Cross-Covariance Matrix The cross-covariance matrix measures the covariance between two random vectors, providing insights into the linear relationships between different components of these vectors. Given two random vectors \\(\\mathbf{X} \\in \\mathbb{R}^n\\) and \\(\\mathbf{Y} \\in \\mathbb{R}^m\\), the cross-covariance matrix between \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) captures how each component of \\(\\mathbf{X}\\) varies with each component of \\(\\mathbf{Y}\\). 2.3.3.1 Definition Let \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) be two random vectors with mean vectors \\(\\mathbb{E}[\\mathbf{X}] = \\boldsymbol{\\mu}_{\\mathbf{X}}\\) and \\(\\mathbb{E}[\\mathbf{Y}] = \\boldsymbol{\\mu}_{\\mathbf{Y}}\\). The cross-covariance matrix of \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) is defined as: \\[ {\\mathbb{C}} (\\mathbf{X}, \\mathbf{Y}) = \\mathbb{E}[(\\mathbf{X} - \\boldsymbol{\\mu}_{\\mathbf{X}})(\\mathbf{Y} - \\boldsymbol{\\mu}_{\\mathbf{Y}})^T]. \\] This matrix is of dimension \\(n \\times m\\), where each element \\(( {\\mathbb{C}} (\\mathbf{X}, \\mathbf{Y}))_{ij}\\) represents the covariance between the \\(i\\)-th component of \\(\\mathbf{X}\\) and the \\(j\\)-th component of \\(\\mathbf{Y}\\): \\[ ( {\\mathbb{C}} (\\mathbf{X}, \\mathbf{Y}))_{ij} = {\\mathbb{C}} (X_i, Y_j) = \\mathbb{E}[(X_i - \\mu_{X_i})(Y_j - \\mu_{Y_j})]. \\] 2.3.3.2 Key Properties Symmetry in Covariance: If \\(\\mathbf{X} = \\mathbf{Y}\\), then \\( {\\mathbb{C}} (\\mathbf{X}, \\mathbf{Y})\\) reduces to the covariance matrix of \\(\\mathbf{X}\\), which is symmetric. For distinct vectors \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\), the cross-covariance matrix \\( {\\mathbb{C}} (\\mathbf{X}, \\mathbf{Y})\\) is generally not symmetric. Relationship with Joint Covariance Matrix: If \\(\\mathbf{Z} = \\begin{bmatrix} \\mathbf{X} \\\\ \\mathbf{Y} \\end{bmatrix}\\) is a combined random vector, then the covariance matrix of \\(\\mathbf{Z}\\) is: \\[ {\\mathbb{C}} (\\mathbf{Z}) = \\begin{bmatrix} {\\mathbb{C}} (\\mathbf{X}) &amp; {\\mathbb{C}} (\\mathbf{X}, \\mathbf{Y}) \\\\ {\\mathbb{C}} (\\mathbf{Y}, \\mathbf{X}) &amp; {\\mathbb{C}} (\\mathbf{Y}) \\end{bmatrix}. \\] Linearity: If \\(a\\) and \\(b\\) are constants and \\(\\mathbf{X}_1\\) and \\(\\mathbf{X}_2\\) are random vectors of the same dimension as \\(\\mathbf{X}\\), then: \\[ {\\mathbb{C}} (a\\mathbf{X}_1 + b\\mathbf{X}_2, \\mathbf{Y}) = a {\\mathbb{C}} (\\mathbf{X}_1, \\mathbf{Y}) + b {\\mathbb{C}} (\\mathbf{X}_2, \\mathbf{Y}). \\] Furthermore, if \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are matrices of compatible dimensions, and \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) are random vectors then the cross-covariance of \\(\\mathbf{A}\\mathbf{X}\\) and \\(\\mathbf{B}\\mathbf{Y}\\) is given by: \\[ {\\mathbb{C}} (\\mathbf{A}\\mathbf{X}, \\mathbf{B}\\mathbf{Y}) = \\mathbf{A} {\\mathbb{C}} (\\mathbf{X}, \\mathbf{Y}) \\mathbf{B}^T. \\] Zero Cross-Covariance and Independence: If \\( {\\mathbb{C}} (\\mathbf{X}, \\mathbf{Y}) = \\mathbf{0}\\), it implies that \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) are uncorrelated, but it does not necessarily imply independence unless \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) are jointly normally distributed. 2.3.4 Multivariate Normal Distribution The multivariate normal distribution generalizes the concept of the normal distribution to multiple dimensions, describing the behavior of random vectors whose elements are jointly normally distributed. It is widely used in statistics and machine learning due to its well-behaved properties and its applicability to modeling correlations between variables. 2.3.4.1 Definition A random vector \\(\\mathbf{X} = (X_1, X_2, \\dots, X_n)^T\\) is said to follow a multivariate normal distribution if it has a probability density function of the form: \\[ f(\\mathbf{X}) = \\frac{1}{(2\\pi)^{n/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{X} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{X} - \\boldsymbol{\\mu}) \\right), \\] where: \\(\\mathbf{X} \\in \\mathbb{R}^n\\) is the random vector, \\(\\boldsymbol{\\mu} \\in \\mathbb{R}^n\\) is the mean vector, \\(\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{n \\times n}\\) is the covariance matrix, assumed to be symmetric and positive definite. This distribution is denoted as \\(\\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\). 2.3.4.2 Key Properties Marginal Distributions: Any subset of the components of \\(\\mathbf{X}\\) is also normally distributed. If \\(\\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), then any partition of \\(\\mathbf{X}\\) results in a marginal distribution that is also multivariate normal. Affine Transformation: For a matrix \\(\\mathbf{A}\\) and vector \\(\\mathbf{b}\\) of appropriate dimensions, the affine transformation \\(\\mathbf{Y} = \\mathbf{A}\\mathbf{X} + \\mathbf{b}\\) also follows a multivariate normal distribution, \\(\\mathbf{Y} \\sim \\mathcal{N}(\\mathbf{A} \\boldsymbol{\\mu} + \\mathbf{b}, \\mathbf{A} \\boldsymbol{\\Sigma} \\mathbf{A}^T)\\). Conditional Distributions: For a partitioned vector \\(\\mathbf{X} = (\\mathbf{X}_1, \\mathbf{X}_2)^T\\), with corresponding partitions of the mean vector and covariance matrix: \\[ \\boldsymbol{\\mu} = \\begin{pmatrix} \\boldsymbol{\\mu}_1 \\\\ \\boldsymbol{\\mu}_2 \\end{pmatrix}, \\quad \\boldsymbol{\\Sigma} = \\begin{pmatrix} \\boldsymbol{\\Sigma}_{11} &amp; \\boldsymbol{\\Sigma}_{12} \\\\ \\boldsymbol{\\Sigma}_{21} &amp; \\boldsymbol{\\Sigma}_{22} \\end{pmatrix}, \\] the conditional distribution \\(\\mathbf{X}_1 | \\mathbf{X}_2 = \\mathbf{x}_2\\) is also multivariate normal: \\[ \\mathbf{X}_1 | \\mathbf{X}_2 = \\mathbf{x}_2 \\sim \\mathcal{N}(\\boldsymbol{\\mu}_1 + \\boldsymbol{\\Sigma}_{12} \\boldsymbol{\\Sigma}_{22}^{-1} (\\mathbf{x}_2 - \\boldsymbol{\\mu}_2), \\boldsymbol{\\Sigma}_{11} - \\boldsymbol{\\Sigma}_{12} \\boldsymbol{\\Sigma}_{22}^{-1} \\boldsymbol{\\Sigma}_{21}). \\] Independence and Uncorrelatedness: For a multivariate normal distribution, zero covariance implies independence. If two components \\(X_i\\) and \\(X_j\\) (or two subvectors) have a covariance of zero, they are independent. Moment Generating Function: The moment generating function of \\(\\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) is: \\[ M_{\\mathbf{X}}(\\mathbf{t}) = \\exp\\left( \\mathbf{t}^T \\boldsymbol{\\mu} + \\frac{1}{2} \\mathbf{t}^T \\boldsymbol{\\Sigma} \\mathbf{t} \\right). \\] Maximum Entropy: Among all distributions with a given mean vector and covariance matrix, the multivariate normal distribution has the maximum entropy, making it the most “uninformative” or spread out. 2.3.5 \\(\\chi^2\\) Distribution The chi-squared distribution is a probability distribution that arises naturally in statistics, particularly in hypothesis testing and estimation problems. It is defined as the distribution of a sum of the squares of independent standard normal random variables. Formally, let \\(Z_1, Z_2, \\dots, Z_k\\) be \\(k\\) independent random variables, each following a standard normal distribution (mean \\(0\\) and variance \\(1\\)). Then, the sum of their squares: \\[ Q = Z_1^2 + Z_2^2 + \\cdots + Z_k^2 \\] follows a chi-squared distribution with \\(k\\) degrees of freedom. We denote this as: \\[ Q \\sim \\chi^2_k \\] 2.3.5.1 Key Properties of the Chi-Squared Distribution Degrees of Freedom (\\(k\\)): The parameter \\(k\\) determines the shape of the distribution. Larger \\(k\\) values result in a distribution that becomes more symmetric and approaches a normal distribution (as \\(k \\to \\infty\\)). Mean and Variance: Mean: \\(\\mathbb{E}[Q] = k\\) Variance: \\(\\mathbb{V}[Q] = 2k\\) Additivity: If \\(Q_1 \\sim \\chi^2_{k_1}\\) and \\(Q_2 \\sim \\chi^2_{k_2}\\) are independent, then \\(Q_1 + Q_2 \\sim \\chi^2_{k_1 + k_2}\\). Special Case: For \\(k = 1\\), the chi-squared distribution is simply the square of a standard normal variable, \\(Q = Z^2\\). The chi-squared distribution is widely used in statistical tests, such as the chi-squared test for independence or goodness-of-fit, and in the construction of confidence intervals for variances. 2.3.6 \\(t\\) Distribution The t-distribution, also known as Student’s t-distribution, is a probability distribution that arises frequently in hypothesis testing, particularly when the sample size is small, and the population standard deviation is unknown. It is defined as the distribution of a standard normal random variable divided by the square root of an independent chi-squared random variable, scaled by its degrees of freedom. 2.3.6.1 Definition Let: - \\(Z\\) be a standard normal random variable (\\(Z \\sim N(0, 1)\\)), - \\(Q\\) be a chi-squared random variable with \\(k\\) degrees of freedom (\\(Q \\sim \\chi^2_k\\)), independent of \\(Z\\). Then, the random variable: \\[ T = \\frac{Z}{\\sqrt{\\frac{Q}{k}}} \\] follows a t-distribution with \\(k\\) degrees of freedom. We write: \\[ T \\sim t_k \\] 2.3.6.2 Key Properties of the t-Distribution Degrees of Freedom (\\(k\\)): The parameter \\(k\\) determines the shape of the t-distribution. As \\(k\\) increases, the t-distribution approaches the standard normal distribution. Symmetry: The t-distribution is symmetric about zero, similar to the normal distribution. Tails: The t-distribution has heavier tails than the normal distribution, meaning it gives more probability to extreme values. This reflects increased uncertainty when estimating the population mean with small sample sizes. Moments: Mean: \\(\\mathbb{E}[T] = 0\\) for \\(k &gt; 1\\) Variance: \\(\\mathbb{V}[T] = \\frac{k}{k-2}\\) for \\(k &gt; 2\\) Higher moments exist only for \\(k &gt; m\\), where \\(m\\) is the order of the moment. 2.3.6.3 Applications The t-distribution is widely used in: 1. t-tests for hypothesis testing, such as testing means of small samples. 2. Constructing confidence intervals for a population mean when the population standard deviation is unknown. 3. Regression analysis, where it appears in tests for regression coefficients. The t-distribution plays a fundamental role in statistics, bridging the gap between small-sample and large-sample inference. 2.3.7 \\(F\\) Distribution The F-distribution, also known as Fisher-Snedecor distribution, arises frequently in statistics, especially in hypothesis testing and variance analysis (ANOVA). It is defined as the distribution of the ratio of two independent chi-squared random variables, each divided by their respective degrees of freedom. 2.3.7.1 Definition Let: - \\(Q_1 \\sim \\chi^2_{k_1}\\), a chi-squared random variable with \\(k_1\\) degrees of freedom, - \\(Q_2 \\sim \\chi^2_{k_2}\\), a chi-squared random variable with \\(k_2\\) degrees of freedom, - \\(Q_1\\) and \\(Q_2\\) are independent. Then, the random variable: \\[ F = \\frac{\\frac{Q_1}{k_1}}{\\frac{Q_2}{k_2}} \\] follows an F-distribution with \\(k_1\\) and \\(k_2\\) degrees of freedom. We write: \\[ F \\sim F(k_1, k_2) \\] 2.3.7.2 Key Properties of the F-Distribution Degrees of Freedom: The parameters \\(k_1\\) and \\(k_2\\) determine the shape of the F-distribution. \\(k_1\\) (numerator degrees of freedom) is associated with the variability of the first chi-squared variable. \\(k_2\\) (denominator degrees of freedom) is associated with the variability of the second chi-squared variable. Support: \\(F\\) is defined for \\(F \\geq 0\\). Asymmetry: The F-distribution is not symmetric; it is skewed to the right, especially for small degrees of freedom. As \\(k_1\\) and \\(k_2\\) increase, it approaches a normal distribution. Mean: \\(\\mathbb{E}[F] = \\frac{k_2}{k_2 - 2}\\) for \\(k_2 &gt; 2\\). Variance: \\(\\mathbb{V}[F] = \\frac{2k_2^2 (k_1 + k_2 - 2)}{k_1 (k_2 - 2)^2 (k_2 - 4)}\\) for \\(k_2 &gt; 4\\). Special Cases: When \\(k_1 = 1\\), the F-distribution is equivalent to the square of a t-distribution with \\(k_2\\) degrees of freedom: \\(F(1, k_2) = t_{k_2}^2\\). 2.3.7.3 Applications ANOVA (Analysis of Variance): The F-statistic is used to test whether multiple groups have the same variance or means. Model Comparison: In regression, the F-distribution is used to compare nested models, assessing whether additional predictors improve the fit of the model. Hypothesis Testing: The F-distribution appears in tests of equality of variances (Levene’s test or Bartlett’s test). The F-distribution is essential in statistics for comparing variances and testing model adequacy, making it a cornerstone of many inferential procedures. 2.4 Statistics Essential statistical concepts include: Bias of an Estimator Unbiased Estimator Mean Square Error of an Estimator Consistent Minimum Variance Interval Estimation Hypothesis Testing 2.4.1 Bias of an Estimator The bias of an estimator \\(\\hat{\\mathbf{\\theta}}\\) for a vector parameter \\(\\mathbf{\\theta}\\) is defined as the difference between the expected value of the estimator and the true value of the parameter. Formally, if \\(\\mathbf{\\theta} \\in \\mathbb{R}^n\\) is a vector parameter, then the bias of the estimator \\(\\hat{\\mathbf{\\theta}}\\) is: \\[ \\text{Bias}(\\hat{\\mathbf{\\theta}}) = \\mathbb{E}[\\hat{\\mathbf{\\theta}}] - \\mathbf{\\theta}. \\] 2.4.1.1 Key Points Interpretation: Bias measures how far, on average, the estimator \\(\\hat{\\mathbf{\\theta}}\\) is from the true parameter \\(\\mathbf{\\theta}\\). If the bias is zero, the estimator is said to be unbiased. Component-wise Bias: For each component of the estimator \\(\\hat{\\mathbf{\\theta}} = (\\hat{\\theta}_1, \\dots, \\hat{\\theta}_n)^T\\), the bias can be expressed as: \\[ \\text{Bias}(\\hat{\\theta}_i) = \\mathbb{E}[\\hat{\\theta}_i] - \\theta_i, \\quad \\text{for each } i = 1, \\dots, n. \\] This component-wise breakdown is helpful for understanding how each part of the vector estimator deviates from the true values. Total Bias: The total bias can be viewed as a vector, summarizing the systematic error in each dimension of the estimator. 2.4.1.2 Example In practice, if \\(\\hat{\\mathbf{\\theta}}\\) is the sample mean vector of a random vector \\(\\mathbf{X}\\), and \\(\\mathbf{\\theta} = \\mathbb{E}[\\mathbf{X}]\\), then \\(\\text{Bias}(\\hat{\\mathbf{\\theta}})\\) will be zero, making the sample mean an unbiased estimator of the population mean. 2.4.2 Unbiased Estimator An unbiased estimator of a vector parameter is an estimator that, on average, accurately estimates the true value of the parameter. Formally, let \\(\\mathbf{\\theta} \\in \\mathbb{R}^n\\) be a vector parameter of interest, and let \\(\\hat{\\mathbf{\\theta}}\\) be an estimator of \\(\\mathbf{\\theta}\\). Then, \\(\\hat{\\mathbf{\\theta}}\\) is an unbiased estimator of \\(\\mathbf{\\theta}\\) if the following condition holds: \\[ \\mathbb{E}[\\hat{\\mathbf{\\theta}}] = \\mathbf{\\theta} \\quad \\forall {\\boldsymbol \\tau}\\in {\\mathbb{R}} ^n. \\] 2.4.2.1 Key Points Component-wise Unbiasedness: Each component of \\(\\hat{\\mathbf{\\theta}}\\), say \\(\\hat{\\theta}_i\\), must be an unbiased estimator of the corresponding component \\(\\theta_i\\) of \\(\\mathbf{\\theta}\\): \\[ \\mathbb{E}[\\hat{\\theta}_i] = \\theta_i, \\quad \\text{for all } i = 1, \\dots, n. \\] No Systematic Bias: Unbiasedness implies that, on average, the estimator neither overestimates nor underestimates the true value of \\(\\mathbf{\\theta}\\) across repeated sampling. Applications: Unbiased estimators are crucial in statistics, as they provide estimations that are theoretically centered around the true parameter values, though they may vary due to sampling variation. An example is the sample mean \\(\\mathbf{\\bar{X}}\\) as an estimator for the population mean \\(\\mathbf{\\mu}\\) of a random vector \\(\\mathbf{X}\\), where \\(\\mathbb{E}[\\mathbf{\\bar{X}}] = \\mathbf{\\mu}\\). 2.4.3 Mean Square Error of an Estimator The Mean Square Error (MSE) of an estimator \\(\\hat{\\mathbf{\\theta}}\\) for a vector parameter \\(\\mathbf{\\theta}\\) measures the average squared deviation of the estimator from the true parameter. Formally, for a parameter vector \\(\\mathbf{\\theta} \\in \\mathbb{R}^n\\) and an estimator \\(\\hat{\\mathbf{\\theta}}\\), the MSE is defined as: \\[ {\\mathbb{M}} (\\hat{\\mathbf{\\theta}}) = \\mathbb{E} \\left[ \\|\\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}\\|^2 \\right], \\] where \\(\\|\\cdot\\|\\) denotes the Euclidean (or 2-norm) of a vector. Expanding this expression, we can write: \\[ {\\mathbb{M}} (\\hat{\\mathbf{\\theta}}) = \\mathbb{E} \\left[ (\\hat{\\mathbf{\\theta}} - \\mathbf{\\theta})^T (\\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}) \\right]. \\] 2.4.3.1 Key Components The MSE can be decomposed into two main parts: variance and squared bias. \\[ {\\mathbb{M}} (\\hat{\\mathbf{\\theta}}) = \\text{tr}(\\text{Var}(\\hat{\\mathbf{\\theta}})) + \\|\\text{Bias}(\\hat{\\mathbf{\\theta}})\\|^2, \\] where: - Variance: \\(\\text{Var}(\\hat{\\mathbf{\\theta}}) = \\mathbb{E}[(\\hat{\\mathbf{\\theta}} - \\mathbb{E}[\\hat{\\mathbf{\\theta}}])(\\hat{\\mathbf{\\theta}} - \\mathbb{E}[\\hat{\\mathbf{\\theta}}])^T]\\), representing the variability in the estimator. - Bias: \\(\\text{Bias}(\\hat{\\mathbf{\\theta}}) = \\mathbb{E}[\\hat{\\mathbf{\\theta}}] - \\mathbf{\\theta}\\), representing the systematic deviation from the true parameter. 2.4.3.2 Key Properties Unbiased Estimator: If \\(\\hat{\\mathbf{\\theta}}\\) is unbiased, then \\(\\text{Bias}(\\hat{\\mathbf{\\theta}}) = 0\\), and the MSE is simply the trace of the covariance matrix of \\(\\hat{\\mathbf{\\theta}}\\): \\[ {\\mathbb{M}} (\\hat{\\mathbf{\\theta}}) = \\text{tr}(\\text{Var}(\\hat{\\mathbf{\\theta}})). \\] Bias-Variance Trade-off: The MSE quantifies the balance between bias and variance. Lowering bias often increases variance and vice versa, a key concept in statistical estimation. Overall Error Metric: The MSE provides a comprehensive measure of the estimator’s accuracy, taking both variance and bias into account, making it an essential criterion in evaluating estimators. "],["introduction.html", "3 Introduction 3.1 Key Challenges 3.2 Core Concepts 3.3 Applications", " 3 Introduction High-dimensional multivariate statistics deals with the analysis of data where both the number of variables (\\(p\\)) and the number of observations (\\(n\\)) can be large, and often, \\(p\\) is comparable to or even greater than \\(n\\), making traditional methods unavailable or computationally impractical. This setting arises naturally in modern applications like genomics, finance, and machine learning, where data sets contain hundreds or thousands of variables. 3.1 Key Challenges Curse of Dimensionality: As \\(p\\) grows, classical statistical methods (e.g., ordinary least squares, classical covariance estimation) break down, become unstable or are computationally expensive. Multicollinearity: High correlation between variables can lead to singular or nearly singular covariance matrices. As the number of variables increases this scenario becomes more likely, if \\(p \\gg n\\) this have to be the case necessarily. Overfitting: When \\(p \\gg n\\), models tend to fit noise rather than signal. 3.2 Core Concepts Regularized Estimation: Methods like ridge regression, LASSO, and graphical models introduce constraints to stabilize estimation. Dimensionality Reduction: Techniques like Principal Component Analysis (PCA) and Factor Analysis help summarize information in fewer dimensions. High-Dimensional Covariance and Precision Matrices: Classical estimators (e.g., sample covariance) fail when \\(p &gt; n\\), requiring alternatives like shrinkage estimators or sparsity inducing approaches. Multiple Testing and False Discovery Rate (FDR): In high-dimensional settings, multiple hypothesis tests lead to inflated error rates, necessitating corrections like the Benjamini-Hochberg procedure. 3.3 Applications Genomics: Identifying genes associated with diseases from thousands of genetic markers. Finance: Portfolio optimization where the number of assets is large relative to available data. Machine Learning: Feature selection and model regularization in predictive modeling. High-dimensional statistics continues to evolve, with ongoing research in areas like robust estimation, Bayesian methods, and deep learning applications. As more computational power and data becomes availale, methods that were considered High-Dimensional can be approached with traditional methods. "],["linear-regression.html", "4 Linear Regression 4.1 Machine Learning 4.2 Bayesian Linear Regression 4.3 Computational Comparisson 4.4 Efficient Computation", " 4 Linear Regression 4.1 Machine Learning In this section, we will not make any probability assumption, and we will treat the problem only as an optimization problem. 4.1.1 Ordinary Least Squares 4.1.1.1 Model Specification Let \\(y_i \\in \\mathbb{R}\\) be the response variable and \\(x_i \\in \\mathbb{R}^p\\) be the vector of predictors for observation \\(i\\), where \\(i = 1, \\dots, n\\). The multiple linear regression model is given by: \\[ {\\boldsymbol y} = {\\boldsymbol X} {\\boldsymbol \\beta} + {\\boldsymbol e} \\] where: - \\( {\\boldsymbol y} \\in \\mathbb{R}^{n}\\) is the response vector (each entry corresponds to an observation), - \\( {\\boldsymbol X} \\in \\mathbb{R}^{n \\times p}\\) is the design matrix (including predictors, typically with an intercept column of ones), - \\( {\\boldsymbol \\beta} \\in \\mathbb{R}^{p}\\) is the coefficient vector to be estimated, - \\( {\\boldsymbol e} \\in \\mathbb{R}^{n}\\) is the error vector. 4.1.1.2 Minimization Problem The objective is to minimize the sum of squared errors (SSE): \\[ \\min_{ {\\boldsymbol \\beta} } \\mathcal{L}( {\\boldsymbol \\beta} ) = \\min_{ {\\boldsymbol \\beta} } \\| {\\boldsymbol y} - {\\boldsymbol X} {\\boldsymbol \\beta} \\|_2^2. \\] Expanding the loss function: \\[ \\mathcal{L}( {\\boldsymbol \\beta} ) = ( {\\boldsymbol y} - {\\boldsymbol X} {\\boldsymbol \\beta} )&#39; ( {\\boldsymbol y} - {\\boldsymbol X} {\\boldsymbol \\beta} ). \\] 4.1.1.3 Solution To minimize \\(\\mathcal{L}(\\beta)\\), we take the derivative with respect to \\(\\beta\\): \\[ \\frac{\\partial \\mathcal{L}( {\\boldsymbol \\beta} )}{\\partial {\\boldsymbol \\beta} } = -2 {\\boldsymbol X} &#39; ( {\\boldsymbol y} - {\\boldsymbol X} {\\boldsymbol \\beta} ). \\] Setting this to zero, we obtain the normal equation: \\[ {\\boldsymbol X} &#39; {\\boldsymbol X} {\\boldsymbol \\beta} = {\\boldsymbol X} &#39; {\\boldsymbol y} . \\] If \\( {\\boldsymbol X} &#39; {\\boldsymbol X} \\) is invertible (i.e., \\( {\\boldsymbol X} \\) has full column rank), we solve for \\( {\\boldsymbol \\beta} \\): \\[ {\\boldsymbol \\beta} = ( {\\boldsymbol X} &#39; {\\boldsymbol X} )^{-1} {\\boldsymbol X} &#39; {\\boldsymbol y} . \\] To check that this is a minimizer, we compute the Hessian of \\(\\mathcal{L}( {\\boldsymbol \\beta} )\\): \\[ H = \\frac{\\partial^2 \\mathcal{L}( {\\boldsymbol \\beta} )}{\\partial {\\boldsymbol \\beta} \\partial {\\boldsymbol \\beta} &#39;} = 2 {\\boldsymbol X} &#39; {\\boldsymbol X} . \\] Since \\( {\\boldsymbol X} &#39; {\\boldsymbol X} \\) is positive semidefinite and positive definite if \\( {\\boldsymbol X} \\) has full column rank, the function \\(\\mathcal{L}( {\\boldsymbol \\beta} )\\) is convex. Hence, the critical point \\( {\\boldsymbol \\beta} = ( {\\boldsymbol X} &#39; {\\boldsymbol X} )^{-1} {\\boldsymbol X} &#39; {\\boldsymbol y} \\) is the unique global minimum. The OLS solution is often denoted as: \\[ \\boldsymbol{\\beta}_{\\text{OLS}} = (\\mathbf{X}&#39;\\mathbf{X})^{-1} \\mathbf{X}&#39;\\mathbf{y}. \\] Notice that we noted that \\( {\\boldsymbol X} \\) is full column rank, when this condition is not met (or is close to not be met), other approaches are necessary. 4.1.2 Ridge Regression 4.1.2.1 Introduction When the design matrix \\(\\mathbf{X}\\) is not full rank, the matrix \\(\\mathbf{X}&#39;\\mathbf{X}\\) is singular (i.e., not invertible), making it impossible to compute the least squares solution \\[ \\boldsymbol{\\beta}_{\\text{OLS}} = (\\mathbf{X}&#39;\\mathbf{X})^{-1} \\mathbf{X}&#39;\\mathbf{y}. \\] This issue arises when there are more predictors than observations (\\(p &gt; n\\)) or when there is multicollinearity among the predictors. To address this, Ridge Regression introduces a small positive penalty term that regularizes the matrix \\(\\mathbf{X}&#39;\\mathbf{X}\\), making it invertible. This method is also known as Tikhonov regularization or \\(L_2\\) regularization. When \\(p &gt; n\\) then we can approximate \\(\\mathbf{X}&#39;\\mathbf{X}\\) with \\(\\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I}\\), where \\(\\lambda &gt; 0\\) can be as small as necessary. This approximation is helpful since \\(\\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I}\\) is always non-singular. 4.1.2.2 Non-singularity of the Approximation Let \\(\\mathbf{X}\\) be an \\(n \\times p\\) matrix. Its SVD decomposition is \\[ \\mathbf{X} = \\mathbf{U} \\mathbf{D} \\mathbf{V}&#39; \\] where: - \\(\\mathbf{U} \\in \\mathbb{R}^{n \\times n}\\) is an orthonormal matrix (\\(\\mathbf{U}&#39; \\mathbf{U} = \\mathbf{I}_n\\)), - \\(\\mathbf{V} \\in \\mathbb{R}^{p \\times p}\\) is an orthonormal matrix (\\(\\mathbf{V}&#39; \\mathbf{V} = \\mathbf{I}_p\\)), - \\(\\mathbf{D} \\in \\mathbb{R}^{n \\times p}\\) is a diagonal matrix with singular values \\(d_1, d_2, \\dots, d_n \\geq 0\\) along the diagonal. Since \\(p &gt; n\\), the number of singular values is at most \\(n\\), meaning that \\(\\mathbf{X}&#39; \\mathbf{X}\\) has at most rank \\(n\\) and is not invertible when \\(p &gt; n\\). Using the SVD of \\(\\mathbf{X}\\), we can express \\[ \\mathbf{X}&#39; \\mathbf{X} = (\\mathbf{U} \\mathbf{D} \\mathbf{V}&#39;)&#39; (\\mathbf{U} \\mathbf{D} \\mathbf{V}&#39;) = \\mathbf{V} \\mathbf{D}&#39; \\mathbf{U}&#39; \\mathbf{U} \\mathbf{D} \\mathbf{V}&#39; = \\mathbf{V} (\\mathbf{D}&#39; \\mathbf{D}) \\mathbf{V}&#39;. \\] Since \\(\\mathbf{U}\\) is an \\(n \\times n\\) orthogonal matrix, \\(\\mathbf{D}&#39; \\mathbf{D}\\) is a \\(p \\times p\\) diagonal matrix: \\[ \\mathbf{D}&#39; \\mathbf{D} = \\begin{bmatrix} d_1^2 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; d_2^2 &amp; \\dots &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; d_n^2 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ \\end{bmatrix}. \\] The first \\(n\\) diagonal entries are \\(d_i^2\\), corresponding to the squared singular values of \\(\\mathbf{X}\\). The remaining \\(p - n\\) diagonal entries are zero, meaning \\(\\mathbf{X}&#39; \\mathbf{X}\\) has \\(p - n\\) zero eigenvalues and is not full rank. Now, consider the modified matrix: \\[ \\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I}. \\] Using the SVD form of \\(\\mathbf{X}&#39; \\mathbf{X}\\), we have: \\[ \\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I} = \\mathbf{V} (\\mathbf{D}&#39; \\mathbf{D}) \\mathbf{V}&#39; + \\lambda \\mathbf{I}. \\] Since \\(\\mathbf{I} = \\mathbf{V} \\mathbf{I} \\mathbf{V}&#39;\\), we rewrite this as: \\[ \\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I} = \\mathbf{V} (\\mathbf{D}&#39; \\mathbf{D} + \\lambda \\mathbf{I}) \\mathbf{V}&#39;. \\] Since \\(\\mathbf{D}&#39; \\mathbf{D}\\) is diagonal, adding \\(\\lambda \\mathbf{I}\\) results in: \\[ \\mathbf{D}&#39; \\mathbf{D} + \\lambda \\mathbf{I} = \\begin{bmatrix} d_1^2 + \\lambda &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; d_2^2 + \\lambda &amp; \\dots &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; d_n^2 + \\lambda &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; \\dots &amp; 0 &amp; \\lambda &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; \\dots &amp; \\lambda \\\\ \\end{bmatrix}. \\] The first \\(n\\) diagonal entries are \\(d_i^2 + \\lambda\\), all strictly positive because \\(d_i^2 \\geq 0\\) and \\(\\lambda &gt; 0\\). The last \\(p - n\\) diagonal entries are \\(\\lambda\\) (also strictly positive). Thus, all eigenvalues of \\(\\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I}\\) are strictly positive, implying that it is full rank and invertible. Since \\(\\mathbf{V}\\) is an orthogonal matrix, the entire expression \\[ \\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I} = \\mathbf{V} (\\mathbf{D}&#39; \\mathbf{D} + \\lambda \\mathbf{I}) \\mathbf{V}&#39; \\] is invertible, because an orthogonal matrix times an invertible diagonal matrix remains invertible. So, even when \\(p &gt; n\\), adding \\(\\lambda \\mathbf{I}\\) shifts all eigenvalues away from zero, ensuring that \\[ \\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I} \\] is always invertible for \\(\\lambda &gt; 0\\). This guarantees that Ridge Regression always has a unique solution: \\[ \\boldsymbol{\\beta}_{\\text{ridge}} = (\\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\mathbf{X}&#39; \\mathbf{y}. \\] 4.1.2.3 Ridge Regression as a Minimization Problem Instead of minimizing the standard sum of squared errors, Ridge Regression solves the following regularized problem: \\[ \\min_{\\boldsymbol{\\beta}} \\mathcal{L}(\\boldsymbol{\\beta}) = \\min_{\\boldsymbol{\\beta}} \\left\\{ \\| \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|_2^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_2^2 \\right\\}. \\] where \\(\\lambda &gt; 0\\) is a tuning parameter that controls the amount of regularization: - When \\(\\lambda = 0\\): The problem reduces to ordinary least squares (OLS). - When \\(\\lambda \\to \\infty\\): The penalty dominates, forcing \\(\\boldsymbol{\\beta}\\) toward zero, shrinking coefficients. Expanding the loss function: \\[ \\mathcal{L}(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})&#39; (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}) + \\lambda \\boldsymbol{\\beta}&#39; \\boldsymbol{\\beta}. \\] Taking the derivative with respect to \\(\\boldsymbol{\\beta}\\) and setting it to zero: \\[ -2 \\mathbf{X}&#39; (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}) + 2 \\lambda \\boldsymbol{\\beta} = 0. \\] Rearranging: \\[ \\mathbf{X}&#39; \\mathbf{X} \\boldsymbol{\\beta} + \\lambda \\boldsymbol{\\beta} = \\mathbf{X}&#39; \\mathbf{y}. \\] Factoring out \\(\\boldsymbol{\\beta}\\): \\[ (\\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I}) \\boldsymbol{\\beta} = \\mathbf{X}&#39; \\mathbf{y}. \\] Since \\(\\mathbf{X}&#39;\\mathbf{X}\\) may be singular, adding \\(\\lambda \\mathbf{I}\\) ensures that the matrix \\((\\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I})\\) is always invertible for any \\(\\lambda &gt; 0\\). Thus, the Ridge Regression solution is \\[ \\boldsymbol{\\beta}_{\\text{ridge}} = (\\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\mathbf{X}&#39; \\mathbf{y}. \\] 4.1.2.4 Conclusion Matrix Regularization: The term \\(\\lambda \\mathbf{I}\\) ensures that \\(\\mathbf{X}&#39;\\mathbf{X} + \\lambda \\mathbf{I}\\) is always invertible because it shifts the eigenvalues of \\(\\mathbf{X}&#39;\\mathbf{X}\\) away from zero. Bias-Variance Tradeoff: Ridge Regression reduces variance at the cost of introducing some bias, which can improve prediction accuracy when \\(\\mathbf{X}\\) is ill-conditioned or when \\(p &gt; n\\). Shrinkage Effect: Larger \\(\\lambda\\) values shrink the coefficients towards zero, preventing overfitting. When \\(\\lambda = 0\\): The problem reduces to ordinary least squares (OLS). When \\(\\lambda \\to \\infty\\): The penalty dominates, forcing \\(\\boldsymbol{\\beta}\\) toward zero, shrinking coefficients. The only thing that is left is selecting the value of \\(\\lambda\\) 4.1.3 Lasso Regression Lasso regression (Least Absolute Shrinkage and Selection Operator) is a variation of linear regression that adds a penalty (like Ridge Regression) to the loss function to promote sparsity in the coefficients, effectively setting some of them to zero (unulike Ridge Regression). This makes Lasso a useful technique for feature selection, especially when we have many predictors, some of which may be irrelevant or highly correlated. 4.1.3.1 Lasso Regression as an Optimization Problem The Lasso regression formulation is: \\[ \\min_{\\boldsymbol{\\beta}} \\mathcal{L}(\\boldsymbol{\\beta}) = \\min_{\\boldsymbol{\\beta}} \\left\\{ \\| \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|_2^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_1 \\right\\}. \\] Where: - \\(\\|\\boldsymbol{\\beta}\\|_1 = \\sum_{i=1}^p |\\beta_i|\\) is the L1 norm of the coefficients (sum of absolute values of the coefficients), - \\(\\lambda \\geq 0\\) is the regularization parameter controlling the strength of the penalty. The loss function consists of: 1. Residual Sum of Squares (RSS): \\(\\| \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|_2^2\\), which measures the fit of the model (same as in ordinary least squares regression). 2. L1 Penalty: \\(\\lambda \\|\\boldsymbol{\\beta}\\|_1\\), which shrinks the coefficients towards zero and encourages sparsity (i.e., some coefficients are exactly zero). The objective is to minimize the sum of squared residuals along with a penalty term: \\[ \\min_{\\boldsymbol{\\beta}} \\left\\{ \\| \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|_2^2 + \\lambda \\sum_{i=1}^p |\\beta_i| \\right\\}. \\] The key feature of Lasso is that the L1 penalty promotes sparsity by shrinking some coefficients exactly to zero, which results in a simpler, more interpretable model. The parameter \\(\\lambda\\) controls the trade-off between fit and sparsity: - When \\(\\lambda = 0\\): Lasso reduces to ordinary least squares regression (OLS), where no penalty is applied. - When \\(\\lambda\\) is large: The penalty dominates, and more coefficients are shrunk to zero. Unfortunately, unlike Ridge Regression, Lasso has no closed form solution and it is necessary to find the solution numerically. 4.1.4 Elastic Net Elastic Net is a regularization technique that combines the strengths of Lasso and Ridge regression. While Lasso uses an L1 penalty and Ridge uses an L2 penalty, Elastic Net applies a mix of both penalties, giving a balance between sparsity and regularization strength. Elastic Net is particularly useful when there are highly correlated features or when the number of features is larger than the number of observations (\\(p &gt; n\\)). 4.1.5 Elastic Net as a Mixed Penalty The Elastic Net loss function is defined as: \\[ \\min_{\\boldsymbol{\\beta}} \\left\\{ \\| \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|_2^2 + \\lambda_1 \\|\\boldsymbol{\\beta}\\|_1 + \\frac{\\lambda_2}{2} \\|\\boldsymbol{\\beta}\\|_2^2 \\right\\}. \\] Where: - \\(\\mathbf{y}\\) is the \\(n \\times 1\\) vector of observed responses, - \\(\\mathbf{X}\\) is the \\(n \\times p\\) matrix of predictor variables, - \\(\\boldsymbol{\\beta}\\) is the \\(p \\times 1\\) vector of regression coefficients, - \\(\\|\\boldsymbol{\\beta}\\|_1 = \\sum_{i=1}^p |\\beta_i|\\) is the L1 norm (Lasso penalty), - \\(\\|\\boldsymbol{\\beta}\\|_2^2 = \\sum_{i=1}^p \\beta_i^2\\) is the L2 norm (Ridge penalty), - \\(\\lambda_1 \\geq 0\\) is the L1 regularization parameter (controlling the Lasso penalty), - \\(\\lambda_2 \\geq 0\\) is the L2 regularization parameter (controlling the Ridge penalty). L1 Penalty (Lasso term): \\(\\lambda_1 \\|\\boldsymbol{\\beta}\\|_1\\) encourages sparsity, meaning that it drives some coefficients to exactly zero. This is helpful for feature selection and reduces the complexity of the model. L2 Penalty (Ridge term): \\(\\frac{\\lambda_2}{2} \\|\\boldsymbol{\\beta}\\|_2^2\\) shrinks the coefficients toward zero without setting them exactly to zero. This helps with multicollinearity, preventing large fluctuations in the estimated coefficients when predictors are highly correlated. 4.1.5.1 Why Use Elastic Net? Correlation between predictors: When predictors are highly correlated, Lasso tends to select one variable and ignore the others. Elastic Net, by mixing L1 and L2 penalties, can help by including correlated variables in the model but still controlling their coefficients through the L2 penalty. Feature selection with many predictors: In cases where the number of features \\(p\\) is much greater than the number of observations \\(n\\), Lasso can become unstable. Elastic Net helps stabilize the model by adding a Ridge component, which shrinks the coefficients of less important features without forcing them to zero. The Elastic Net can be seen as a weighted sum of the Lasso and Ridge penalties, where: \\[ \\text{Elastic Net Loss} = \\lambda_1 \\|\\boldsymbol{\\beta}\\|_1 + \\frac{\\lambda_2}{2} \\|\\boldsymbol{\\beta}\\|_2^2. \\] You can think of \\(\\lambda_1\\) as controlling the strength of the Lasso penalty (feature selection), and \\(\\lambda_2\\) as controlling the strength of the Ridge penalty (shrinkage). The Elastic Net is useful when you need both sparsity (for feature selection) and regularization (to prevent overfitting). 4.1.5.2 Optimization Problem The Elastic Net optimization problem is: \\[ \\min_{\\boldsymbol{\\beta}} \\left\\{ \\| \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|_2^2 + \\lambda_1 \\|\\boldsymbol{\\beta}\\|_1 + \\frac{\\lambda_2}{2} \\|\\boldsymbol{\\beta}\\|_2^2 \\right\\}. \\] Objective function: The goal is to minimize the sum of squared residuals (RSS) plus the combined penalty terms. The optimal values of \\(\\lambda_1\\) and \\(\\lambda_2\\) are typically chosen via cross-validation. 4.1.5.3 Connections to Lasso and Ridge Regression When \\(\\lambda_2 = 0\\): Elastic Net becomes Lasso regression, as the Ridge term disappears and only the L1 penalty is applied. When \\(\\lambda_1 = 0\\): Elastic Net becomes Ridge regression, as the L1 penalty is removed and only the L2 penalty is applied. When both \\(\\lambda_1\\) and \\(\\lambda_2\\) are non-zero: Elastic Net is a combination of both regularization methods, providing a balanced approach. 4.1.6 Other Options of Regularization In addition to Elastic Net, Ridge, and Lasso, there are other regularization methods used in machine learning and statistical modeling: Group Lasso: Formula: \\[ \\min_{\\boldsymbol{\\beta}} \\left\\{ \\| \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|_2^2 + \\lambda \\sum_{g} \\|\\boldsymbol{\\beta}_g\\|_2 \\right\\}. \\] Penalty: Group Lasso is used when variables are grouped, and the penalty is applied at the group level. It forces entire groups of variables to be either included or excluded from the model. Fused Lasso: Formula: \\[ \\min_{\\boldsymbol{\\beta}} \\left\\{ \\| \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|_2^2 + \\lambda_1 \\|\\boldsymbol{\\beta}\\|_1 + \\lambda_2 \\sum_{i} |\\beta_i - \\beta_{i-1}| \\right\\}. \\] Penalty: Fused Lasso adds a penalty to the differences between adjacent coefficients, encouraging smoothness in the solution. This is useful in time series or spatial data where adjacent coefficients are expected to be similar. Bayesian Regularization: Formula: Bayesian regularization methods, like Bayesian Ridge Regression, assume a probabilistic model for the coefficients and add a prior distribution (often Gaussian) to the coefficients. The regularization comes from the prior’s influence on the model. Penalty: The prior serves as a regularizer, encouraging smaller coefficients with the Gaussian prior. 4.2 Bayesian Linear Regression Bayesian regression provides a probabilistic framework for regression analysis by incorporating prior knowledge about the parameters. It offers a direct connection to regularized regression, by introducing a prior on the regression coefficients. 4.2.1 Basic Bayesian Linear Regression As before, we consider the standard linear regression model: \\[ \\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{e}, \\quad \\mathbf{e} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}). \\] where: - \\(\\mathbf{y}\\) is the \\(n \\times 1\\) vector of observed responses, - \\(\\mathbf{X}\\) is the \\(n \\times p\\) matrix of predictor variables, - \\(\\boldsymbol{\\beta}\\) is the \\(p \\times 1\\) vector of regression coefficients, - \\(\\mathbf{e} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})\\) is the noise term, assumed to follow a normal distribution with variance \\(\\sigma^2\\). The likelihood function follows from the assumption that \\(\\mathbf{y}\\) is normally distributed given \\(\\mathbf{X}\\) and \\(\\boldsymbol{\\beta}\\): \\[ p(\\mathbf{y} | \\boldsymbol{\\beta}) = \\mathcal{N}(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}). \\] In a Bayesian framework, we assume a prior distribution for \\(\\boldsymbol{\\beta}\\). There are several (infinite) alternatives to set a prior, however in this case, we are going to work with a very basic model, in fact in a more gneral setting a prior distribution for \\(\\sigma^2\\) is usually specified. We take a normal prior with mean zero and covariance matrix \\(\\sigma^2 \\mathbf{\\Sigma}_\\beta\\), where \\(\\mathbf{\\Sigma}_\\beta\\) captures prior beliefs about the relationships between the coefficients: \\[ p(\\boldsymbol{\\beta}) = \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{\\Sigma}_\\beta). \\] where \\(\\mathbf{\\Sigma}_\\beta\\) is a positive definite \\(p \\times p\\) covariance matrix. 4.2.1.1 Posterior Distribution of \\(\\boldsymbol{\\beta}\\) Applying Bayes’ theorem, the posterior is proportional to the product of the likelihood and the prior: \\[ p(\\boldsymbol{\\beta} | \\mathbf{y}) \\propto p(\\mathbf{y} | \\boldsymbol{\\beta}) p(\\boldsymbol{\\beta}). \\] Since both the likelihood and prior are Gaussian, the posterior will also be Gaussian. To derive its mean and covariance, we complete the square in the exponent. \\[ p(\\mathbf{y} | \\boldsymbol{\\beta}) \\propto \\exp \\left( -\\frac{1}{2\\sigma^2} \\| \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|_2^2 \\right). \\] Expanding: \\[ \\| \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|_2^2 = (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})&#39; (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}). \\] \\[ p(\\boldsymbol{\\beta}) \\propto \\exp \\left( -\\frac{1}{2\\sigma^2} \\boldsymbol{\\beta}&#39; \\mathbf{\\Sigma}_\\beta^{-1} \\boldsymbol{\\beta} \\right). \\] The posterior distribution is proportional to: \\[ \\exp \\left( -\\frac{1}{2\\sigma^2} \\left[ (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})&#39; (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}) + \\boldsymbol{\\beta}&#39; \\mathbf{\\Sigma}_\\beta^{-1} \\boldsymbol{\\beta} \\right] \\right). \\] Expanding the quadratic term: \\[ \\mathbf{y}&#39; \\mathbf{y} - 2 \\mathbf{y}&#39; \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\beta}&#39; \\mathbf{X}&#39; \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\beta}&#39; \\mathbf{\\Sigma}_\\beta^{-1} \\boldsymbol{\\beta}. \\] Rewriting, \\[ - 2 \\mathbf{y}&#39; \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\beta}&#39; (\\mathbf{X}&#39; \\mathbf{X} + \\mathbf{\\Sigma}_\\beta^{-1}) \\boldsymbol{\\beta}. \\] Completing the square, we identify the posterior mean: \\[ \\boldsymbol{\\beta} | \\mathbf{y} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_\\beta, \\mathbf{\\Sigma}_\\beta^*), \\] where: \\[ \\mathbf{\\Sigma}_\\beta^* = \\left( \\mathbf{X}&#39; \\mathbf{X} + \\mathbf{\\Sigma}_\\beta^{-1} \\right)^{-1} \\sigma^2, \\] \\[ \\boldsymbol{\\mu}_\\beta = \\mathbf{\\Sigma}_\\beta^* \\mathbf{X}&#39; \\mathbf{y}. \\] 4.2.1.2 Connection to Ridge Regression If we assume that the prior covariance is a scaled identity matrix, i.e., \\[ \\mathbf{\\Sigma}_\\beta = \\frac{1}{\\lambda} \\mathbf{I}, \\] then its inverse is: \\[ \\mathbf{\\Sigma}_\\beta^{-1} = \\lambda \\mathbf{I}. \\] Substituting this into the posterior mean formula: \\[ \\boldsymbol{\\mu}_\\beta = \\left( \\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\mathbf{X}&#39; \\mathbf{y}. \\] which is exactly the Ridge estimator: \\[ \\hat{\\boldsymbol{\\beta}}_{\\text{ridge}} = \\left( \\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\mathbf{X}&#39; \\mathbf{y}. \\] Thus, we see that Bayesian regression with a normal prior on \\(\\boldsymbol{\\beta}\\) corresponds to Ridge regression, where the regularization parameter \\(\\lambda\\) is determined by the prior covariance. 4.2.1.3 Behavior of the Posterior Distribution as \\(\\lambda\\) Varies Since we have shown that the posterior mean of \\(\\boldsymbol{\\beta}\\) is: \\[ \\boldsymbol{\\mu}_\\beta = \\left( \\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\mathbf{X}&#39; \\mathbf{y}, \\] and the posterior covariance is: \\[ \\mathbf{\\Sigma}_\\beta^* = \\sigma^2 \\left( \\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1}, \\] we can analyze the behavior of the posterior distribution under extreme values of \\(\\lambda\\). When \\(\\lambda \\to 0\\) (No Regularization, Pure MLE) As \\(\\lambda \\to 0\\), the prior becomes uninformative, meaning we are not imposing any shrinkage on the coefficients. In this case: \\[ \\left( \\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\to (\\mathbf{X}&#39; \\mathbf{X})^{-1}, \\] assuming \\(\\mathbf{X}&#39; \\mathbf{X}\\) is invertible. Then, the posterior mean simplifies to the ordinary least squares (OLS) estimator: \\[ \\boldsymbol{\\mu}_\\beta \\to (\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{X}&#39; \\mathbf{y}. \\] Likewise, the posterior covariance reduces to: \\[ \\mathbf{\\Sigma}_\\beta^* \\to \\sigma^2 (\\mathbf{X}&#39; \\mathbf{X})^{-1}. \\] This shows that, when \\(\\lambda \\to 0\\), Bayesian regression becomes equivalent to the classical maximum likelihood estimate (MLE) from OLS, with high variance when \\(\\mathbf{X}&#39; \\mathbf{X}\\) is ill-conditioned. When \\(\\lambda \\to \\infty\\) (Strong Prior, Heavy Shrinkage) As \\(\\lambda \\to \\infty\\), the prior dominates and strongly shrinks \\(\\boldsymbol{\\beta}\\) toward zero. In this case: \\[ \\left( \\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\approx \\frac{1}{\\lambda} \\mathbf{I}, \\quad \\text{(for large \\( \\lambda \\))}. \\] Thus, the posterior mean behaves as: \\[ \\boldsymbol{\\mu}_\\beta \\approx \\frac{1}{\\lambda} \\mathbf{X}&#39; \\mathbf{y} \\to \\mathbf{0} \\quad \\text{as} \\quad \\lambda \\to \\infty. \\] Similarly, the posterior covariance reduces to: \\[ \\mathbf{\\Sigma}_\\beta^* \\approx \\frac{\\sigma^2}{\\lambda} \\mathbf{I} \\to \\mathbf{0}. \\] This means that, for very large \\(\\lambda\\), the posterior distribution becomes highly concentrated around zero: \\[ \\boldsymbol{\\beta} | \\mathbf{y} \\sim \\mathcal{N}(\\mathbf{0}, 0). \\] Interpretation: - As \\(\\lambda \\to \\infty\\), the prior overwhelms the data and forces all coefficients to shrink to zero. - The posterior variance also vanishes, meaning the uncertainty about \\(\\boldsymbol{\\beta}\\) disappears—everything is shrunk toward the prior mean (which is zero in this case). - This corresponds to extreme regularization, effectively setting all coefficients to zero, similar to a very strong Ridge penalty. Summary of \\(\\lambda\\)-Dependence: \\(\\lambda\\) Posterior Mean \\(\\boldsymbol{\\mu}_\\beta\\) Posterior Covariance \\(\\mathbf{\\Sigma}_\\beta^*\\) Interpretation \\(\\lambda \\to 0\\) OLS estimate: \\((\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{X}&#39; \\mathbf{y}\\) \\(\\sigma^2 (\\mathbf{X}&#39; \\mathbf{X})^{-1}\\) No regularization (pure MLE). Large variance if \\(\\mathbf{X}&#39; \\mathbf{X}\\) is ill-conditioned. Small \\(\\lambda\\) Close to OLS Slightly shrunk covariance Light regularization. Small shrinkage toward zero. Large \\(\\lambda\\) Strongly shrunk toward zero Shrunk covariance, but still adaptive to data Ridge-like regularization, balances data and prior. \\(\\lambda \\to \\infty\\) \\(\\mathbf{0}\\) \\(\\mathbf{0}\\) Extreme shrinkage; model ignores data and forces coefficients to zero. 4.2.1.4 Conclusion The Bayesian regression model presented here is a basic formulation that assumes a Gaussian likelihood and a normal prior on the regression coefficients. This simple setup already reveals deep connections to Ridge regression, demonstrating how prior beliefs influence parameter estimation through shrinkage. Also note that the Bayesian framework allows you to perform Linear Regression even in the case where \\(p &gt; n\\) without the need to change models, as long as a proper prior for \\( {\\boldsymbol \\beta} \\) is used. The model developed before, is just one possible Bayesian approach to regression. Many alternative priors can be used to encode different assumptions about the regression coefficients, leading to distinct forms of regularization: Laplace prior: Leads to Bayesian Lasso, which promotes sparsity by encouraging some coefficients to be exactly zero. Spike-and-slab prior: A mixture of a point mass at zero and a diffuse normal distribution, allowing for automatic feature selection. Horseshoe prior: A heavy-tailed prior that shrinks small coefficients strongly while allowing large ones to remain, making it useful for sparse models with some large effects. Gaussian Process priors: Used in nonparametric Bayesian regression, allowing for flexible modeling of relationships without assuming a fixed functional form. These richer prior choices allow Bayesian regression to adapt to a variety of settings, from high-dimensional problems to nonlinear relationships. Bayesian approaches also provide full posterior distributions, enabling uncertainty quantification in predictions—a key advantage over standard frequentist methods. 4.2.2 Bayesian Lasso Regression Given the standard regression model: \\[ \\mathbf{y} | \\boldsymbol{\\beta}, \\sigma^2 \\sim \\mathcal{N}(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}), \\] we place a Laplace prior on each coefficient \\(\\beta_j\\), scaled by \\(\\sigma\\), as follows: \\[ p(\\beta_j | \\sigma^2) = \\frac{\\lambda}{2\\sigma} \\exp \\left( - \\frac{\\lambda}{\\sigma} | \\beta_j | \\right). \\] This prior encourages sparsity, shrinking small coefficients toward zero while allowing some large ones. Unlike the basic Bayesian approach in the last section, the Bayesian Lasso does not have a closed form posterior distribution. However, it is easy to sample from following a hierarchical prior approach. 4.2.2.1 Hierarchical Representation of the Laplace Prior The Laplace prior can be rewritten as a hierarchical model using a Gaussian scale mixture representation. Specifically, we introduce auxiliary variance parameters \\(\\tau_j^2\\), where: \\[ \\beta_j | \\tau_j^2, \\sigma^2 \\sim \\mathcal{N}(0, \\sigma^2 \\tau_j^2). \\] The prior on \\(\\tau_j^2\\) follows an exponential distribution: \\[ p(\\tau_j^2 | \\lambda^2) = \\frac{\\lambda^2}{2} \\exp \\left( -\\frac{\\lambda^2}{2} \\tau_j^2 \\right). \\] Thus, the Bayesian Lasso can be interpreted as Bayesian ridge regression with an adaptive prior variance for each coefficient. 4.2.2.2 Posterior Distribution and MAP Estimator The posterior distribution of \\(\\boldsymbol{\\beta}\\) is given by: \\[ p(\\boldsymbol{\\beta} | \\mathbf{y}, \\sigma^2) \\propto p(\\mathbf{y} | \\boldsymbol{\\beta}, \\sigma^2) p(\\boldsymbol{\\beta} | \\sigma^2). \\] Since: - The likelihood is Gaussian: \\[ p(\\mathbf{y} | \\boldsymbol{\\beta}, \\sigma^2) \\propto \\exp \\left( - \\frac{1}{2\\sigma^2} \\|\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|_2^2 \\right), \\] - The prior is Laplace: \\[ p(\\boldsymbol{\\beta} | \\sigma^2) \\propto \\exp \\left( - \\frac{\\lambda}{2\\sigma} \\|\\boldsymbol{\\beta}\\|_1 \\right), \\] then the posterior mode (i.e., the Maximum A Posteriori (MAP) estimator) is obtained by solving: \\[ \\hat{\\boldsymbol{\\beta}} = \\arg \\min_{\\boldsymbol{\\beta}} \\left\\{ \\|\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|_2^2 + \\sigma \\lambda \\|\\boldsymbol{\\beta}\\|_1 \\right\\}. \\] This exactly recovers the traditional Lasso estimator, where the regularization term depends on \\(\\sigma \\lambda\\). Thus, the Bayesian Lasso provides a probabilistic justification for the Lasso estimator and explains how shrinkage is controlled by both \\(\\lambda\\) and \\(\\sigma^2\\). 4.2.2.3 Behavior of the Posterior as \\(\\lambda\\) and \\(\\sigma^2\\) Vary As \\(\\lambda \\to 0\\): The prior becomes uninformative, and the MAP estimate approaches the MLE (ordinary least squares). As \\(\\lambda \\to \\infty\\): The prior dominates the likelihood, and the posterior distribution becomes highly concentrated at zero (extreme sparsity). 4.3 Computational Comparisson We generate a synthetic dataset where only a subset of the predictors are relevant. We compare: Ridge regression. Lasso regression. Basic Bayesian Regression. Bayesian Lasso Regression. Horseshoe Prior Bayesian Regression. 4.3.1 Set-Up First we need some functions and libraries: library(glmnet) Loads the glmnet package, which is used for Lasso, Ridge, and Elastic Net regression. The glmnet package provides functions like cv.glmnet() and glmnet() to perform penalized regression with cross-validation. library(monomvn) Loads the monomvn package, which provides Bayesian regression models, including Bayesian Lasso and Bayesian Ridge. library(mvtnorm) Loads the mvtnorm package, which allows working with the multivariate normal distribution. Used for simulating correlated predictors in the design matrix (X) and for Bayesian sampling. source(&quot;./horseshoe_sampler.R&quot;) source(&quot;./fast_sampler.R&quot;) source(&quot;./fast_horseshoe.R&quot;) Loads external R scripts (horseshoe_sampler.R, fast_sampler.R, and fast_horseshoe.R), which likely contain Bayesian sampling functions for Horseshoe priors. These scripts implement Markov Chain Monte Carlo (MCMC) algorithms or other methods to generate posterior samples. set.seed(222025) Sets the random seed to ensure that results are reproducible. Ensures that simulated data and stochastic processes (e.g., cross-validation, Bayesian sampling) yield the same results every time the script is run. numTra &lt;- 300 # Training samples numSam &lt;- numTra * 2 # Total samples numVar &lt;- 500 # Number of predictors sizBlo &lt;- 10 # Block size for Correlation Matrix numNze &lt;- 10 # Number of nonzero coefficients sigNoi &lt;- 1 # Signal to Noise Ratio We have that: numTra: The number of samples available for training. numSam: The total number of samples in the dataset, including both training and testing data. It is set to twice the number of training samples. numVar: The number of predictor variables (features) in the dataset. sizBlo: The size of blocks in the correlation structure of the design matrix. This determines how many variables are within each block. numNze: The number of nonzero coefficients in the true regression model. These correspond to the features that actually influence the response variable. sigNoi: The signal-to-noise ratio, which controls the relative strength of the true signal (nonzero coefficients) compared to the noise in the observations. 4.3.2 Simulation # X, b and y simulation disCor &lt;- 0.5 # Correlation decay C &lt;- exp(- disCor * abs(rep(1, sizBlo) %*% t(1:sizBlo) - (1:sizBlo) %*% t(rep(1, sizBlo)))) X &lt;- matrix(NA, nrow = numSam, ncol = 0) for(i in 1:(numVar / sizBlo)){ X &lt;- cbind(X, rmvnorm(n = numSam, sigma = C)) } # Generates Coefficients and y coeDecRat &lt;- 4 b &lt;- rep(0, numVar) b[1:numNze] &lt;- exp(- (1:numNze) / coeDecRat) s2 &lt;- (mean(b[1:numNze]) / sigNoi)^2 y &lt;- X %*% b + rnorm(n = numSam, sd = sqrt(s2)) # Coefficient Values print(b[1:(2 * numNze)]) ## [1] 0.7788008 0.6065307 0.4723666 0.3678794 0.2865048 0.2231302 0.1737739 0.1353353 0.1053992 0.0820850 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 ## [17] 0.0000000 0.0000000 0.0000000 0.0000000 # Response Variable Variance print(var(y)) ## [,1] ## [1,] 4.276878 # Error Standard Deviation print(s2) ## [1] 0.1044457 This chunk of code generates the design matrix \\(\\mathbf{X}\\), regression coefficients \\(\\mathbf{b}\\), and response variable \\(\\mathbf{y}\\) for a simulated linear regression problem. It incorporates a correlated feature structure, meaning some predictors are related to each other. disCor &lt;- 0.5 # Correlation decay disCor controls how quickly correlations decay between predictors in the same block. A higher value means stronger correlations between nearby variables, while a lower value leads to weaker correlations. C &lt;- exp(- disCor * abs(rep(1, sizBlo) %*% t(1:sizBlo) - (1:sizBlo) %*% t(rep(1, sizBlo)))) C is a block correlation matrix of size sizBlo × sizBlo. The matrix is Toeplitz-like, meaning each predictor is correlated with its neighbors, and correlation decays exponentially as you move further away. The function exp(-disCor * distance) ensures that correlations are strongest within blocks and weaken with distance. X &lt;- matrix(NA, nrow = numSam, ncol = 0) for(i in 1:(numVar / sizBlo)){ X &lt;- cbind(X, rmvnorm(n = numSam, sigma = C)) } X is initialized as an empty matrix. The for-loop constructs \\(\\mathbf{X}\\) by generating blocks of correlated features using rmvnorm() (from mvtnorm package), which samples from a multivariate normal distribution with covariance matrix C. Each block of predictors has size sizBlo, and the total number of blocks is numVar / sizBlo. coeDecRat &lt;- 4 # Controls how fast coefficients decay b &lt;- rep(0, numVar) # Initializes all coefficients to zero b[1:numNze] &lt;- exp(-(1:numNze) / coeDecRat) coeDecRat controls the decay rate of the true regression coefficients. Only the first numNze coefficients are nonzero, and they follow an exponential decay pattern: \\[ b_j = \\exp(-j / \\text{coeDecRat}), \\quad \\text{for } j = 1, \\dots, \\text{numNze} \\] This means important variables have larger effects, and their influence decreases exponentially. sigNoi &lt;- mean(b[1:numNze]) / 2 sigNoi represents the signal-to-noise ratio: Higher sigNoi → stronger signal, less noise. Lower sigNoi → weaker signal, more noise. s2 &lt;- (mean(b[1:numNze]) / sigNoi)^2 s2 is the variance of the noise term. It ensures that the ratio of signal to noise remains controlled. y &lt;- X %*% b + rnorm(n = numSam, sd = sqrt(s2)) print(b[1:(2 * numNze)]) # Print the first few coefficients Displays the first 2 * numNze coefficients to confirm they follow the expected decay pattern and zero coefficients afterwards. print(var(y)) # Print the variance of the response variable Checks the total variance of \\(y\\), which is affected by both the signal and the noise. print(s2) # Print error variance Prints the variance of the noise to verify the expected level of randomness in the response variable. Variable Description disCor Decay parameter controlling correlation between predictors. C Correlation matrix defining block-wise correlated predictors. X Design matrix, with correlated predictor blocks. coeDecRat Decay rate for true coefficients, controlling sparsity. b True regression coefficients, only numNze are nonzero. s2 Noise variance, ensuring controlled randomness in \\(y\\). y Response variable, generated from \\(X\\) and \\(b\\) with noise. traInd &lt;- sample(1:numSam, numTra, replace = FALSE) traY &lt;- y[traInd] traX &lt;- X[traInd, ] tesY &lt;- y[-traInd] tesX &lt;- X[-traInd, ] This block splits the dataset into training and testing sets, which is crucial for evaluating model performance. Step-by-Step Explanation: traInd &lt;- sample(1:numSam, numTra, replace = FALSE) sample(1:numSam, numTra, replace = FALSE) Randomly selects numTra indices from 1:numSam (the full dataset). replace = FALSE ensures that no index is selected more than once, maintaining a random subset without duplication. traInd stores the indices of the training samples. traY &lt;- y[traInd] traX &lt;- X[traInd, ] traY: Subset of y containing only the selected training indices → training response values. traX: Corresponding rows from design matrix X → training predictor values. tesY &lt;- y[-traInd] tesX &lt;- X[-traInd, ] tesY: Subset of y excluding training indices → test response values. tesX: Corresponding rows from X excluding training indices → test predictor values. Summary of Variables: Variable Description traInd Indices randomly chosen for training. traX Training set predictors (subset of X). traY Training set response values (subset of y). tesX Test set predictors (remaining rows of X). tesY Test set response values (remaining rows of y). Why This Matters? 1. Prevents overfitting: The model is trained on traX, traY but evaluated on tesX, tesY, ensuring it generalizes to unseen data. 2. Mimics real-world scenarios: In practice, models predict new data points, so testing on unseen data measures true performance. 3. Ensures unbiased evaluation: A random split avoids bias in model evaluation, ensuring the test set represents different feature patterns. 4.3.3 OLS Then we first OLS outOLS &lt;- lm(traY ~ traX - 1) coeOLS &lt;- outOLS$coefficients coeOLS[is.na(coeOLS)] &lt;- 0 mseOLS &lt;- mean((tesX %*% coeOLS - tesY)^2) pr2OLS &lt;- 1 - mseOLS / var(tesY) This block fits an OLS regression model, extracts the estimated coefficients, and evaluates the model’s performance on the test set. outOLS &lt;- lm(traY ~ traX - 1) lm(traY ~ traX - 1): Fits a linear model where traY is the response variable and traX is the design matrix. The -1 removes the default intercept, ensuring that all coefficients correspond directly to the predictors in traX. outOLS stores the fitted model, including coefficients and residuals. coeOLS &lt;- outOLS$coefficients Retrieves the estimated regression coefficients from the fitted model. coeOLS[is.na(coeOLS)] &lt;- 0 Replaces any NA values with 0 (this can happen when certain predictors are collinear, leading to undefined coefficients). mseOLS &lt;- mean((tesX %*% coeOLS - tesY)^2) Computes the Mean Squared Error (MSE) on the test set: \\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} ( \\hat{y}_i - y_i )^2 \\] tesX %*% coeOLS calculates predicted values \\(\\hat{y}\\) on the test data. pr2OLS &lt;- 1 - mseOLS / var(tesY) Computes the predictive R-squared \\(R^2\\): \\[ R^2 = 1 - \\frac{\\text{MSE}}{\\text{Var}(\\text{Test } y)} \\] Interpretation: \\(R^2 \\approx 1\\) → Model explains most of the variance in tesY (good performance). \\(R^2 \\approx 0\\) → Model performs as poorly as simply predicting the mean. \\(R^2 &lt; 0\\) → Model performs worse than a constant predictor (severe overfitting or poor generalization). Summary of Key Variables: Variable Description outOLS OLS model fitted to the training data. coeOLS Estimated coefficients from the OLS model. mseOLS Mean Squared Error on the test set (lower is better). pr2OLS Predictive \\(R^2\\), measuring how well the model generalizes to new data. Why This Matters? OLS assumes no multicollinearity → Since traX contains correlated predictors, OLS may suffer from unstable coefficient estimates. Regularization (e.g., Ridge, Lasso) is often needed to improve performance when predictors are highly correlated. Comparison to Ridge/Lasso/Bayesian models will highlight how regularization techniques handle multicollinearity better than OLS. 4.3.4 Ridge Regression cvRID &lt;- cv.glmnet(x = traX, y = traY, alpha = 0) outRID &lt;- glmnet(x = traX, y = traY, alpha = 0, lambda = cvRID$lambda.min) preRID &lt;- predict(outRID, newx = tesX) mseRID &lt;- mean((preRID - tesY)^2) pr2RID &lt;- 1 - mseRID / var(tesY) This block fits a Ridge Regression model, selects an optimal regularization parameter \\(\\lambda\\) via cross-validation, and evaluates the model’s performance on the test set. cvRID &lt;- cv.glmnet(x = traX, y = traY, alpha = 0) cv.glmnet() performs cross-validation to find the best \\(\\lambda\\) (regularization strength). alpha = 0 specifies Ridge Regression (if alpha = 1, it would perform Lasso instead). The function: Splits the training data into folds. Trains Ridge regression models with different \\(\\lambda\\) values. Selects the best \\(\\lambda\\) by minimizing cross-validated MSE. outRID &lt;- glmnet(x = traX, y = traY, alpha = 0, lambda = cvRID$lambda.min) Fits a Ridge Regression model using the best \\(\\lambda\\) found from cv.glmnet(). Key arguments: x = traX, y = traY: Training data. alpha = 0: Specifies Ridge Regression. lambda = cvRID$lambda.min: Uses the optimal \\(\\lambda\\) from cross-validation. Effect of Ridge Regularization: Shrinks all coefficients (unlike Lasso, which can set some to zero). Reduces variance by stabilizing estimates, especially in highly correlated predictor settings. preRID &lt;- predict(outRID, newx = tesX) Uses the trained Ridge model (outRID) to predict response values on the test set (tesX). mseRID &lt;- mean((preRID - tesY)^2) Computes the Mean Squared Error (MSE) on the test data: \\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} ( \\hat{y}_i - y_i )^2 \\] Lower MSE indicates better predictive performance. pr2RID &lt;- 1 - mseRID / var(tesY) Computes the predictive \\(R^2\\): \\[ R^2 = 1 - \\frac{\\text{MSE}}{\\text{Var}(\\text{Test } y)} \\] Interpretation: \\(R^2 \\approx 1\\) → Model explains most of the variance (good performance). \\(R^2 \\approx 0\\) → Model performs similarly to a constant predictor. \\(R^2 &lt; 0\\) → Model performs worse than predicting the mean (poor generalization). Summary of Key Variables: Variable Description cvRID Performs cross-validation to select optimal \\(\\lambda\\). outRID Trained Ridge Regression model with best \\(\\lambda\\). preRID Predicted values for the test set. mseRID Mean Squared Error on the test set (lower is better). pr2RID Predictive \\(R^2\\), measuring model generalization. Why Use Ridge Regression? Handles multicollinearity: When predictors are highly correlated, OLS estimates become unstable. Ridge stabilizes them. Reduces overfitting: Adding a penalty term \\(\\lambda \\|\\boldsymbol{\\beta}\\|_2^2\\) discourages overly large coefficients. Retains all features: Unlike Lasso, Ridge does not force coefficients to be exactly zero. 4.3.5 Lasso cvLAS &lt;- cv.glmnet(x = traX, y = traY, alpha = 1) plot(cvLAS) plot(cvLAS$glmnet.fit) outLAS &lt;- glmnet(x = traX, y = traY, alpha = 1, lambda = cvLAS$lambda.min) preLAS &lt;- predict(outLAS, newx = tesX) mseLAS &lt;- mean((preLAS - tesY)^2) pr2LAS &lt;- 1 - mseLAS / var(tesY) This block fits a Lasso Regression model, selects an optimal regularization parameter \\(\\lambda\\) via cross-validation, and evaluates the model’s performance on the test set. cvLAS &lt;- cv.glmnet(x = traX, y = traY, alpha = 1) cv.glmnet() performs cross-validation to find the best \\(\\lambda\\) (regularization strength). alpha = 1 specifies Lasso Regression (if alpha = 0, it would perform Ridge Regression). The function: Splits the training data into folds. Trains Lasso models with different \\(\\lambda\\) values. Selects the best \\(\\lambda\\) by minimizing cross-validated MSE. plot(cvLAS) plot(cvLAS$glmnet.fit) plot(cvLAS): Plots the cross-validation error for different values of \\(\\lambda\\). Shows the selected \\(\\lambda\\) (the one that minimizes error). plot(cvLAS$glmnet.fit): Plots the Lasso path: how coefficients change as \\(\\lambda\\) increases. Higher \\(\\lambda\\) values shrink more coefficients to zero, performing feature selection. outLAS &lt;- glmnet(x = traX, y = traY, alpha = 1, lambda = cvLAS$lambda.min) Fits a Lasso Regression model using the best \\(\\lambda\\) found from cv.glmnet(). Key arguments: x = traX, y = traY: Training data. alpha = 1: Specifies Lasso Regression. lambda = cvLAS$lambda.min: Uses the optimal \\(\\lambda\\) from cross-validation. Effect of Lasso Regularization: Shrinks some coefficients to exactly zero, performing automatic feature selection. Helps when only a subset of predictors is relevant. preLAS &lt;- predict(outLAS, newx = tesX) Uses the trained Lasso model (outLAS) to predict response values on the test set (tesX). mseLAS &lt;- mean((preLAS - tesY)^2) Computes the Mean Squared Error (MSE) on the test data: \\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} ( \\hat{y}_i - y_i )^2 \\] Lower MSE indicates better predictive performance. pr2LAS &lt;- 1 - mseLAS / var(tesY) Computes the predictive \\(R^2\\): \\[ R^2 = 1 - \\frac{\\text{MSE}}{\\text{Var}(\\text{Test } y)} \\] Interpretation: \\(R^2 \\approx 1\\) → Model explains most of the variance (good performance). \\(R^2 \\approx 0\\) → Model performs similarly to a constant predictor. \\(R^2 &lt; 0\\) → Model performs worse than predicting the mean (poor generalization). Summary of Key Variables: Variable Description cvLAS Performs cross-validation to select optimal \\(\\lambda\\). outLAS Trained Lasso Regression model with best \\(\\lambda\\). preLAS Predicted values for the test set. mseLAS Mean Squared Error on the test set (lower is better). pr2LAS Predictive \\(R^2\\), measuring model generalization. Why Use Lasso Regression? Feature Selection: Unlike Ridge, Lasso can shrink some coefficients exactly to zero, removing irrelevant predictors. Handles high-dimensional data: When \\(p &gt; n\\), Lasso helps by selecting the most relevant features. Interpretable models: Because some coefficients are set to zero, Lasso produces simpler models. 4.3.6 Basic Bayesian Regression outBAY &lt;- rmvnorm(n = 2000, mean = outRID$beta, sigma = s2 * solve(t(X) %*% X + cvRID$lambda.min * diag(numVar))) coeBAY &lt;- apply(X = outBAY, MARGIN = 2, FUN = median) preBAY &lt;- tesX %*% coeBAY mseBAY &lt;- mean((preBAY - tesY)^2) pr2BAY &lt;- 1 - mseBAY / var(tesY) outBAY &lt;- rmvnorm(n = 2000, mean = outRID$beta, sigma = s2 * solve(t(X) %*% X + cvRID$lambda.min * diag(numVar))) rmvnorm() generates 2,000 samples from a multivariate normal distribution, approximating the posterior distribution of the regression coefficients \\(\\boldsymbol{\\beta}\\). Posterior Mean: Uses the Ridge Regression solution (outRID$beta) as the mean of the distribution. Posterior Covariance: \\[ \\sigma^2 (\\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\] s2 * solve(t(X) %*% X + cvRID$lambda.min * diag(numVar)) is the posterior covariance matrix, incorporating both the observed data and the prior information. The \\(\\lambda \\mathbf{I}\\) term (from Ridge) ensures stability even if \\(\\mathbf{X}&#39; \\mathbf{X}\\) is singular.  Key Idea: Unlike Ridge, which gives a point estimate for \\(\\boldsymbol{\\beta}\\), Bayesian Ridge treats \\(\\boldsymbol{\\beta}\\) as a random variable and samples from its posterior distribution. coeBAY &lt;- apply(X = outBAY, MARGIN = 2, FUN = median) For each coefficient \\(\\beta_j\\), we take the median across the 2,000 posterior samples. This is a Bayesian point estimate, similar to the Ridge solution but incorporating posterior uncertainty. preBAY &lt;- tesX %*% coeBAY Uses the Bayesian posterior median coefficients to predict tesY. mseBAY &lt;- mean((preBAY - tesY)^2) Computes the Mean Squared Error (MSE) on the test data: \\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} ( \\hat{y}_i - y_i )^2 \\] Lower MSE indicates better predictive performance. pr2BAY &lt;- 1 - mseBAY / var(tesY) Computes the predictive \\(R^2\\): \\[ R^2 = 1 - \\frac{\\text{MSE}}{\\text{Var}(\\text{Test } y)} \\] Interpretation: \\(R^2 \\approx 1\\) → Model explains most of the variance (good performance). \\(R^2 \\approx 0\\) → Model performs similarly to a constant predictor. \\(R^2 &lt; 0\\) → Model performs worse than predicting the mean (poor generalization). Summary of Key Variables: Variable Description outBAY Posterior samples of \\(\\boldsymbol{\\beta}\\) from a Bayesian Ridge model. coeBAY Posterior median coefficients (Bayesian point estimate). preBAY Predicted values for the test set. mseBAY Mean Squared Error on the test set (lower is better). pr2BAY Predictive \\(R^2\\), measuring model generalization. Why Use Bayesian Ridge Regression? Incorporates uncertainty: Instead of a single estimate, we get a posterior distribution over \\(\\boldsymbol{\\beta}\\). Handles multicollinearity: Like Ridge, the prior shrinks coefficients to prevent instability. Flexibility: Bayesian methods allow incorporating informative priors when prior knowledge is available. 4.3.7 Bayesian Lasso outBLA &lt;- blasso(X = traX, y = traY, T = 500) ## t=100, m=215 ## t=200, m=209 ## t=300, m=226 ## t=400, m=203 coeBLA &lt;- apply(X = outBLA$beta, MARGIN = 2, FUN = median) preBLA &lt;- tesX %*% coeBLA mseBLA &lt;- mean((preBLA - tesY)^2) pr2BLA &lt;- 1 - mseBLA / var(tesY) This block implements a Bayesian version of Lasso Regression, where instead of solving a convex optimization problem (like traditional Lasso), it samples from the posterior distribution of the regression coefficients using a Bayesian framework. outBLA &lt;- blasso(X = traX, y = traY, T = 50) blasso() performs Bayesian Lasso regression, which introduces a Laplace (double-exponential) prior on \\(\\boldsymbol{\\beta}\\). Key argument: T = 50: Runs 50 iterations of a Markov Chain Monte Carlo (MCMC) sampler to draw posterior samples of \\(\\boldsymbol{\\beta}\\).  Bayesian Lasso vs. Traditional Lasso - Traditional Lasso: Estimates \\(\\boldsymbol{\\beta}\\) by solving an optimization problem with an \\(\\ell_1\\)-penalty. - Bayesian Lasso: Places a Laplace prior on \\(\\boldsymbol{\\beta}\\), then samples from the posterior. - Unlike Traditional Lasso, the Bayesian Lasso doesn’t perform exact variable selection. coeBLA &lt;- apply(X = outBLA$beta, MARGIN = 2, FUN = median) Takes the median of the sampled coefficients across the 50 iterations. This provides a point estimate similar to the Lasso solution but incorporates posterior uncertainty. preBLA &lt;- tesX %*% coeBLA Uses the posterior median coefficients to predict response values on the test set. mseBLA &lt;- mean((preBLA - tesY)^2) Computes the Mean Squared Error (MSE) on the test data: \\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} ( \\hat{y}_i - y_i )^2 \\] Lower MSE indicates better predictive performance. pr2BLA &lt;- 1 - mseBLA / var(tesY) Computes the predictive \\(R^2\\): \\[ R^2 = 1 - \\frac{\\text{MSE}}{\\text{Var}(\\text{Test } y)} \\] Interpretation: \\(R^2 \\approx 1\\) → Model explains most of the variance (good performance). \\(R^2 \\approx 0\\) → Model performs similarly to a constant predictor. \\(R^2 &lt; 0\\) → Model performs worse than predicting the mean (poor generalization). Summary of Key Variables: Variable Description outBLA Posterior samples of \\(\\boldsymbol{\\beta}\\) from the Bayesian Lasso model. coeBLA Posterior median coefficients (Bayesian point estimate). preBLA Predicted values for the test set. mseBLA Mean Squared Error on the test set (lower is better). pr2BLA Predictive \\(R^2\\), measuring model generalization. Why Use Bayesian Lasso Regression? Incorporates Uncertainty: Unlike standard Lasso, it provides posterior distributions instead of just point estimates. More Robust to Noise: Bayesian priors smooth out overfitting issues that can occur in traditional Lasso. 4.3.8 Bayesian Horseshoe Prior outHOR &lt;- horseshoe_sampler(X = X, y = y, S = 500) ## | | | 0% | | | 1% | |= | 1% | |= | 2% | |== | 2% | |== | 3% | |=== | 4% | |=== | 5% | |==== | 5% | |==== | 6% | |===== | 6% | |===== | 7% | |===== | 8% | |====== | 8% | |====== | 9% | |======= | 9% | |======= | 10% | |======== | 11% | |======== | 12% | |========= | 12% | |========= | 13% | |========== | 13% | |========== | 14% | |=========== | 15% | |=========== | 16% | |============ | 16% | |============ | 17% | |============= | 17% | |============= | 18% | |============= | 19% | |============== | 19% | |============== | 20% | |=============== | 20% | |=============== | 21% | |================ | 22% | |================ | 23% | |================= | 23% | |================= | 24% | |================== | 24% | |================== | 25% | |================== | 26% | |=================== | 26% | |=================== | 27% | |==================== | 27% | |==================== | 28% | |===================== | 29% | |===================== | 30% | |====================== | 30% | |====================== | 31% | |======================= | 31% | |======================= | 32% | |======================= | 33% | |======================== | 33% | |======================== | 34% | |========================= | 34% | |========================= | 35% | |========================== | 36% | |========================== | 37% | |=========================== | 37% | |=========================== | 38% | |============================ | 38% | |============================ | 39% | |============================= | 40% | |============================= | 41% | |============================== | 41% | |============================== | 42% | |=============================== | 42% | |=============================== | 43% | |=============================== | 44% | |================================ | 44% | |================================ | 45% | |================================= | 45% | |================================= | 46% | |================================== | 47% | |================================== | 48% | |=================================== | 48% | |=================================== | 49% | |==================================== | 49% | |==================================== | 50% | |==================================== | 51% | |===================================== | 51% | |===================================== | 52% | |====================================== | 52% | |====================================== | 53% | |======================================= | 54% | |======================================= | 55% | |======================================== | 55% | |======================================== | 56% | |========================================= | 56% | |========================================= | 57% | |========================================= | 58% | |========================================== | 58% | |========================================== | 59% | |=========================================== | 59% | |=========================================== | 60% | |============================================ | 61% | |============================================ | 62% | |============================================= | 62% | |============================================= | 63% | |============================================== | 63% | |============================================== | 64% | |=============================================== | 65% | |=============================================== | 66% | |================================================ | 66% | |================================================ | 67% | |================================================= | 67% | |================================================= | 68% | |================================================= | 69% | |================================================== | 69% | |================================================== | 70% | |=================================================== | 70% | |=================================================== | 71% | |==================================================== | 72% | |==================================================== | 73% | |===================================================== | 73% | |===================================================== | 74% | |====================================================== | 74% | |====================================================== | 75% | |====================================================== | 76% | |======================================================= | 76% | |======================================================= | 77% | |======================================================== | 77% | |======================================================== | 78% | |========================================================= | 79% | |========================================================= | 80% | |========================================================== | 80% | |========================================================== | 81% | |=========================================================== | 81% | |=========================================================== | 82% | |=========================================================== | 83% | |============================================================ | 83% | |============================================================ | 84% | |============================================================= | 84% | |============================================================= | 85% | |============================================================== | 86% | |============================================================== | 87% | |=============================================================== | 87% | |=============================================================== | 88% | |================================================================ | 88% | |================================================================ | 89% | |================================================================= | 90% | |================================================================= | 91% | |================================================================== | 91% | |================================================================== | 92% | |=================================================================== | 92% | |=================================================================== | 93% | |=================================================================== | 94% | |==================================================================== | 94% | |==================================================================== | 95% | |===================================================================== | 95% | |===================================================================== | 96% | |====================================================================== | 97% | |====================================================================== | 98% | |======================================================================= | 98% | |======================================================================= | 99% | |========================================================================| 99% | |========================================================================| 100% coeHOR &lt;- apply(X = outHOR$B, MARGIN = 2, FUN = median) preHOR &lt;- tesX %*% coeHOR mseHOR &lt;- mean((preHOR - tesY)^2) pr2HOR &lt;- 1 - mseHOR / var(tesY) This block implements Bayesian regression using the Horseshoe prior, which is particularly useful for sparse models where only a small subset of predictors are truly relevant. The Horseshoe prior is known for strong shrinkage of irrelevant coefficients while allowing large signals to remain unshrunk. In this case custom code is used to perform faster sampling when \\(p &gt; n\\). outHOR &lt;- horseshoe_sampler(X = X, y = y, S = 500) horseshoe_sampler() performs Bayesian regression with a Horseshoe prior, which is an adaptive shrinkage prior. Key argument: S = 500: Runs 500 MCMC iterations to generate posterior samples of the coefficients \\(\\boldsymbol{\\beta}\\).  Horseshoe Prior vs. Other Bayesian Methods - Bayesian Ridge: Uses a Gaussian prior, shrinking all coefficients uniformly. - Bayesian Lasso: Uses a Laplace prior, putting more weight at zero in the prior. - Horseshoe Prior: Uses a hierarchical prior that strongly shrinks small coefficients but allows large coefficients to stay large. coeHOR &lt;- apply(X = outHOR$B, MARGIN = 2, FUN = median) Takes the median of the posterior samples for each coefficient. This provides a point estimate similar to Bayesian Lasso but with adaptive shrinkage. preHOR &lt;- tesX %*% coeHOR Uses the posterior median coefficients to predict response values on the test set. mseHOR &lt;- mean((preHOR - tesY)^2) Computes the Mean Squared Error (MSE) on the test data: \\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} ( \\hat{y}_i - y_i )^2 \\] Lower MSE indicates better predictive performance. pr2HOR &lt;- 1 - mseHOR / var(tesY) Computes the predictive \\(R^2\\): \\[ R^2 = 1 - \\frac{\\text{MSE}}{\\text{Var}(\\text{Test } y)} \\] Interpretation: \\(R^2 \\approx 1\\) → Model explains most of the variance (good performance). \\(R^2 \\approx 0\\) → Model performs similarly to a constant predictor. \\(R^2 &lt; 0\\) → Model performs worse than predicting the mean (poor generalization). Summary of Key Variables: Variable Description outHOR Posterior samples of \\(\\boldsymbol{\\beta}\\) from the Horseshoe prior model. coeHOR Posterior median coefficients (Bayesian point estimate). preHOR Predicted values for the test set. mseHOR Mean Squared Error on the test set (lower is better). pr2HOR Predictive \\(R^2\\), measuring model generalization. Why Use the Horseshoe Prior? Handles Sparse Models Well: Encourages strong shrinkage for irrelevant coefficients while keeping relevant ones intact. Better than Lasso for Small Signals: Lasso tends to shrink all coefficients, while Horseshoe allows some to stay large. Works in High-Dimensional Settings: Performs well when \\(p &gt; n\\), where traditional methods like OLS and Ridge struggle. 4.3.9 Results Comparisson 4.3.9.1 Estimation Comparisson # Plots Both coeInd &lt;- 1 denBAY &lt;- density(x = outBAY[, coeInd]) denBLA &lt;- density(x = outBLA$beta[101:500, coeInd]) denHOR &lt;- density(x = outHOR$B[101:500, coeInd]) xmin &lt;- min(b[coeInd], outRID$beta[coeInd], outLAS$beta[coeInd], outBAY[, coeInd], outBLA$beta[101:500, coeInd], outHOR$B[101:500, coeInd]) xmax &lt;- max(b[coeInd], outRID$beta[coeInd], outLAS$beta[coeInd], outBAY[, coeInd], outBLA$beta[101:500, coeInd], outHOR$B[101:500, coeInd]) ymax &lt;- max(denBAY$y, denBLA$y, denHOR$y) plot(denBAY, col = rgb(1, 0, 0), xlim = c(xmin, xmax), ylim = c( 0, ymax), main = paste0(&quot;Coefficient &quot;, coeInd), xlab = bquote(beta[.(coeInd)])) polygon(denBAY, col = rgb(1, 0, 0, 0.5), border = NA) par(new=TRUE) plot(denBLA, col = rgb(1, 0, 1), xlim = c(xmin, xmax), ylim = c( 0, ymax), main = &quot;&quot;, xaxt = &quot;n&quot;, xlab = &quot;&quot;) polygon(denBLA, col = rgb(1, 0, 1, 0.5), border = NA) par(new=TRUE) plot(denHOR, col = rgb(0, 1, 1), xlim = c(xmin, xmax), ylim = c( 0, ymax), main = &quot;&quot;, xaxt = &quot;n&quot;, xlab = &quot;&quot;) polygon(denHOR, col = rgb(0, 1, 1, 0.5), border = NA) abline(v = b[coeInd], lwd = 2, col = rgb(0, 0, 0)) abline(v = outRID$beta[coeInd], lwd = 2, col = rgb(0, 0, 1)) abline(v = outLAS$beta[coeInd], lwd = 2, col = rgb(0, 1, 0)) This code visualizes the posterior distributions of a single regression coefficient \\(\\beta_{\\text{coeInd}}\\) estimated using Bayesian Ridge, Bayesian Lasso, and Bayesian Horseshoe priors. Additionally, it compares these estimates to those from Ridge and Lasso regression. Why This Matters? Ridge (blue line) shrinks coefficients but never sets them exactly to zero. Lasso (green line) performs feature selection by setting some coefficients exactly to zero. Bayesian Ridge (red curve) resembles Ridge but accounts for uncertainty. Bayesian Lasso (purple curve) is similar to Lasso but includes a distribution over coefficients. Horseshoe (cyan curve) applies strong shrinkage to small coefficients while allowing important ones to remain large. coeInd &lt;- 1 coeInd = 1 selects the first coefficient (\\(\\beta_1\\)) for visualization. The user can change this to plot other coefficients. denBAY &lt;- density(x = outBAY[, coeInd]) denBLA &lt;- density(x = outBLA$beta[101:500, coeInd]) denHOR &lt;- density(x = outHOR$B[101:500, coeInd]) density() estimates the probability density function (PDF) of the sampled coefficients. Bayesian Models: outBAY[, coeInd]: Posterior samples from Bayesian Ridge Regression. outBLA$beta[101:500, coeInd]: Posterior samples from Bayesian Lasso (excluding first 100 as burn-in). outHOR$B[101:500, coeInd]: Posterior samples from Bayesian Horseshoe (excluding first 100 as burn-in).  Why exclude the first 100 samples? - In MCMC sampling, the first few iterations (burn-in period) may not be from the true posterior distribution. xmin &lt;- min(b[coeInd], outRID$beta[coeInd], outLAS$beta[coeInd], outBAY[, coeInd], outBLA$beta[101:500, coeInd], outHOR$B[101:500, coeInd]) xmax &lt;- max(b[coeInd], outRID$beta[coeInd], outLAS$beta[coeInd], outBAY[, coeInd], outBLA$beta[101:500, coeInd], outHOR$B[101:500, coeInd]) ymax &lt;- max(denBAY$y, denBLA$y, denHOR$y) xmin &amp; xmax: Find the minimum and maximum values of the estimated coefficients to set the x-axis range. ymax: Finds the maximum density value across all models to set the y-axis range. plot(denBAY, col = rgb(1, 0, 0), xlim = c(xmin, xmax), ylim = c(0, ymax), main = paste0(&quot;Coefficient &quot;, coeInd), xlab = bquote(beta[.(coeInd)])) polygon(denBAY, col = rgb(1, 0, 0, 0.5), border = NA) Plots the density of Bayesian Ridge estimates (in red). polygon() fills the area under the curve for better visualization. par(new=TRUE) plot(denBLA, col = rgb(1, 0, 1), xlim = c(xmin, xmax), ylim = c(0, ymax), main = &quot;&quot;, xaxt = &quot;n&quot;, xlab = &quot;&quot;) polygon(denBLA, col = rgb(1, 0, 1, 0.5), border = NA) Plots Bayesian Lasso estimates (in purple). par(new=TRUE) allows overlaying this plot on the previous one. par(new=TRUE) plot(denHOR, col = rgb(0, 1, 1), xlim = c(xmin, xmax), ylim = c(0, ymax), main = &quot;&quot;, xaxt = &quot;n&quot;, xlab = &quot;&quot;) polygon(denHOR, col = rgb(0, 1, 1, 0.5), border = NA) Plots Bayesian Horseshoe estimates (in cyan/light blue). The densities overlap, allowing a direct comparison of uncertainty across models. abline(v = b[coeInd], lwd = 2, col = rgb(0, 0, 0)) # True coefficient (black) abline(v = outRID$beta[coeInd], lwd = 2, col = rgb(0, 0, 1)) # Ridge estimate (blue) abline(v = outLAS$beta[coeInd], lwd = 2, col = rgb(0, 1, 0)) # Lasso estimate (green) Black line → True coefficient value. Blue line → Ridge estimate. Green line → Lasso estimate.  Why are Ridge and Lasso shown separately? - Unlike Bayesian methods, Ridge and Lasso provide point estimates instead of full posterior distributions. 4.3.9.2 Performance Metrics # Create a table with rounded values coef_table &lt;- round(cbind( True_Beta = b, OLS = outOLS$coefficients, Ridge = as.numeric(outRID$beta), Lasso = as.numeric(outLAS$beta), B_Ridge = coeBAY, B_Lasso = coeBLA, Horseshoe = coeHOR )[1:15, ], 4) rownames(coef_table) &lt;- paste0(&quot;Coefficient &quot;, 1:15) # Print as a formatted table kable(as.matrix(coef_table), caption = &quot;Comparison of Coefficient Estimates Across Methods&quot;) Table 4.1: Comparison of Coefficient Estimates Across Methods True_Beta OLS Ridge Lasso B_Ridge B_Lasso Horseshoe Coefficient 1 0.7788 1.0284 0.1298 0.7248 0.1291 0.7485 0.7657 Coefficient 2 0.6065 0.8395 0.1416 0.6148 0.1407 0.6366 0.6066 Coefficient 3 0.4724 1.2123 0.1398 0.4795 0.1414 0.4802 0.4812 Coefficient 4 0.3679 -0.0911 0.1192 0.3627 0.1187 0.3661 0.3635 Coefficient 5 0.2865 -0.5125 0.0986 0.2963 0.0993 0.2965 0.2957 Coefficient 6 0.2231 1.1521 0.0778 0.2083 0.0782 0.2129 0.2056 Coefficient 7 0.1738 -0.8806 0.0567 0.1360 0.0564 0.1474 0.1661 Coefficient 8 0.1353 -0.0308 0.0382 0.1173 0.0385 0.1154 0.1175 Coefficient 9 0.1054 0.7145 0.0288 0.0585 0.0285 0.0616 0.0970 Coefficient 10 0.0821 0.3688 0.0261 0.0858 0.0280 0.1142 0.0829 Coefficient 11 0.0000 -0.1746 -0.0186 0.0000 -0.0176 0.0000 -0.0001 Coefficient 12 0.0000 0.5834 -0.0202 0.0000 -0.0208 0.0000 -0.0001 Coefficient 13 0.0000 -0.4630 -0.0106 0.0000 -0.0114 0.0057 0.0000 Coefficient 14 0.0000 -0.3090 -0.0019 0.0000 -0.0022 0.0000 0.0000 Coefficient 15 0.0000 0.0410 -0.0051 0.0000 -0.0045 0.0000 0.0000 Observations: - OLS coefficients tend to be large and unstable due to multicollinearity. - Ridge shrinks all coefficients but keeps them nonzero. - Lasso performs feature selection, setting some coefficients exactly to zero. - Bayesian methods provide uncertainty-aware estimates and adaptive shrinkage. - Horseshoe applies strong shrinkage to irrelevant coefficients while keeping relevant ones unshrunk. Would you like to include a heatmap visualization of these coefficient estimates?  4.3.9.3 Predictive Performance # Create a performance table performance_table &lt;- data.frame( Method = c(&quot;OLS&quot;, &quot;Ridge&quot;, &quot;Lasso&quot;, &quot;Bayesian Ridge&quot;, &quot;Bayesian Lasso&quot;, &quot;Horseshoe&quot;), MSE = round(c(mseOLS, mseRID, mseLAS, mseBAY, mseBLA, mseHOR), 4), Predictive_R2 = round(c(pr2OLS, pr2RID, pr2LAS, pr2BAY, pr2BLA, pr2HOR), 4) ) # Print as a formatted table kable(performance_table, caption = &quot;Model Comparison: MSE and Predictive R²&quot;) Table 4.2: Model Comparison: MSE and Predictive R² Method MSE Predictive_R2 OLS 35.7967 -8.0661 Ridge 2.3529 0.4041 Lasso 0.1065 0.9730 Bayesian Ridge 2.3150 0.4137 Bayesian Lasso 0.1162 0.9706 Horseshoe 0.0939 0.9762 Interpretation Lower MSE → Better predictive performance. Higher \\(R^2\\) → Model explains more variance in the test set. OLS likely performs the worst due to overfitting and instability. Ridge improves stability but retains all predictors. Lasso performs feature selection, possibly improving interpretability. Bayesian methods incorporate uncertainty and adaptive shrinkage. Horseshoe is expected to perform well in sparse settings. 4.3.9.4 Variable Selection Post-Processing # No post-processing necessary estBLA &lt;- apply(X = outBLA$beta, MARGIN = 2, FUN = median) estLAS &lt;- as.numeric(outLAS$beta) # Post-processing for Ridge regression estRID &lt;- outRID$beta kmeRID &lt;- kmeans(x = as.numeric(abs(outRID$beta)), centers = 2) if(kmeRID$centers[1] &gt; kmeRID$centers[2]){ estRID[kmeRID$cluster == 2] &lt;- 0 } else { estRID[kmeRID$cluster == 1] &lt;- 0 } estRID &lt;- as.numeric(estRID) # Post-processing for Basic Bayesian regression kmeBAY &lt;- kmeans(x = as.numeric(abs(outBAY)), centers = 2) if(kmeBAY$centers[1] &gt; kmeBAY$centers[2]){ outBAY[kmeBAY$cluster == 2] &lt;- 0 } else { outBAY[kmeBAY$cluster == 1] &lt;- 0 } estBAY &lt;- apply(X = outBAY, MARGIN = 2, FUN = median) # Post-processing for Horseshoe Regression coeHOR &lt;- outHOR$B kmeHOR &lt;- kmeans(x = as.numeric(abs(coeHOR)), centers = 2) if(kmeHOR$centers[1] &gt; kmeHOR$centers[2]){ coeHOR[kmeHOR$cluster == 2] &lt;- 0 } else { coeHOR[kmeHOR$cluster == 1] &lt;- 0 } estHOR &lt;- apply(X = coeHOR, MARGIN = 2, FUN = median) # Create a table with rounded values coef_table &lt;- round(cbind( True_Beta = b, Ridge = estRID, Lasso = estLAS, B_Ridge = estBAY, B_Lasso = estBLA, Horseshoe = estHOR )[1:15, ], 4) rownames(coef_table) &lt;- paste0(&quot;Coefficient &quot;, 1:15) # Print as a formatted table kable(as.matrix(coef_table), caption = &quot;Comparison of Post-processed Coefficient Across Methods&quot;) Table 4.3: Comparison of Post-processed Coefficient Across Methods True_Beta Ridge Lasso B_Ridge B_Lasso Horseshoe Coefficient 1 0.7788 0.1298 0.7248 0.1291 0.7485 0.7657 Coefficient 2 0.6065 0.1416 0.6148 0.1407 0.6366 0.6066 Coefficient 3 0.4724 0.1398 0.4795 0.1414 0.4802 0.4812 Coefficient 4 0.3679 0.1192 0.3627 0.1187 0.3661 0.3635 Coefficient 5 0.2865 0.0986 0.2963 0.0993 0.2965 0.2957 Coefficient 6 0.2231 0.0778 0.2083 0.0782 0.2129 0.0000 Coefficient 7 0.1738 0.0000 0.1360 0.0564 0.1474 0.0000 Coefficient 8 0.1353 0.0000 0.1173 0.0385 0.1154 0.0000 Coefficient 9 0.1054 0.0000 0.0585 0.0000 0.0616 0.0000 Coefficient 10 0.0821 0.0000 0.0858 0.0000 0.1142 0.0000 Coefficient 11 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 Coefficient 12 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 Coefficient 13 0.0000 0.0000 0.0000 0.0000 0.0057 0.0000 Coefficient 14 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 Coefficient 15 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 This block refines and compares the coefficient estimates across different regression methods. Some methods, like Bayesian Ridge and Horseshoe, require post-processing to identify which coefficients should be set to zero. The final output is a table comparing the first 15 coefficients across methods. estBLA &lt;- apply(X = outBLA$beta, MARGIN = 2, FUN = median) estLAS &lt;- as.numeric(outLAS$beta) estBLA: Extracts posterior median estimates from Bayesian Lasso. estLAS: Converts Lasso coefficients to a numeric vector. No post-processing is required for these methods since Lasso and Bayesian Lasso naturally set some coefficients to zero. estRID &lt;- outRID$beta kmeRID &lt;- kmeans(x = as.numeric(abs(outRID$beta)), centers = 2) if(kmeRID$centers[1] &gt; kmeRID$centers[2]){ estRID[kmeRID$cluster == 2] &lt;- 0 } else { estRID[kmeRID$cluster == 1] &lt;- 0 } estRID &lt;- as.numeric(estRID) Ridge Regression retains all coefficients (i.e., does not set any to zero). Post-processing approach: Uses K-means clustering to separate large and small coefficients into two groups (centers = 2). The group with the smaller average magnitude is assumed to contain insignificant coefficients, so they are set to zero.  Why is this needed? - Ridge does not perform feature selection, so this helps identify zero coefficients. kmeBAY &lt;- kmeans(x = as.numeric(abs(outBAY)), centers = 2) if(kmeBAY$centers[1] &gt; kmeBAY$centers[2]){ outBAY[kmeBAY$cluster == 2] &lt;- 0 } else { outBAY[kmeBAY$cluster == 1] &lt;- 0 } estBAY &lt;- apply(X = outBAY, MARGIN = 2, FUN = median) Bayesian Ridge also does not perform feature selection, so we apply K-means post-processing. The median of the posterior samples is used as the final estimate. coeHOR &lt;- outHOR$B kmeHOR &lt;- kmeans(x = as.numeric(abs(coeHOR)), centers = 2) if(kmeHOR$centers[1] &gt; kmeHOR$centers[2]){ coeHOR[kmeHOR$cluster == 2] &lt;- 0 } else { coeHOR[kmeHOR$cluster == 1] &lt;- 0 } estHOR &lt;- apply(X = coeHOR, MARGIN = 2, FUN = median) The Horseshoe prior naturally encourages sparsity, but we reinforce it using the K-means clustering method. The posterior median is used as the final estimate. # Create a table with rounded values coef_table &lt;- round(cbind( True_Beta = b, Ridge = estRID, Lasso = estLAS, B_Ridge = estBAY, B_Lasso = estBLA, Horseshoe = estHOR )[1:15, ], 4) rownames(coef_table) &lt;- paste0(&quot;Coefficient &quot;, 1:15) # Print as a formatted table kable(as.matrix(coef_table), caption = &quot;Comparison of Post-processed Coefficient Across Methods&quot;) Combines all coefficient estimates (True \\(\\beta\\), Ridge, Lasso, Bayesian Ridge, Bayesian Lasso, Horseshoe). Rounds values to 4 decimal places for readability. Formats the table using kable() for a clean output. Key Insights - OLS and Ridge Regression retain all coefficients → Ridge needs post-processing to remove small values. - Lasso and Bayesian Lasso naturally set some coefficients exactly to zero. - Bayesian Ridge needs post-processing to enforce sparsity. - Horseshoe Prior is adaptive, but post-processing enhances sparsity. 4.4 Efficient Computation When dealing with high-dimensional settings where \\(p &gt; n\\), direct computation of matrix inverses and decompositions becomes computationally expensive. Specifically: Matrix inversion for an \\(n \\times p\\) matrix is typically \\(O(p^3)\\), which is prohibitive when \\(p\\) is large. Cholesky decomposition, commonly used for solving linear systems, is \\(O(p^3)\\) as well. To make computations more efficient, we can express operations in terms of \\(n\\) rather than \\(p\\) whenever possible. A key technique in Machine Learning and Statistics is the Woodbury Matrix Identity, which allows us to rewrite certain matrix inversions in a computationally cheaper form. 4.4.1 Efficient Computation of Ridge Regression using the Woodbury Identity Ridge regression estimates the coefficients \\(\\boldsymbol{\\beta}\\) as: \\[ \\hat{\\boldsymbol{\\beta}}_{\\text{ridge}} = (\\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\mathbf{X}&#39; \\mathbf{y} \\] where: - \\(\\mathbf{X}\\) is an \\(n \\times p\\) design matrix, - \\(\\lambda \\mathbf{I}\\) is a regularization term (where \\(\\lambda &gt; 0\\) ensures invertibility), - \\(\\mathbf{y}\\) is an \\(n \\times 1\\) response vector. Computing \\((\\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I})^{-1}\\) directly is expensive when \\(p\\) is large. Instead, we use the Woodbury Identity, which states: \\[ (\\mathbf{A} + \\mathbf{U} \\mathbf{C} \\mathbf{V})^{-1} = \\mathbf{A}^{-1} - \\mathbf{A}^{-1} \\mathbf{U} (\\mathbf{C}^{-1} + \\mathbf{V} \\mathbf{A}^{-1} \\mathbf{U})^{-1} \\mathbf{V} \\mathbf{A}^{-1} \\] To apply this to Ridge Regression, let’s define: \\(\\mathbf{A} = \\lambda \\mathbf{I}_p\\) (a \\(p \\times p\\) diagonal matrix), \\(\\mathbf{U} = \\mathbf{X}&#39;\\) (a \\(p \\times n\\) matrix), \\(\\mathbf{C} = \\mathbf{I}_n\\) (an \\(n \\times n\\) identity matrix), \\(\\mathbf{V} = \\mathbf{X}\\) (an \\(n \\times p\\) matrix). Applying the Woodbury Identity: \\[ (\\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I})^{-1} = \\lambda^{-1} \\mathbf{I} - \\lambda^{-1} \\mathbf{X}&#39; (\\mathbf{I} + \\mathbf{X} \\lambda^{-1} \\mathbf{X}&#39;)^{-1} \\mathbf{X} \\lambda^{-1} \\] Substituting this into the Ridge Regression equation: \\[ \\hat{\\boldsymbol{\\beta}}_{\\text{ridge}} = \\left( \\lambda^{-1} \\mathbf{I} - \\lambda^{-1} \\mathbf{X}&#39; (\\mathbf{I} + \\mathbf{X} \\lambda^{-1} \\mathbf{X}&#39;)^{-1} \\mathbf{X} \\lambda^{-1} \\right) \\mathbf{X}&#39; \\mathbf{y} \\] Rearranging: \\[ \\hat{\\boldsymbol{\\beta}}_{\\text{ridge}} = \\lambda^{-1} \\mathbf{X}&#39; \\mathbf{y} - \\lambda^{-1} \\mathbf{X}&#39; (\\mathbf{I} + \\mathbf{X} \\lambda^{-1} \\mathbf{X}&#39;)^{-1} \\mathbf{X} \\lambda^{-1} \\mathbf{X}&#39; \\mathbf{y} \\] Defining \\(n \\times n\\) matrix inversion instead of \\(p \\times p\\): \\[ \\mathbf{M} = (\\mathbf{I} + \\mathbf{X} \\lambda^{-1} \\mathbf{X}&#39;)^{-1} \\] \\[ \\hat{\\boldsymbol{\\beta}}_{\\text{ridge}} = \\lambda^{-1} \\mathbf{X}&#39; \\mathbf{y} - \\lambda^{-1} \\mathbf{X}&#39; \\mathbf{M} \\mathbf{X} \\lambda^{-1} \\mathbf{X}&#39; \\mathbf{y} \\] Computational Benefits: Direct inversion of \\(\\mathbf{X}&#39; \\mathbf{X} + \\lambda {\\boldsymbol I} \\) (size \\(p \\times p\\)) is \\(O(p^3)\\). Computing \\(\\mathbf{M}\\) involves inverting an \\(n \\times n\\) matrix, which is only \\(O(n^3)\\) (much faster when \\(p \\gg n\\)). Since \\(n \\ll p\\) in high-dimensional settings, this trick significantly reduces computational cost. Conclusion: Using the Woodbury Identity, we reformulate the Ridge Regression inversion in terms of \\(n\\) instead of \\(p\\). This approach is particularly useful in high-dimensional settings where \\(p &gt; n\\). Many Bayesian methods, such as Gaussian Process Regression and Variational Inference, also rely on the Woodbury identity for computational efficiency. 4.4.2 Efficient Bayesian Sampling for Gaussian Scale-Mixture Priors (Based on Bhattacharya et al. (2016), “Fast Sampling with Gaussian Scale-Mixture Priors in High-Dimensional Regression”) 4.4.2.1 Motivation Gaussian scale-mixture priors, such as the Horseshoe prior and Bayesian Lasso, are widely used for high-dimensional regression due to their adaptive shrinkage properties. These priors impose global-local shrinkage on regression coefficients, ensuring that irrelevant predictors are strongly shrunk, while allowing large signals to remain unshrunk. However, standard Markov Chain Monte Carlo (MCMC) sampling for these priors can be computationally expensive when \\(p \\gg n\\), particularly due to the matrix inversion required in Gibbs sampling steps. Existing approaches rely on Cholesky decomposition, which has a computational complexity of \\(O(p^3)\\), making it prohibitive for large-scale problems. Bhattacharya et al. (2016) propose an exact sampling algorithm that avoids direct matrix inversion by leveraging: 1. Latent variable augmentation to restructure the sampling problem. 2. Sherman–Morrison–Woodbury identity to reduce matrix inversion complexity from \\(O(p^3)\\) to \\(O(n^3)\\). 3. Block Gibbs sampling, allowing efficient updates of the regression coefficients. This method scales linearly with \\(p\\), making Bayesian inference feasible in high-dimensional settings. 4.4.2.2 Model Setup: Gaussian Scale-Mixture Priors in Bayesian Regression We consider the standard Bayesian linear regression model: \\[ \\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{e}, \\quad \\mathbf{e} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}) \\] where: - \\(\\mathbf{X}\\) is the \\(n \\times p\\) design matrix. - \\(\\boldsymbol{\\beta}\\) is the \\(p\\)-dimensional vector of regression coefficients. - \\(\\mathbf{y}\\) is the \\(n\\)-dimensional response vector. - \\(\\mathbf{e}\\) represents independent Gaussian noise. A Gaussian scale-mixture prior is imposed on \\(\\boldsymbol{\\beta}\\): \\[ \\beta_j | \\lambda_j, \\tau, \\sigma^2 \\sim \\mathcal{N}(0, \\lambda_j^2 \\tau^2 \\sigma^2) \\] where: - \\(\\lambda_j\\) are local shrinkage parameters, controlling the sparsity of each coefficient. - \\(\\tau\\) is a global shrinkage parameter, controlling overall sparsity. - Different choices of \\(p(\\lambda_j)\\) lead to different priors (e.g., Horseshoe, Bayesian Lasso). The full conditional posterior of \\(\\boldsymbol{\\beta}\\) is given by: \\[ \\boldsymbol{\\beta} | \\mathbf{y}, \\lambda, \\tau, \\sigma^2 \\sim \\mathcal{N} \\left( \\mathbf{A}^{-1} \\mathbf{X}&#39; \\mathbf{y}, \\sigma^2 \\mathbf{A}^{-1} \\right) \\] where: \\[ \\mathbf{A} = \\mathbf{X}&#39; \\mathbf{X} + \\Lambda^{-1}, \\quad \\Lambda = \\tau^2 \\text{diag}(\\lambda_1^2, ..., \\lambda_p^2). \\] Computing \\(\\mathbf{A}^{-1}\\) directly is prohibitively expensive for large \\(p\\), motivating the need for an efficient sampling algorithm. To sample from: \\[ \\boldsymbol{\\beta} \\sim \\mathcal{N} \\left( \\mathbf{A}^{-1} \\mathbf{X}&#39; \\mathbf{y}, \\sigma^2 \\mathbf{A}^{-1} \\right) \\] define: \\[ \\mathbf{Q} = (\\mathbf{X}&#39; \\mathbf{X} + \\Lambda^{-1})^{-1}, \\quad \\mathbf{b} = \\mathbf{X}&#39; \\mathbf{y}. \\] Instead of inverting \\(\\mathbf{Q}\\), introduce an auxiliary variable: \\[ \\mathbf{v} = \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{z}, \\quad \\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) \\] which allows a Woodbury-based reformulation of the problem. 4.4.2.3 Fast Gibbs Sampling Algorithm Step 1: Sample Auxiliary Variables Sample \\(\\mathbf{u} \\sim \\mathcal{N}(\\mathbf{0}, \\Lambda)\\). Sample \\(\\boldsymbol{\\delta} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_n)\\). Step 2: Compute a Latent Variable Compute: \\[ \\mathbf{v} = \\mathbf{X} \\mathbf{u} + \\boldsymbol{\\delta} \\] Step 3: Solve a Linear System Solve for \\(\\mathbf{w}\\) in: \\[ (\\mathbf{X} \\Lambda \\mathbf{X}&#39; + \\mathbf{I}) \\mathbf{w} = (\\mathbf{y} - \\mathbf{v}) \\] This avoids inverting \\(\\mathbf{X}&#39; \\mathbf{X} + \\Lambda^{-1}\\). Step 4: Compute the Updated Sample for \\(\\boldsymbol{\\beta}\\) Compute: \\[ \\boldsymbol{\\beta} = \\mathbf{u} + \\Lambda \\mathbf{X}&#39; \\mathbf{w} \\]  Key Insight: Instead of inverting a \\(p \\times p\\) matrix, the algorithm solves a linear system of size \\(n \\times n\\), reducing computational complexity from \\(O(p^3)\\) to \\(O(n^3)\\). 4.4.2.4 Computational Efficiency Standard MCMC: Requires inverting \\(p \\times p\\) matrices (\\(O(p^3)\\)). Proposed method: Uses matrix multiplications and solves a system of size \\(n \\times n\\) (\\(O(n^3)\\)). When \\(p \\gg n\\), this leads to significant computational savings. For example, the algorithm achieves a 250× speedup when \\(p = 5000\\) compared to standard methods. "],["principal-component-analysis.html", "5 Principal Component Analysis 5.1 PCA as a Low-Rank Approximation of \\(\\mathbf{X}\\) 5.2 Variance Maximization in PCA 5.3 PARAFAC decomposition 5.4 High-Dimensional PCA", " 5 Principal Component Analysis Principal Component Analysis (PCA) is a widely used dimensionality reduction technique in statistics and machine learning. While it is often introduced as a method for finding orthogonal directions of maximum variance, another fundamental perspective is that PCA provides the best low-rank approximation of the data matrix \\(\\mathbf{X}\\). In many real-world datasets, the observed variables exhibit significant redundancy due to correlations between features. This means that the true intrinsic dimensionality of the data is often much lower than the number of measured variables. PCA exploits this redundancy by representing the data using a lower-dimensional subspace while preserving as much of the original information as possible. 5.1 PCA as a Low-Rank Approximation of \\(\\mathbf{X}\\) Let \\(\\mathbf{X}\\) be an \\(n \\times p\\) data matrix, where \\(n\\) is the number of observations and \\(p\\) is the number of variables. The goal of PCA is to find a low-rank representation of \\(\\mathbf{X}\\) that captures its most important structure. This is achieved through the Singular Value Decomposition (SVD): \\[ \\mathbf{X} = \\sum_{i=1}^{r} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i&#39; \\] where: - \\(\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_r &gt; 0\\) are the singular values of \\(\\mathbf{X}\\), - \\(\\mathbf{u}_i\\) and \\(\\mathbf{v}_i\\) are the corresponding left and right singular vectors, - \\(r = \\text{rank}(\\mathbf{X})\\). PCA provides a rank-\\(k\\) approximation by retaining only the top \\(k\\) singular values and singular vectors, leading to: \\[ \\mathbf{X}_k = \\sum_{i=1}^{k} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i&#39;. \\] By the Eckart–Young–Mirsky theorem, this is the best approximation to \\(\\mathbf{X}\\) under the Frobenius norm, meaning it minimizes the reconstruction error: \\[ \\|\\mathbf{X} - \\mathbf{X}_k\\|_F^2 = \\sum_{i=k+1}^{r} \\sigma_i^2. \\] 5.1.1 Why Low-Rank Approximation Matters Dimensionality Reduction The original data matrix \\(\\mathbf{X}\\) is often high-dimensional and contains redundant information. PCA allows us to approximate \\(\\mathbf{X}\\) with a much smaller representation while preserving most of the variance. Noise Reduction Higher-order singular values \\(\\sigma_{k+1}, \\dots, \\sigma_r\\) are often associated with noise rather than meaningful structure. Truncating these components leads to a denoised version of the data. Compression and Storage Efficiency Instead of storing \\(n \\times p\\) raw data values, we only need to store the top \\(k\\) singular values and vectors, which is much more memory-efficient. Feature Extraction The principal components (columns of \\(\\mathbf{V}_k\\)) define a new basis for the data that is more informative and often interpretable. These features can be used for classification, clustering, and visualization. 5.1.2 The Singular Value Solution is the Best Low-Rank Approximation (Eckart–Young–Mirsky Theorem) We want to prove that the truncated Singular Value Decomposition (SVD) provides the best rank-\\(k\\) approximation to a matrix \\(\\mathbf{X}\\) in the Frobenius norm and spectral norm. This result is known as the Eckart–Young–Mirsky theorem. 5.1.2.1 Problem Statement (Eckart–Young–Mirsky Theorem) Let \\(\\mathbf{X}\\) be an \\(n \\times p\\) matrix with full SVD: \\[ \\mathbf{X} = \\sum_{i=1}^{r} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i&#39; \\] where: - \\(\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_r &gt; 0\\) are the singular values (ordered decreasingly), - \\(\\mathbf{u}_i\\) are the left singular vectors (columns of \\(\\mathbf{U}\\), an \\(n \\times n\\) orthonormal matrix), - \\(\\mathbf{v}_i\\) are the right singular vectors (columns of \\(\\mathbf{V}\\), a \\(p \\times p\\) orthonormal matrix), - \\(r = \\text{rank}(\\mathbf{X})\\). We seek a rank-\\(k\\) matrix \\(\\mathbf{Y}\\) that best approximates \\(\\mathbf{X}\\) by minimizing the Frobenius norm error: \\[ \\min_{\\text{rank}(\\mathbf{Y}) = k} \\|\\mathbf{X} - \\mathbf{Y}\\|_F. \\] The theorem states that the best rank-\\(k\\) approximation is given by the truncated SVD: \\[ \\mathbf{X}_k = \\sum_{i=1}^{k} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i&#39;. \\] We now prove this claim. 5.1.2.2 Proof (Eckart–Young–Mirsky Theorem) We will show this in four steps. Step 1: The Frobenius norm of \\(\\mathbf{X}\\) is: \\[ \\|\\mathbf{X}\\|_F^2 = \\sum_{i=1}^{r} \\sigma_i^2. \\] Step 2: Any rank-\\(k\\) approximation \\(\\mathbf{Y}\\) can be written in terms of some linear combination of singular vectors: \\[ \\mathbf{Y} = \\sum_{i=1}^{k} a_i \\mathbf{u}_i \\mathbf{v}_i&#39;. \\] Step 3: The residual error \\[ \\|\\mathbf{X} - \\mathbf{Y}\\|_F^2 = \\sum_{i=1}^{k} (\\sigma_i - a_i)^2 + \\sum_{i=k+1}^{r} \\sigma_i^2 \\] Step 4: The optimal choice is \\(a_i = \\sigma_i\\) for the first \\(k\\) terms and \\(a_i = 0\\) for the rest. For Step 1, we want to show that the Frobenius norm of a matrix \\(\\mathbf{X}\\) is equal to the sum of the squared singular values: \\[ \\|\\mathbf{X}\\|_F^2 = \\sum_{i=1}^{r} \\sigma_i^2. \\] The Frobenius norm of an \\(n \\times p\\) matrix \\(\\mathbf{X}\\) is defined as: \\[ \\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^{n} \\sum_{j=1}^{p} x_{ij}^2}. \\] Squaring both sides: \\[ \\|\\mathbf{X}\\|_F^2 = \\sum_{i=1}^{n} \\sum_{j=1}^{p} x_{ij}^2. \\] An alternative way to express the Frobenius norm is using the trace function: \\[ \\|\\mathbf{X}\\|_F^2 = \\text{Tr}(\\mathbf{X}&#39; \\mathbf{X}). \\] where \\(\\text{Tr}(\\mathbf{A})\\) denotes the trace (sum of the diagonal elements) of a square matrix \\(\\mathbf{A}\\). To see this note that \\[ \\|\\mathbf{X}\\|_F^2 = \\sum_{i=1}^{n} \\sum_{j=1}^{p} x_{ij}^2 = \\sum_{j=1}^{p} \\sum_{i=1}^{n} x_{ij}^2 = \\sum_{j=1}^{p} {\\boldsymbol x} _j&#39; {\\boldsymbol x} _j = \\text{Tr}(\\mathbf{X}&#39; \\mathbf{X}). \\] From the SVD decomposition, we write: \\[ \\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}&#39; \\] where: - \\(\\mathbf{U}\\) is an \\(n \\times n\\) orthonormal matrix of left singular vectors, - \\(\\mathbf{V}\\) is a \\(p \\times p\\) orthonormal matrix of right singular vectors, - \\(\\mathbf{\\Sigma}\\) is an \\(n \\times p\\) diagonal matrix of singular values: \\[ \\mathbf{\\Sigma} = \\begin{bmatrix} \\sigma_1 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma_2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_3 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; \\sigma_r \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\end{bmatrix}. \\] The rank of \\(\\mathbf{X}\\) is \\(r\\), meaning it has \\(r\\) nonzero singular values. Now, compute \\(\\mathbf{X}&#39; \\mathbf{X}\\): \\[ \\mathbf{X}&#39; \\mathbf{X} = (\\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}&#39;)&#39; (\\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}&#39;). \\] Using the transpose property \\((\\mathbf{A} \\mathbf{B})&#39; = \\mathbf{B}&#39; \\mathbf{A}&#39;\\): \\[ \\mathbf{X}&#39; \\mathbf{X} = \\mathbf{V} \\mathbf{\\Sigma}&#39; \\mathbf{U}&#39; \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}&#39;. \\] Since \\(\\mathbf{U}\\) is orthonormal (\\(\\mathbf{U}&#39; \\mathbf{U} = \\mathbf{I}\\)), this simplifies to: \\[ \\mathbf{X}&#39; \\mathbf{X} = \\mathbf{V} \\mathbf{\\Sigma}&#39; \\mathbf{\\Sigma} \\mathbf{V}&#39;. \\] Since \\(\\mathbf{\\Sigma}&#39; \\mathbf{\\Sigma}\\) is a diagonal matrix with squared singular values \\(\\sigma_i^2\\), we get: \\[ \\mathbf{X}&#39; \\mathbf{X} = \\mathbf{V} \\begin{bmatrix} \\sigma_1^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma_2^2 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 0 &amp; \\ddots &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma_r^2 \\end{bmatrix} \\mathbf{V}&#39;. \\] Taking the trace on both sides: \\[ \\text{Tr}(\\mathbf{X}&#39; \\mathbf{X}) = \\text{Tr}(\\mathbf{V} \\mathbf{\\Sigma}&#39; \\mathbf{\\Sigma} \\mathbf{V}&#39;). \\] Since the trace is invariant under cyclic permutations: \\[ \\text{Tr}(\\mathbf{X}&#39; \\mathbf{X}) = \\text{Tr}( \\mathbf{V}&#39; \\mathbf{V} \\mathbf{\\Sigma}&#39; \\mathbf{\\Sigma}) = \\text{Tr}(\\mathbf{\\Sigma}&#39; \\mathbf{\\Sigma}). \\] Since \\(\\mathbf{\\Sigma}&#39; \\mathbf{\\Sigma}\\) is diagonal, its trace is simply the sum of the diagonal elements: \\[ \\text{Tr}(\\mathbf{\\Sigma}&#39; \\mathbf{\\Sigma}) = \\sum_{i=1}^{r} \\sigma_i^2. \\] Since we previously established that: \\[ \\|\\mathbf{X}\\|_F^2 = \\text{Tr}(\\mathbf{X}&#39; \\mathbf{X}), \\] we conclude: \\[ \\|\\mathbf{X}\\|_F^2 = \\sum_{i=1}^{r} \\sigma_i^2. \\] For Step 2, we claim that any rank-\\(k\\) matrix \\(\\mathbf{Y}\\) can be expressed as: \\[ \\mathbf{Y} = \\sum_{i=1}^{k} a_i \\mathbf{u}_i \\mathbf{v}_i&#39;. \\] This follows from the fact that the outer products of the singular vectors, \\[ \\mathbf{u}_i \\mathbf{v}_i&#39; \\] form an orthonormal basis for matrices in \\(\\mathbb{R}^{n \\times p}\\), meaning that any rank-\\(k\\) matrix can be expressed as a linear combination of at most \\(k\\) of these elements. To see this, consider the inner product between two singular vector outer products, defined as: \\[ \\langle \\mathbf{u}_i \\mathbf{v}_i&#39;, \\mathbf{u}_j \\mathbf{v}_j&#39; \\rangle_F. \\] By the Frobenius inner product, the inner product between two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) is: \\[ \\langle \\mathbf{A}, \\mathbf{B} \\rangle_F = \\text{Tr}(\\mathbf{A}&#39; \\mathbf{B}). \\] Applying this to the singular vectors: \\[ \\langle \\mathbf{u}_i \\mathbf{v}_i&#39;, \\mathbf{u}_j \\mathbf{v}_j&#39; \\rangle_F = \\text{Tr} \\big( (\\mathbf{u}_i \\mathbf{v}_i&#39;)&#39; (\\mathbf{u}_j \\mathbf{v}_j&#39;) \\big). \\] Using the transpose property \\((\\mathbf{A} \\mathbf{B})&#39; = \\mathbf{B}&#39; \\mathbf{A}&#39;\\), we get: \\[ \\langle \\mathbf{u}_i \\mathbf{v}_i&#39;, \\mathbf{u}_j \\mathbf{v}_j&#39; \\rangle_F = \\text{Tr} \\big( \\mathbf{v}_i \\mathbf{u}_i&#39; \\mathbf{u}_j \\mathbf{v}_j&#39; \\big). \\] Since \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are orthonormal, we use: \\[ \\mathbf{u}_i&#39; \\mathbf{u}_j = \\delta_{ij}, \\quad \\mathbf{v}_i&#39; \\mathbf{v}_j = \\delta_{ij}. \\] where \\(\\delta_{ij} = 1\\), if \\(i=j\\) and \\(\\delta_{ij} = 0\\), if \\(i \\neq j\\). Thus, \\[ \\langle \\mathbf{u}_i \\mathbf{v}_i&#39;, \\mathbf{u}_j \\mathbf{v}_j&#39; \\rangle_F = \\delta_{ij}. \\] This proves that \\(\\{ \\mathbf{u}_i \\mathbf{v}_i&#39; \\}_{i=1}^{k}\\) are orthonormal under the Frobenius inner product, therefore linearly independent and forming an spanning bases for a rank \\(k\\) matrix. For Step 3, mote that the error matrix is: \\[ \\mathbf{X} - \\mathbf{Y} = \\sum_{i=1}^{r} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i&#39; - \\sum_{i=1}^{k} a_i \\mathbf{u}_i \\mathbf{v}_i&#39;. \\] Rewriting this sum: \\[ \\mathbf{X} - \\mathbf{Y} = \\sum_{i=1}^{k} (\\sigma_i - a_i) \\mathbf{u}_i \\mathbf{v}_i&#39; + \\sum_{i=k+1}^{r} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i&#39;. \\] Since the singular vector outer products \\(\\mathbf{u}_i \\mathbf{v}_i&#39;\\) form an orthonormal basis under the Frobenius norm, the squared Frobenius norm of a sum of such terms is the sum of the squared coefficients: \\[ \\|\\mathbf{X} - \\mathbf{Y}\\|_F^2 = \\sum_{i=1}^{k} (\\sigma_i - a_i)^2 + \\sum_{i=k+1}^{r} \\sigma_i^2. \\] Finally, in Step 4, we need to minimize the approximation error: \\[ \\sum_{i=1}^{k} (\\sigma_i - a_i)^2 + \\sum_{i=k+1}^{r} \\sigma_i^2, \\] the best choice is clearly: \\[ a_i = \\sigma_i, \\quad \\text{for } i = 1, \\dots, k. \\] Thus, the best rank-\\(k\\) approximation is: \\[ \\mathbf{X}_k = \\sum_{i=1}^{k} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i&#39;. \\] This minimizes the error: \\[ \\|\\mathbf{X} - \\mathbf{X}_k\\|_F^2 = \\sum_{i=k+1}^{r} \\sigma_i^2. \\] which confirms the Eckart–Young–Mirsky theorem. 5.1.3 Eckart–Young–Mirsky Theorem for the Spectral Norm The Eckart–Young–Mirsky theorem is a fundamental result in matrix approximation, stating that the best rank-k approximation of a matrix in terms of the spectral norm is obtained through its singular value decomposition (SVD). However, unlike in the Frobenius norm case, the solution is not always unique in the spectral norm. 5.1.3.1 Spectral Norm Theorem Statement For any matrix \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\) with singular values \\(\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_r &gt; 0\\), the rank-k matrix \\(\\mathbf{X}_k\\) that minimizes the spectral norm error \\(\\|\\mathbf{X} - \\mathbf{Y}\\|_2\\) is given by: \\[ \\mathbf{X}_k = \\sum_{i=1}^k \\sigma_i \\mathbf{u}_i \\mathbf{v}_i&#39;, \\] where \\(\\mathbf{u}_i\\) and \\(\\mathbf{v}_i\\) are the left and right singular vectors corresponding to \\(\\sigma_i\\). The minimal spectral norm error is: \\[ \\|\\mathbf{X} - \\mathbf{X}_k\\|_2 = \\sigma_{k+1}. \\] 5.1.3.2 Spectral Norm Proof Sketch The spectral norm of a matrix is its largest singular value. Given any rank-k matrix \\(\\mathbf{Y}\\) written as before \\(\\mathbf{Y} = \\sum_{i=1}^{k} a_i \\mathbf{u}_i \\mathbf{v}_i&#39;\\), its singular values are \\(a_1, a_2, \\dots, a_k\\). The error matrix \\(\\mathbf{X} - \\mathbf{Y}\\) has singular values: \\[ \\{ |\\sigma_1 - a_1|, |\\sigma_2 - a_2|, \\dots, |\\sigma_k - a_k|, \\sigma_{k+1}, \\dots, \\sigma_r \\}. \\] To minimize the spectral norm of the error, one must ensure that: \\[ \\max \\{ |\\sigma_1 - a_1|, |\\sigma_2 - a_2|, \\dots, \\sigma_{k+1} \\} \\] is as small as possible. A way to achieve the minimal error \\(\\sigma_{k+1}\\) is to set \\(a_i = \\sigma_i\\) for \\(i=1, \\dots, k\\), as with the spectral norm, but we can construct different rank-k matrices that also achieve this value. 5.1.3.3 Non-Uniqueness for the Spectral Norm Unlike in the Frobenius norm case, where the truncated SVD solution is unique, the spectral norm allows for multiple solutions. If we require: \\[ |\\sigma_i - a_i| \\leq \\sigma_{k+1}, \\quad \\forall i=1, \\dots, k, \\] any such choice of \\(a_i\\) will yield the same spectral norm error \\(\\sigma_{k+1}\\). This provides a class of rank-k approximations that achieve the minimal spectral norm error. 5.1.3.3.1 How to Find Alternative Solutions To find alternative rank-k approximations with the same spectral norm error, one can: Perturb Singular Values: Adjust \\(a_i\\) such that \\(|\\sigma_i - a_i| \\leq \\sigma_{k+1}\\), ensuring no singular value difference exceeds \\(\\sigma_{k+1}\\). Linear Combinations of Singular Vectors: Combine singular vectors corresponding to equal singular values to create different approximations. 5.1.3.4 Conclusion for the Spectral Norm The Eckart–Young–Mirsky theorem guarantees the minimal spectral norm error with the truncated SVD solution. However, in cases of spectral norm approximation, the solution is not unique. By carefully choosing singular values and vectors within the specified bounds, one can construct alternative solutions that achieve the same minimal error. This flexibility in spectral norm approximation highlights the subtle difference between approximations in spectral and Frobenius norms, making it an important consideration in applications like low-rank matrix approximation, data compression, and principal component analysis. 5.2 Variance Maximization in PCA Principal Component Analysis (PCA) is traditionally introduced as a method that finds directions (principal components) along which the variance of the data is maximized. Given a data matrix \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\) with centered columns, PCA seeks an orthonormal set of vectors \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_p\\}\\) such that: The first principal component \\(\\mathbf{v}_1\\) maximizes the variance: \\[ \\mathbf{v}_1 = \\arg \\max_{\\|\\mathbf{v}\\|=1} \\text{Var}(\\mathbf{X}\\mathbf{v}) = \\arg \\max_{\\|\\mathbf{v}\\|=1} \\|\\mathbf{X}\\mathbf{v}\\|_2^2. \\] Subsequent principal components are chosen orthogonal to the previous ones and also maximize the variance. Each principal component corresponds to a singular vector from the Singular Value Decomposition (SVD) of \\(\\mathbf{X}\\): \\[ \\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}&#39;, \\] where \\(\\mathbf{\\Sigma}\\) contains the singular values \\(\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r &gt; 0\\). The first \\(k\\) principal components are the first \\(k\\) columns of \\(\\mathbf{V}\\): \\[ \\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_k, \\] with the corresponding principal component scores given by: \\[ \\mathbf{X} \\mathbf{v}_i = \\sigma_i \\mathbf{u}_i, \\quad i = 1, 2, \\dots, k. \\] 5.2.1 First Principal Cmponent We aim to find the direction \\(\\mathbf{v}_1 \\in \\mathbb{R}^p\\) (a unit vector, \\(\\| \\mathbf{v}_1 \\| = 1\\)) that maximizes the variance of the data when projected onto \\(\\mathbf{v}_1\\). The dataset is represented by \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\) (with centered columns), and the projection of \\(\\mathbf{X}\\) onto \\(\\mathbf{v}_1\\) is given by \\(\\mathbf{Xv}_1\\). 5.2.1.1 Variance of the Projection The variance of the projected data \\(\\mathbf{Xv}_1\\) is: \\[ \\text{Var}(\\mathbf{Xv}_1) = \\frac{1}{n} \\| \\mathbf{Xv}_1 \\|_2^2 = \\frac{1}{n} (\\mathbf{Xv}_1)&#39;(\\mathbf{Xv}_1). \\] Expanding this: \\[ \\text{Var}(\\mathbf{Xv}_1) = \\frac{1}{n} \\mathbf{v}_1&#39; \\mathbf{X}&#39; \\mathbf{X} \\mathbf{v}_1. \\] Since \\(\\frac{1}{n} \\mathbf{X}&#39; \\mathbf{X}\\) is the sample covariance matrix \\(\\mathbf{S}\\), we rewrite the objective as: \\[ \\text{maximize } \\mathbf{v}_1&#39; \\mathbf{S} \\mathbf{v}_1 \\quad \\text{subject to} \\quad \\| \\mathbf{v}_1 \\| = 1. \\] 5.2.1.2 Optimization Problem (Rayleigh Quotient) This is a constrained optimization problem that can be solved using the method of Lagrange multipliers: \\[ \\mathcal{L}(\\mathbf{v}_1, \\lambda) = \\mathbf{v}_1&#39; \\mathbf{S} \\mathbf{v}_1 - \\lambda (\\mathbf{v}_1&#39; \\mathbf{v}_1 - 1). \\] Taking the derivative with respect to \\(\\mathbf{v}_1\\) and setting it to zero gives: \\[ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{v}_1} = 2 \\mathbf{S} \\mathbf{v}_1 - 2 \\lambda \\mathbf{v}_1 = 0. \\] Thus: \\[ \\mathbf{S} \\mathbf{v}_1 = \\lambda \\mathbf{v}_1. \\] 5.2.1.3 Eigenvalue Problem This is an eigenvalue problem where \\(\\lambda\\) is an eigenvalue of \\(\\mathbf{S}\\) and \\(\\mathbf{v}_1\\) is the corresponding eigenvector. The direction \\(\\mathbf{v}_1\\) that maximizes the variance is the eigenvector corresponding to the largest eigenvalue \\(\\lambda_1\\). Hence, the first principal component \\(\\mathbf{v}_1\\) is the eigenvector of \\(\\mathbf{S}\\) associated with the largest eigenvalue \\(\\lambda_1\\), and the maximum variance is \\(\\lambda_1\\). 5.2.1.4 Connection to SVD Recall that the SVD of \\(\\mathbf{X}\\) is: \\[ \\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}&#39;. \\] The sample covariance matrix \\(\\mathbf{S} = \\frac{1}{n} \\mathbf{X}&#39; \\mathbf{X}\\) can be written as: \\[ \\mathbf{S} = \\frac{1}{n} \\mathbf{V} \\mathbf{\\Sigma}&#39; \\mathbf{U}&#39; \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}&#39; = \\frac{1}{n} \\mathbf{V} \\mathbf{\\Sigma}^2 \\mathbf{V}&#39;. \\] The eigenvectors of \\(\\mathbf{S}\\) are the columns of \\(\\mathbf{V}\\), and the eigenvalues are the squared singular values \\(\\sigma_i^2\\) divided by \\(n\\). Thus: - The first principal component direction \\(\\mathbf{v}_1\\) is the first column of \\(\\mathbf{V}\\) from the SVD. - The variance explained by the first principal component is \\(\\frac{\\sigma_1^2}{n}\\). 5.2.2 Second Principal Component The second principal component \\(\\mathbf{v}_2\\) is the direction that: - Maximizes the variance of the projected data \\(\\mathbf{Xv}_2\\), - Subject to being orthogonal to the first principal component \\(\\mathbf{v}_1\\). 5.2.2.1 Optimization Problem We want to maximize: \\[ \\text{Var}(\\mathbf{Xv}_2) = \\mathbf{v}_2&#39; \\mathbf{S} \\mathbf{v}_2 \\] subject to: \\[ \\|\\mathbf{v}_2\\| = 1 \\quad \\text{and} \\quad \\mathbf{v}_2&#39; \\mathbf{v}_1 = 0. \\] 5.2.2.2 Solution Using Lagrange Multipliers The constraint \\(\\mathbf{v}_2&#39; \\mathbf{v}_1 = 0\\) ensures orthogonality to the first principal component. Solving this optimization problem leads to: \\[ \\mathbf{S} \\mathbf{v}_2 = \\lambda_2 \\mathbf{v}_2 \\] where \\(\\mathbf{v}_2\\) is the eigenvector associated with the second largest eigenvalue \\(\\lambda_2\\) of \\(\\mathbf{S}\\). 5.2.2.3 Intuition Since the first principal component \\(\\mathbf{v}_1\\) has already captured the maximum variance, the second principal component \\(\\mathbf{v}_2\\) captures the next largest amount of variance while being orthogonal to \\(\\mathbf{v}_1\\). We can continue in this way to find the next Principal Components. 5.3 PARAFAC decomposition The PARAFAC decomposition (also known as CANDECOMP/PARAFAC or CP decomposition) is a natural extension of the concept of Principal Component Analysis (PCA) from matrices to higher-order tensors. 5.3.1 What is a Tensor? A tensor is a multi-dimensional array, extending the concept of vectors (1D arrays) and matrices (2D arrays) to higher dimensions: - A vector is a tensor of order 1. - A matrix is a tensor of order 2. - A tensor of order \\(N\\) has \\(N\\) dimensions (also called modes). For example: - An image is a matrix (2D tensor). - A color video can be represented as a 3D tensor (height × width × time). 5.3.2 Motivation: Why Generalize PCA to Tensors? PCA works well with matrices, but many real-world data are naturally represented as tensors: - Psychology: Data from multiple individuals across multiple tests and times. - Signal Processing: Multi-channel EEG data recorded over time. - Chemometrics: Spectral data collected over different conditions. PCA provides a low-rank approximation for matrices by decomposing them into principal components. The PARAFAC decomposition extends this idea to tensors, finding components along each dimension simultaneously. 5.3.3 PARAFAC Decomposition Definition Given a tensor \\(\\mathcal{X} \\in \\mathbb{R}^{I_1 \\times I_2 \\times \\cdots \\times I_N}\\), the PARAFAC decomposition expresses \\(\\mathcal{X}\\) as a sum of \\(R\\) rank-1 tensors: \\[ \\mathcal{X} \\approx \\sum_{r=1}^{R} \\mathbf{a}_r^{(1)} \\circ \\mathbf{a}_r^{(2)} \\circ \\cdots \\circ \\mathbf{a}_r^{(N)}, \\] where: - \\(R\\) is the rank of the decomposition. - \\(\\mathbf{a}_r^{(n)} \\in \\mathbb{R}^{I_n}\\) are vectors along the \\(n\\)-th mode. - \\(\\circ\\) denotes the outer product. - Each rank-1 tensor \\(\\mathbf{a}_r^{(1)} \\circ \\mathbf{a}_r^{(2)} \\circ \\cdots \\circ \\mathbf{a}_r^{(N)}\\) is analogous to a principal component in PCA. 5.3.4 How is PARAFAC Related to PCA? PCA decomposes a matrix \\(\\mathbf{X}\\) into a sum of rank-1 matrices: \\[ \\mathbf{X} \\approx \\sum_{r=1}^{R} \\sigma_r \\mathbf{u}_r \\mathbf{v}_r&#39;, \\] where \\(\\mathbf{u}_r\\) and \\(\\mathbf{v}_r\\) are left and right singular vectors, and \\(\\sigma_r\\) are singular values. PARAFAC generalizes this concept by decomposing a tensor \\(\\mathcal{X}\\) into a sum of rank-1 tensors. Just like PCA finds directions that capture the most variance in a matrix, PARAFAC finds multi-dimensional factors that capture the most structure in a tensor. 5.3.5 Key Properties of PARAFAC Decomposition Uniqueness: Unlike PCA (where different rotations of principal components can yield the same solution), the PARAFAC decomposition is unique under mild conditions. This uniqueness makes PARAFAC particularly useful for interpretability. Low-rank Approximation: PARAFAC provides a low-rank approximation of tensors, analogous to PCA for matrices. Interpretability: Each component can be interpreted as a factor along each mode, making it valuable in fields like chemometrics and neuroscience. 5.4 High-Dimensional PCA When the number of features or variables is \\(p\\), computing the eigenvalue decomposition or SVD can be prohebitely expensive. We can have some alternatives depending on the 5.4.1 High-Dimensional PCA (\\(p \\gg n\\)) When the number of features \\(p\\) is much larger than the number of samples \\(n\\) (a common scenario in high-dimensional data analysis), directly computing the PCA decomposition from the \\(p \\times p\\) covariance matrix \\(\\mathbf{X}&#39; \\mathbf{X}\\) is computationally expensive. To address this, we can use the matrix \\(\\mathbf{X} \\mathbf{X}&#39;\\) instead, which is an \\(n \\times n\\) matrix, making the computation much more efficient. 5.4.1.1 Key Idea Instead of directly solving the eigenvalue problem: \\[ \\mathbf{X}&#39; \\mathbf{X} \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i, \\] we solve the equivalent problem: \\[ \\mathbf{X} \\mathbf{X}&#39; \\mathbf{u}_i = \\lambda_i \\mathbf{u}_i, \\] where \\(\\mathbf{u}_i\\) are the left singular vectors of \\(\\mathbf{X}\\). 5.4.1.2 Steps for Efficient Computation Form the \\(n \\times n\\) matrix \\(\\mathbf{X} \\mathbf{X}&#39;\\) instead of \\(\\mathbf{X}&#39; \\mathbf{X}\\). Compute the eigenvalues and eigenvectors of \\(\\mathbf{X} \\mathbf{X}&#39;\\): \\(\\mathbf{X} \\mathbf{X}&#39; \\mathbf{u}_i = \\sigma_i^2 \\mathbf{u}_i.\\) Obtain singular values \\(\\sigma_i\\) as the square roots of the eigenvalues. Compute the principal directions \\(\\mathbf{v}_i\\) from the \\(\\mathbf{u}_i\\) vectors: \\[ \\mathbf{v}_i = \\frac{\\mathbf{X}&#39; \\mathbf{u}_i}{\\sigma_i}. \\] Compute the PCA scores directly as: \\[ \\mathbf{X} \\mathbf{v}_i = \\sigma_i \\mathbf{u}_i. \\] 5.4.1.3 Why Does This Work? The matrix \\(\\mathbf{X} \\mathbf{X}&#39;\\) is much smaller when \\(n \\ll p\\), making eigen-decomposition computationally cheaper. Once we have the left singular vectors \\(\\mathbf{u}_i\\) from \\(\\mathbf{X} \\mathbf{X}&#39;\\), we can easily find the right singular vectors \\(\\mathbf{v}_i\\) without ever forming the large \\(p \\times p\\) matrix. This approach is crucial for handling high-dimensional data efficiently, ensuring that PCA remains computationally feasible even when \\(p \\gg n\\). 5.4.1.4 Efficient Computation Create the two methods two perform PCA # PCA using X&#39;X (traditional method) pca_traditional &lt;- function(X) { n &lt;- nrow(X) S &lt;- crossprod(X) # X&#39;X eig &lt;- eigen(S, symmetric = TRUE) # Eigen decomposition priCom &lt;- eig$vectors # PC varExp &lt;- eig$values / sum(eig$values) # Explain Variance by PC pcaSco &lt;- X %*% eig$vectors # PCA scores return(list(priCom = priCom, pcaSco = pcaSco, varExp = varExp)) } # PCA using X X&#39; (efficient method) pca_efficient &lt;- function(X) { S &lt;- tcrossprod(X) # X X&#39; eig &lt;- eigen(S, symmetric = TRUE) # Eigen decomposition singular_values &lt;- sqrt(eig$values) priCom &lt;- t(t(t(X) %*% eig$vectors) / singular_values) # Compute V = X&#39;U / Sigma pcaSco &lt;- t(t(eig$vectors) * singular_values) # PCA scores varExp &lt;- eig$values / sum(eig$values) # Explain Variance by PC return(list(priCom = priCom, pcaSco = pcaSco, varExp = varExp)) } Simulate a Data Set # Simulate high-dimensional data set.seed(123) n &lt;- 100 # Number of samples p &lt;- 1000 # Number of features # Design Matrix X &lt;- matrix(data = NA, nrow = n, ncol = 0) # Block-Correlation Matrix in Blocks sizBlo &lt;- 10 disCor &lt;- 0.5 C &lt;- exp(- disCor * abs(rep(1, sizBlo) %*% t(1:sizBlo) - (1:sizBlo) %*% t(rep(1, sizBlo)))) # Simulates the Design Matrix for(i in 1:(p / sizBlo)){ X &lt;- cbind(X, mvtnorm::rmvnorm(n = n, sigma = C)) } Compare the results to check they are the same # Performs PCA by Both Methods outTra &lt;- pca_traditional(X = X) outEff &lt;- pca_efficient(X = X) # Compare First Two Principal Components cbind(outTra$priCom[1:10, 1:2], outEff$priCom[1:10, 1:2]) ## [,1] [,2] [,3] [,4] ## [1,] -0.001202379 0.004997560 0.001202379 0.004997560 ## [2,] 0.014206244 0.032971374 -0.014206244 0.032971374 ## [3,] 0.017112378 0.030209540 -0.017112378 0.030209540 ## [4,] 0.008336267 0.029518877 -0.008336267 0.029518877 ## [5,] 0.007510180 0.041815584 -0.007510180 0.041815584 ## [6,] -0.013239150 -0.009654624 0.013239150 -0.009654624 ## [7,] 0.008449956 0.042200920 -0.008449956 0.042200920 ## [8,] 0.028501647 0.026011567 -0.028501647 0.026011567 ## [9,] 0.008413915 0.004696628 -0.008413915 0.004696628 ## [10,] 0.018911051 -0.012567287 -0.018911051 -0.012567287 # Compares the First Two Principal Component Scores cbind(outTra$pcaSco[1:10, 1:2], outEff$pcaSco[1:10, 1:2]) ## [,1] [,2] [,3] [,4] ## [1,] -6.5413193 -4.0702069 6.5413193 -4.0702069 ## [2,] -8.5679536 0.3470996 8.5679536 0.3470996 ## [3,] -2.7203524 2.0908395 2.7203524 2.0908395 ## [4,] 2.7429942 3.8261368 -2.7429942 3.8261368 ## [5,] -0.1077707 -6.3139474 0.1077707 -6.3139474 ## [6,] 0.2253885 0.2471944 -0.2253885 0.2471944 ## [7,] -1.3734419 -4.6083631 1.3734419 -4.6083631 ## [8,] 3.2775010 -8.2332778 -3.2775010 -8.2332778 ## [9,] -3.3366402 1.1154150 3.3366402 1.1154150 ## [10,] 8.1907932 -0.2565279 -8.1907932 -0.2565279 # Plots the Variance Explained par(mar = c(2,2,0,0)) par(mfrow = c(2, 1)) plot(outTra$varExp[1:50], xaxt = &quot;n&quot;) par(mar = c(2,2,0,0)) plot(outTra$varExp[1:50]) Compare the results in terms of computational efficiency # Load necessary library library(microbenchmark) # Benchmark both methods benchmark_results &lt;- microbenchmark( Traditional = pca_traditional(X), Efficient = pca_efficient(X), times = 5 ) # Print the results print(benchmark_results) ## Unit: milliseconds ## expr min lq mean median uq max neval ## Traditional 466.9068 468.4296 469.48742 470.2176 470.7544 471.1287 5 ## Efficient 4.6733 4.7558 4.77242 4.7740 4.8044 4.8546 5 Now, we try a bigger example # Simulate high-dimensional data set.seed(123) n &lt;- 100 # Number of samples p &lt;- 2000 # Number of features # Design Matrix X &lt;- matrix(data = NA, nrow = n, ncol = 0) # Block-Correlation Matrix in Blocks sizBlo &lt;- 10 disCor &lt;- 0.5 C &lt;- exp(- disCor * abs(rep(1, sizBlo) %*% t(1:sizBlo) - (1:sizBlo) %*% t(rep(1, sizBlo)))) # Simulates the Design Matrix for(i in 1:(p / sizBlo)){ X &lt;- cbind(X, mvtnorm::rmvnorm(n = n, sigma = C)) } # Benchmark both methods benchmark_results &lt;- microbenchmark( Traditional = pca_traditional(X), Efficient = pca_efficient(X), times = 5 ) # Print the results print(benchmark_results) ## Unit: milliseconds ## expr min lq mean median uq max neval ## Traditional 3787.3264 3828.9639 3880.3079 3832.9685 3960.7155 3991.5654 5 ## Efficient 9.0859 9.2456 9.9712 10.3295 10.4905 10.7045 5 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
