[["index.html", "DS 6388 Spring 2025 1 DS 6388: Multivariate Statistical Methods for High-dimensional Data 1.1 Calendar", " DS 6388 Spring 2025 Rene Gutierrez University of Texas at El Paso 1 DS 6388: Multivariate Statistical Methods for High-dimensional Data This is the website for DS 6388. It contains relevant information for the course and lecture notes. 1.1 Calendar 1.1.1 Importatnt Dates February 10: Linear Algebra Extra Credit Test March 2: First Project Submission Deadline March 25: Second Project Submission Deadline 1.1.2 Class Schedule Class 1: Introduction to Multivariate High-Dimensional Methods Class 2: HD Linear Regression: Machine Learning Class 3: HD Linear Regression: Bayesian Methods Class 4: Computational Implementation of HD Linear Regression Class 5: Efficient Computation of HD Linear Regression Class 6: Bonus Linear Algebra Test Class 7: Introduction to PCA Class 8: HD PCA Class 9: Principal Components Regression Class 10: Factor Analysis 1 Class 11: Factor Analysis 2 Class 12: Canonical Correlation Analysis Class 13: HD Canonical Correlation Analysis Class 14: Project 2 Class 15: K-Means Part 1 Class 16: K-Means Part 2 Class 17: Expectation Maximization Class 18: Gaussian Mixture Models (GMM) Class 19: HD GMM Class 20: Project 3 Class 21: Logistic Regression Class 22: Support Vector Machines Class 23: SVM Part 2 Class 24: Decision Tree Learning "],["prerequisites.html", "2 Prerequisites 2.1 Linear Algebra 2.2 Calculus 2.3 Probability 2.4 Statistics", " 2 Prerequisites Before diving into the course, it’s important to have a solid understanding of the following foundational concepts. These are categorized into four key topics: Linear Algebra Probability Statistics Calculus 2.1 Linear Algebra You should be familiar with the following linear algebra concepts: Linear Independence Column Space of a Matrix Rank of a Matrix Full Rank Matrix Inverse Matrix Positive Definite Matrix Singular Value Decomposition Eigendecomposition Idempotent Matrix Determinant of a Matrix 2.1.1 Linear Independence Linear independence is a fundamental concept in linear algebra that describes a set of vectors where no vector can be written as a linear combination of the others. In other words, the vectors are not “redundant,” meaning none of the vectors depends on any other in the set. 2.1.1.1 Definition: A set of vectors \\(\\{ \\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n \\}\\) in a vector space is linearly independent if the only solution to the equation: \\[ c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_n \\mathbf{v}_n = \\mathbf{0} \\] is when all the scalar coefficients \\(c_1, c_2, \\ldots, c_n\\) are zero, i.e., \\(c_1 = c_2 = \\cdots = c_n = 0\\). If any of the coefficients can be non-zero while still satisfying this equation, then the vectors are linearly dependent. 2.1.1.2 Example: Consider two vectors in \\(\\mathbb{R}^2\\): \\[ \\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\] These vectors are linearly independent because there is no way to express one as a multiple of the other. The only solution to: \\[ c_1 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} + c_2 \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\] is \\(c_1 = 0\\) and \\(c_2 = 0\\). In contrast, if: \\[ \\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} \\] These vectors are linearly dependent, because \\(\\mathbf{v}_2 = 2 \\mathbf{v}_1\\). Therefore, you can express \\(mathbf{v}_2\\) as a linear combination of \\(\\mathbf{v}_1\\). 2.1.1.3 Key Points: Linearly independent vectors carry distinct information and cannot be derived from each other. Linearly dependent vectors are redundant because one or more can be expressed as a combination of others. In a set of linearly independent vectors, removing any vector would reduce the span of the vector space they cover. 2.1.1.4 Importance: Linear independence is crucial in determining the rank of a matrix. In systems of equations, linear independence of the rows or columns determines if the system has a unique solution. In vector spaces, the dimension of the space is the maximum number of linearly independent vectors. 2.1.2 Column Space of a Matrix The column space of a matrix is the set of all possible linear combinations of its columns. If you have a matrix \\(\\mathbf{A}\\) with \\(n\\) rows and \\(p\\) columns, the column space of \\(\\mathbf{A}\\), denoted as Col(\\(\\mathbf{A}\\)), consists of all vectors in \\(\\mathbb{R}^n\\) that can be expressed as a linear combination of the columns of \\(\\mathbf{A}\\). 2.1.2.1 Definition: Given a matrix \\(\\mathbf{A}\\) with columns \\(\\mathbf{a}_1, \\mathbf{a}_2, \\dots, \\mathbf{a}_p\\), the column space of \\(\\mathbf{A}\\) is defined as: \\[ \\text{Col}(\\mathbf{A}) = \\left\\{ \\mathbf{y} \\in \\mathbb{R}^n \\mid \\mathbf{y} = \\mathbf{A} \\mathbf{c} \\text{ for some } \\mathbf{c} \\in \\mathbb{R}^p \\right\\} \\] This means the column space is the span of the columns of \\(\\mathbf{A}\\), or equivalently, all vectors that can be written as \\(\\mathbf{y} = c_1 \\mathbf{a}_1 + c_2 \\mathbf{a}_2 + \\dots + c_p \\mathbf{a}_p\\), where \\(c_1, c_2, \\dots, c_p\\) are scalars. 2.1.2.2 Properties: The column space of \\(\\mathbf{A}\\) is a subspace of \\(\\mathbb{R}^n\\). The dimension of the column space of \\(\\mathbf{A}\\) is called the rank of the matrix and corresponds to the number of linearly independent columns in \\(\\mathbf{A}\\). The column space provides valuable information about the linear independence and span of the columns of a matrix. 2.1.2.3 Geometric Interpretation: In geometric terms, the column space represents the set of all possible vectors that can be “reached” by linearly combining the columns of the matrix. For example: - For a matrix with 2 columns in \\(\\mathbb{R}^3\\), the column space will be a plane in \\(\\mathbb{R}^3\\) if the columns are linearly independent. - For a matrix with 3 columns in \\(\\mathbb{R}^2\\), the column space will span all of \\(\\mathbb{R}^2\\) (if the columns are linearly independent) or a line (if they are dependent). 2.1.3 Rank of a Matrix The rank of a matrix is the dimension of its column space, which is the number of linearly independent columns in the matrix. Alternatively, it is also the dimension of the row space, which is the number of linearly independent rows. 2.1.3.1 Definition: For a matrix \\(\\mathbf{A}\\), the rank is defined as: \\[ \\text{rank}(\\mathbf{A}) = \\dim(\\text{Col}(\\mathbf{A})) = \\dim(\\text{Row}(\\mathbf{A})) \\] This is the maximum number of linearly independent rows or columns in the matrix. In other words, it tells you how many of the matrix’s columns (or rows) are not redundant and cannot be written as a linear combination of the others. 2.1.3.2 Key Points: The rank of a matrix \\(\\mathbf{A}\\) is denoted as rank(\\(\\mathbf{A}\\)). It measures the number of independent directions in the column space or row space. Full rank: A matrix is said to have full rank if its rank is equal to the smaller of the number of rows or columns. For an \\(m \\times n\\) matrix: If \\(\\text{rank}(\\mathbf{A}) = m\\) (number of rows), it has full row rank. If \\(\\text{rank}(\\mathbf{A}) = n\\) (number of columns), it has full column rank. Rank-deficient: If the rank of the matrix is less than the smaller of the number of rows or columns, the matrix is called rank-deficient, meaning that some of its rows or columns are linearly dependent. \\(\\text{rank}( {\\boldsymbol A} ) = \\text{rank}( {\\boldsymbol A} &#39;) = \\text{rank}( {\\boldsymbol A} &#39; {\\boldsymbol A} ) = \\text{rank}( {\\boldsymbol A} {\\boldsymbol A} &#39;)\\) 2.1.3.3 Example: Consider the matrix: \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{bmatrix} \\] The rank of \\(\\mathbf{A}\\) is 2 because two of the rows (or columns) are linearly independent, but the third row (or column) is a linear combination of the others. 2.1.3.4 Properties: The rank of a matrix is always less than or equal to the minimum of the number of rows and columns: \\[ \\text{rank}(\\mathbf{A}) \\leq \\min(m, n) \\] The rank of a matrix is equal to the number of non-zero singular values in its singular value decomposition (SVD). In square matrices, the rank gives insight into whether the matrix is invertible. A square matrix is invertible if and only if it has full rank. 2.1.4 Full Rank Matrix A full rank matrix is a matrix in which the rank is equal to the largest possible value for that matrix, meaning: For an \\(m \\times n\\) matrix \\(A\\), the rank is the maximum number of linearly independent rows or columns. If the rank is equal to \\(m\\) (the number of rows), the matrix has full row rank. If the rank is equal to \\(n\\) (the number of columns), the matrix has full column rank. 2.1.4.1 For a square matrix (\\(m = n\\)): A square matrix is full rank if its rank is equal to its dimension, i.e., if the matrix is invertible. In this case, \\(\\text{rank}( {\\boldsymbol A} ) = n\\), meaning all rows and columns are linearly independent, and the matrix has an inverse. 2.1.4.2 For a rectangular matrix (\\(m \\neq n\\)): A matrix is full rank if the rank equals the smaller of the number of rows or columns. For an \\(m \\times n\\) matrix, the rank is at most \\(\\min(m, n)\\). If the matrix has full row rank, all rows are linearly independent. If the matrix has full column rank, all columns are linearly independent. 2.1.4.3 Example: Consider the matrix: \\[ {\\boldsymbol A} = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix} \\] This is a \\(2 \\times 3\\) matrix. Since its two rows are linearly independent, it has full row rank, with rank = 2 (the number of rows). However, it does not have full column rank because it has only two independent rows for three columns. 2.1.4.4 Key Properties: A full rank matrix has no redundant rows or columns (no row or column can be written as a linear combination of others). A square matrix with full rank is invertible (non-singular). For a rectangular matrix, full rank implies the matrix has maximal independent information in terms of its rows or columns. 2.1.4.5 Importance: Full rank matrices are crucial in solving systems of linear equations. A system \\( {\\boldsymbol A} \\mathbf{x} = \\mathbf{b}\\) has a unique solution if \\( {\\boldsymbol A} \\) is a square, full rank matrix. In linear algebra and machine learning, the rank provides insight into the dimensionality and the independence of the data or transformation matrix. 2.1.5 Inverse Matrix An inverse matrix of a square matrix \\( {\\boldsymbol A} \\), denoted as \\( {\\boldsymbol A} ^{-1}\\), is a matrix that, when multiplied by \\( {\\boldsymbol A} \\), results in the identity matrix \\(I\\). This relationship is expressed as: \\[ {\\boldsymbol A} {\\boldsymbol A} ^{-1} = {\\boldsymbol A} ^{-1} {\\boldsymbol A} = {\\boldsymbol I} \\] where \\( {\\boldsymbol I} \\) is the identity matrix, and its diagonal elements are 1, with all off-diagonal elements being 0. 2.1.5.1 Conditions for a Matrix to Have an Inverse: The matrix \\( {\\boldsymbol A} \\) must be square, meaning it has the same number of rows and columns. The matrix \\( {\\boldsymbol A} \\) must be non-singular, meaning its determinant is non-zero (\\(| {\\boldsymbol A} | \\neq 0\\)). 2.1.5.2 Properties of the Inverse Matrix: Uniqueness: If a matrix has an inverse, it is unique. Inverse of a Product: The inverse of the product of two matrices \\( {\\boldsymbol A} \\) and \\( {\\boldsymbol B} \\) is given by \\(( {\\boldsymbol A} {\\boldsymbol B} )^{-1} = {\\boldsymbol B} ^{-1} {\\boldsymbol A} ^{-1}\\). Inverse of the Inverse: \\(( {\\boldsymbol A} ^{-1})^{-1} = {\\boldsymbol A} \\). Transpose of the Inverse: \\(( {\\boldsymbol A} ^{-1})&#39; = ( {\\boldsymbol A} &#39;)^{-1}\\). 2.1.5.3 Special Case 2 by 2 Matrix For a \\(2 \\times 2\\) matrix: \\[ {\\boldsymbol A} = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix} \\] The inverse of \\( {\\boldsymbol A} \\) (if \\(| {\\boldsymbol A} |=\\det( {\\boldsymbol A} ) \\neq 0\\)) is: \\[ A^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix} d &amp; -b \\\\ -c &amp; a \\end{bmatrix} \\] where \\(ad - bc\\) is the determinant of the matrix \\( {\\boldsymbol A} \\). 2.1.5.4 Special Case 2 by 2 Block Matrix The inverse of a \\(2 \\times 2\\) block matrix can be expressed under certain conditions. Let’s consider a block matrix \\(\\mathbf{M}\\) of the form: \\[ \\mathbf{M} = \\begin{bmatrix} \\mathbf{A} &amp; \\mathbf{B} \\\\ \\mathbf{C} &amp; \\mathbf{D} \\end{bmatrix} \\] where: - \\(\\mathbf{A}\\) and \\(\\mathbf{D}\\) are themselves square matrices, and \\(\\mathbf{B}\\) and \\(\\mathbf{C}\\) are matrices (not necessarily square). Then the inverse of \\(\\mathbf{M}\\) is given by: \\[ \\mathbf{M}^{-1} = \\begin{bmatrix} \\mathbf{A}^{-1} + \\mathbf{A}^{-1} \\mathbf{B} \\mathbf{S}^{-1} \\mathbf{C} \\mathbf{A}^{-1} &amp; -\\mathbf{A}^{-1} \\mathbf{B} \\mathbf{S}^{-1} \\\\ -\\mathbf{S}^{-1} \\mathbf{C} \\mathbf{A}^{-1} &amp; \\mathbf{S}^{-1} \\end{bmatrix} \\] where \\(\\mathbf{S}\\) is the Schur complement of \\(\\mathbf{A}\\) and is defined as: \\[ \\mathbf{S} = \\mathbf{D} - \\mathbf{C} \\mathbf{A}^{-1} \\mathbf{B} \\] 2.1.5.4.1 Conditions for the Inverse to Exist: \\(\\mathbf{A}\\) must be invertible, The Schur complement \\(\\mathbf{S}\\) must also be invertible. 2.1.5.4.2 Explanation of the Terms: \\(\\mathbf{A}^{-1}\\): The inverse of matrix \\(\\mathbf{A}\\), \\(\\mathbf{S}^{-1}\\): The inverse of the Schur complement \\(\\mathbf{S}\\), which can be interpreted as the “effective” part of matrix \\(\\mathbf{D}\\) once the contribution of \\(\\mathbf{A}\\) has been removed. This formula generalizes the concept of inverting a matrix when it’s partitioned into blocks. 2.1.5.5 Sherman-Morrison Formula The Sherman-Morrison formula provides a way to compute the inverse of a matrix after it has been updated by a rank-one modification. Specifically, it addresses the situation where a matrix A has been updated by the outer product of two vectors u and v. The formula is: \\[ (\\mathbf{A} + \\mathbf{u} \\mathbf{v}^T)^{-1} = \\mathbf{A}^{-1} - \\frac{\\mathbf{A}^{-1} \\mathbf{u} \\mathbf{v}^T \\mathbf{A}^{-1}}{1 + \\mathbf{v}^T \\mathbf{A}^{-1} \\mathbf{u}} \\] 2.1.5.5.1 Requirements: A must be an invertible matrix. The scalar \\(1 + \\mathbf{v}^T \\mathbf{A}^{-1} \\mathbf{u}\\) must not be zero. 2.1.5.5.2 Explanation of the terms: A is an invertible \\(n \\times n\\) matrix. u and v are \\(n \\times 1\\) column vectors. The outer product \\(\\mathbf{u} \\mathbf{v}^T\\) is an \\(n \\times n\\) rank-one matrix. The term \\(1 + \\mathbf{v}^T \\mathbf{A}^{-1} \\mathbf{u}\\) is a scalar. This formula is useful in situations where you need to efficiently update the inverse of a matrix after a low-rank modification, rather than recomputing the inverse from scratch. 2.1.6 Positive Definite Matrix A positive definite matrix is a symmetric matrix \\( {\\boldsymbol A} \\) where, for any non-zero vector \\(\\mathbf{x}\\), the following condition holds: \\[ \\mathbf{x}&#39; {\\boldsymbol A} \\mathbf{x} &gt; 0 \\] 2.1.6.1 Key Properties: Symmetry: The matrix \\( {\\boldsymbol A} \\) must be symmetric, meaning \\( {\\boldsymbol A} = {\\boldsymbol A} &#39;\\). Positive quadratic form: For any non-zero vector \\(\\mathbf{x}\\), the quadratic form \\(\\mathbf{x}&#39; {\\boldsymbol A} \\mathbf{x}\\) must yield a positive value. 2.1.6.2 Characteristics of a Positive Definite Matrix: All the eigenvalues of a positive definite matrix are positive. The determinants of the leading principal minors (submatrices) of the matrix are positive. The diagonal elements of a positive definite matrix are positive. \\( {\\boldsymbol A} \\) is invertible. \\( {\\boldsymbol A} ^{-1}\\) is also positive definite matrix. 2.1.6.3 Example: The matrix: \\[ {\\boldsymbol A} = \\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 2 \\end{bmatrix} \\] is positive definite, because for any non-zero vector \\(\\mathbf{x}\\), \\(\\mathbf{x}&#39; {\\boldsymbol A} \\mathbf{x} &gt; 0\\). For instance, if \\(\\mathbf{x} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\), then: \\[ \\mathbf{x}&#39; {\\boldsymbol A} \\mathbf{x} = \\begin{bmatrix} 1 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = 6 &gt; 0 \\] 2.1.7 Singular Value Decomposition Singular Value Decomposition (SVD) is a fundamental matrix factorization technique used in linear algebra to break down a matrix into three distinct components. It provides valuable insight into the structure of a matrix and is widely used in applications like data compression, signal processing, and dimensionality reduction. 2.1.7.1 Definition: For any real (or complex) matrix \\(\\mathbf{A}\\) of size \\(m \\times n\\), the Singular Value Decomposition is given by: \\[ \\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}&#39; \\] where: - \\(\\mathbf{U}\\) is an \\(m \\times m\\) orthogonal matrix, whose columns are called the left singular vectors. - \\(\\mathbf{\\Sigma}\\) is an \\(m \\times n\\) diagonal matrix, where the diagonal entries are the singular values of \\(\\mathbf{A}\\). The singular values are always non-negative and arranged in decreasing order. - \\(\\mathbf{V}\\) is an \\(n \\times n\\) orthogonal matrix, whose columns are called the right singular vectors. 2.1.7.2 Interpretation of the Components: \\(\\mathbf{U}\\) represents the orthonormal basis for the column space of \\(\\mathbf{A}\\). \\(\\mathbf{V}\\) represents the orthonormal basis for the row space of \\(\\mathbf{A}\\). \\(\\mathbf{\\Sigma}\\) contains the singular values, which provide information about the importance or magnitude of the corresponding singular vectors. Large singular values indicate directions with significant data spread, while small or zero singular values correspond to directions with little or no data variation. 2.1.7.3 Geometric Interpretation: SVD can be viewed geometrically as a transformation where: 1. \\(\\mathbf{V}\\) applies a rotation or reflection in the input space. 2. \\(\\mathbf{\\Sigma}\\) stretches or compresses the data along certain axes. 3. \\(\\mathbf{U}\\) applies a final rotation or reflection in the output space. 2.1.7.4 Key Points: Rank: The number of non-zero singular values in \\(\\mathbf{\\Sigma}\\) equals the rank of the matrix \\(\\mathbf{A}\\). Dimensionality Reduction: By truncating small singular values in \\(\\mathbf{\\Sigma}\\), we can approximate \\(\\mathbf{A}\\) with a lower-rank matrix, which is useful in compressing data while retaining most of its structure. Condition Number: The ratio of the largest to the smallest non-zero singular value gives the condition number of the matrix, which indicates how sensitive a matrix is to numerical errors or perturbations. 2.1.7.5 Example: For a matrix \\(\\mathbf{A}\\) of size \\(3 \\times 2\\), the SVD would look like: \\[ \\mathbf{A} = \\mathbf{U} \\begin{bmatrix} \\sigma_1 &amp; 0 \\\\ 0 &amp; \\sigma_2 \\\\ 0 &amp; 0 \\end{bmatrix} \\mathbf{V}&#39; \\] where \\(\\sigma_1\\) and \\(\\sigma_2\\) are the singular values, and \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are orthogonal matrices. 2.1.7.6 Applications of SVD: Dimensionality Reduction: SVD is widely used in Principal Component Analysis (PCA) for reducing the dimensionality of large datasets. Low-Rank Approximations: In data compression, SVD helps to approximate matrices with fewer dimensions while maintaining the core structure. Solving Linear Systems: In cases where a matrix is close to singular, SVD can be used to solve linear systems more stably. Latent Semantic Analysis (LSA): In natural language processing, SVD is used to reduce the dimensionality of word-document matrices to capture latent relationships between words and documents. 2.1.8 Eigendecomposition Eigendecomposition is a matrix factorization technique used in linear algebra, where a square matrix is decomposed into its eigenvalues and eigenvectors. It is applicable to square matrices and provides deep insight into the matrix’s structure, particularly in understanding transformations, systems of linear equations, and differential equations. 2.1.8.1 Definition: Given a square matrix \\(\\mathbf{A}\\) of size \\(n \\times n\\), eigendecomposition is a factorization of the matrix into the following form: \\[ \\mathbf{A} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^{-1} \\] where: - \\(\\mathbf{V}\\) is the matrix of eigenvectors of \\(\\mathbf{A}\\), and each column of \\(\\mathbf{V}\\) is an eigenvector. - \\(\\mathbf{\\Lambda}\\) is a diagonal matrix of eigenvalues of \\(\\mathbf{A}\\), with each diagonal element corresponding to an eigenvalue of \\(\\mathbf{A}\\). - \\(\\mathbf{V}^{-1}\\) is the inverse of the matrix of eigenvectors. 2.1.8.2 Eigenvalues and Eigenvectors: Eigenvalue (\\(\\lambda\\)): A scalar \\(\\lambda\\) is an eigenvalue of \\(\\mathbf{A}\\) if there exists a non-zero vector \\(\\mathbf{v}\\) such that: \\[ \\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v} \\] In this case, \\(\\mathbf{v}\\) is called the eigenvector corresponding to the eigenvalue \\(\\lambda\\). Eigenvector: A non-zero vector \\(\\mathbf{v}\\) that remains parallel to itself (i.e., only scaled) when multiplied by \\(\\mathbf{A}\\) is called an eigenvector. 2.1.8.3 Conditions for Eigendecomposition: A matrix \\(\\mathbf{A}\\) is diagonalizable (i.e., it can be factored into \\(\\mathbf{A} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^{-1}\\)) if and only if it has \\(n\\) linearly independent eigenvectors. Not all matrices are diagonalizable. However, if \\(\\mathbf{A}\\) has \\(n\\) distinct eigenvalues, then it is guaranteed to be diagonalizable. Symmetric matrices are always diagonalizable. 2.1.8.4 Geometric Interpretation: Eigendecomposition reveals the directions (eigenvectors) along which the matrix transformation \\(\\mathbf{A}\\) acts as a simple scaling by the eigenvalues. Geometrically: - Eigenvectors point in directions that remain invariant under the transformation by \\(\\mathbf{A}\\). - The corresponding eigenvalues tell us how much the matrix stretches or compresses vectors in the direction of those eigenvectors. 2.1.8.5 Example: For a matrix \\(\\mathbf{A}\\): \\[ \\mathbf{A} = \\begin{bmatrix} 4 &amp; 1 \\\\ 2 &amp; 3 \\end{bmatrix} \\] The eigenvalues \\(\\lambda_1 = 5\\) and \\(\\lambda_2 = 2\\) can be found by solving the characteristic equation \\(\\det(\\mathbf{A} - \\lambda \\mathbf{I}) = 0\\). Corresponding eigenvectors can then be computed, allowing the matrix to be diagonalized as: \\[ \\mathbf{A} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^{-1} \\] where \\(\\mathbf{\\Lambda} = \\text{diag}(5, 2)\\) and \\(\\mathbf{V}\\) is the matrix of eigenvectors. 2.1.8.6 Applications of Eigendecomposition: Diagonalization: Eigendecomposition allows matrices to be diagonalized, simplifying many computations (such as raising matrices to powers). Principal Component Analysis (PCA): In data science, eigendecomposition is used in PCA to find directions of maximum variance in data. Solving Differential Equations: Eigenvalues and eigenvectors are useful in solving systems of linear differential equations. Quantum Mechanics: In physics, eigenvalues and eigenvectors describe the measurable properties (like energy levels) of systems. In summary, eigendecomposition is a powerful tool in linear algebra that provides insight into how a matrix transforms space, offering valuable properties through its eigenvalues and eigenvectors. 2.1.9 Idempotent Matrix An idempotent matrix is a matrix that, when multiplied by itself, yields the same matrix. In other words, a matrix \\(\\mathbf{M}\\) is idempotent if it satisfies the condition: \\[ \\mathbf{M}^2 = \\mathbf{M} \\] 2.1.9.1 Key Properties of Idempotent Matrices: Eigenvalues: The eigenvalues of an idempotent matrix are either 0 or 1. This is because for an eigenvector \\(\\mathbf{v}\\) with eigenvalue \\(\\lambda\\), the equation \\(\\mathbf{M}^2 \\mathbf{v} = \\mathbf{M} \\mathbf{v}\\) simplifies to \\(\\lambda^2 \\mathbf{v} = \\lambda \\mathbf{v}\\), meaning \\(\\lambda(\\lambda - 1) = 0\\), so \\(\\lambda = 0\\) or \\(\\lambda = 1\\). Rank: The rank of an idempotent matrix \\(\\mathbf{M}\\) is equal to the trace of the matrix (the sum of the diagonal elements), which is also the number of eigenvalues equal to 1. Projection Interpretation: Idempotent matrices often represent projection matrices in linear algebra. A projection matrix projects vectors onto a subspace, and applying the projection multiple times doesn’t change the result beyond the first application, which is why it satisfies \\(\\mathbf{M}^2 = \\mathbf{M}\\). 2.1.9.2 Examples: Identity Matrix: The identity matrix \\(\\mathbf{I}\\) is idempotent because: \\[ \\mathbf{I}^2 = \\mathbf{I} \\] Zero Matrix: The zero matrix \\(\\mathbf{0}\\) is also idempotent because: \\[ \\mathbf{0}^2 = \\mathbf{0} \\] Projection Matrix: Consider a projection matrix onto the x-axis in 2D: \\[ \\mathbf{P} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix} \\] This matrix is idempotent since: \\[ \\mathbf{P}^2 = \\mathbf{P} \\] 2.1.9.3 Use in Statistics: Idempotent matrices are commonly used in statistics, particularly in the context of regression analysis. For example, the hat matrix \\(\\mathbf{H}\\) in linear regression, which transforms the observed values into the predicted values, is idempotent: \\[ \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\] where \\(\\mathbf{X}\\) is the design matrix. In summary, idempotent matrices have unique properties and are frequently encountered in linear algebra, projections, and statistical applications. 2.1.10 Determinant of a Matrix The determinant of a matrix is a scalar value that provides important information about the properties of a square matrix. It is a fundamental concept in linear algebra, often used to determine whether a matrix is invertible, as well as to describe geometric transformations and volume scaling. 2.1.10.1 Key Charachteristics of the Determinant: Square Matrices Only: The determinant is only defined for square matrices (i.e., matrices with the same number of rows and columns). Invertibility: A matrix is invertible (i.e., it has an inverse) if and only if its determinant is non-zero. If the determinant is zero, the matrix is singular and does not have an inverse. Geometric Interpretation: The determinant represents the scaling factor of the linear transformation described by the matrix. For example, in two or three dimensions, the determinant tells you how much the matrix scales area or volume: A determinant of 1 means the matrix preserves the area (in 2D) or volume (in 3D). A determinant greater than 1 means the matrix scales the area or volume by that factor. A negative determinant indicates that the transformation also includes a reflection. Significance of Zero Determinant: If the determinant is zero, it means that the matrix maps some vectors to a lower-dimensional space. For instance, in 2D, it might map points onto a line, collapsing the area to zero. 2.1.10.2 Definition: For a 2x2 matrix: \\[ \\mathbf{A} = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix} \\] The determinant of \\(\\mathbf{A}\\) is: \\[ \\det(\\mathbf{A}) = ad - bc \\] This formula gives the area scaling factor for the linear transformation represented by the matrix \\(\\mathbf{A}\\). For a 3x3 matrix: \\[ \\mathbf{B} = \\begin{bmatrix} a &amp; b &amp; c \\\\ d &amp; e &amp; f \\\\ g &amp; h &amp; i \\end{bmatrix} \\] The determinant of \\(\\mathbf{B}\\) is: \\[ \\det(\\mathbf{B}) = a(ei - fh) - b(di - fg) + c(dh - eg) \\] This formula can be extended to higher dimensions using recursive expansion (called cofactor expansion or Laplace expansion). 2.1.10.3 Applications of the Determinant: Solving Linear Systems: The determinant is used in Cramer’s Rule, a method for solving systems of linear equations. If the determinant of the coefficient matrix is non-zero, the system has a unique solution. Eigenvalues and Eigenvectors: The determinant plays a key role in computing the eigenvalues of a matrix. The determinant of a matrix \\(\\mathbf{A} - \\lambda \\mathbf{I}\\), where \\(\\lambda\\) is a scalar and \\(\\mathbf{I}\\) is the identity matrix, gives the characteristic equation whose solutions are the eigenvalues. Volume and Area Calculations: In geometry, the determinant helps calculate the area (in 2D) or volume (in 3D) of a region after applying a linear transformation. Singular Value Decomposition (SVD) and Principal Component Analysis (PCA): The determinant is important in these techniques for understanding data structures, transformations, and dimensionality reduction. 2.1.10.4 Properties of the Determinant For a general block matrix of the form: \\[ \\mathbf{M} = \\begin{bmatrix} \\mathbf{A} &amp; \\mathbf{B} \\\\ \\mathbf{C} &amp; \\mathbf{D} \\end{bmatrix} \\] where \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), \\(\\mathbf{C}\\), and \\(\\mathbf{D}\\) are submatrices, the determinant formula is more complex. If \\(\\mathbf{D}\\) is invertible, we can use the Schur complement to compute the determinant: \\[ \\det(\\mathbf{M}) = \\det(\\mathbf{D}) \\det(\\mathbf{A} - \\mathbf{B} \\mathbf{D}^{-1} \\mathbf{C}) \\] Here, \\(\\mathbf{A} - \\mathbf{B} \\mathbf{D}^{-1} \\mathbf{C}\\) is called the Schur complement of \\(\\mathbf{D}\\) in \\(\\mathbf{M}\\). For any invertible matrix \\( {\\boldsymbol X} \\in {\\mathbb{R}} ^{p \\times p}\\) and matrices \\( {\\boldsymbol A} \\in {\\mathbb{R}} ^{p\\times q}\\) and \\( {\\boldsymbol B} \\in {\\mathbb{R}} ^{q\\times p}\\) we have, that: \\[\\det( {\\boldsymbol X} + {\\boldsymbol A} {\\boldsymbol B} ) = det( {\\boldsymbol X} )\\det( {\\boldsymbol I} _q + {\\boldsymbol B} {\\boldsymbol X} ^{-1} {\\boldsymbol A} ) \\] 2.2 Calculus Key calculus topics include: Gradient Hessian Matrix Calculus Optimization 2.2.1 Gradient The gradient of a function is a vector that contains the partial derivatives of the function with respect to each of its variables. It points in the direction of the steepest ascent of the function, and its magnitude indicates the rate of change in that direction. For a scalar function \\(f(x_1, x_2, \\ldots, x_n)\\), where \\(x_1, x_2, \\ldots, x_n\\) are the variables, the gradient is defined as: \\[ \\nabla f = \\frac{d}{d \\mathbf{x}} f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix} \\] 2.2.1.1 Key Points: Direction: The gradient points in the direction of the greatest increase of the function. Magnitude: The magnitude of the gradient represents how fast the function increases in that direction. Zero Gradient: If \\(\\nabla f = 0\\), it indicates that the function has a critical point, which could be a local minimum, maximum, or saddle point. 2.2.1.2 Example: For a function \\(f(x, y) = x^2 + y^2\\), the gradient is: \\[ \\nabla f = \\begin{bmatrix} \\frac{\\partial}{\\partial x} (x^2 + y^2) \\\\ \\frac{\\partial}{\\partial y} (x^2 + y^2) \\end{bmatrix} = \\begin{bmatrix} 2x \\\\ 2y \\end{bmatrix} \\] This shows that the gradient points outward from the origin, and its magnitude increases as \\(x\\) and \\(y\\) increase. 2.2.1.3 Applications: In optimization, the gradient is used to find the minimum or maximum of a function (e.g., in gradient descent, a common optimization algorithm). In vector calculus, the gradient is used to describe the slope or rate of change of scalar fields (such as temperature, pressure, or altitude in physical applications). 2.2.2 Hessian Matrix The Hessian matrix is a square matrix of second-order partial derivatives of a scalar-valued function. It describes the local curvature of a multivariable function and is used to assess the nature of critical points (i.e., whether they are minima, maxima, or saddle points). For a scalar function \\(f(x_1, x_2, \\ldots, x_n)\\), the Hessian matrix \\( {\\boldsymbol H} \\) is defined as: \\[ {\\boldsymbol H} (f) = \\frac{d}{d \\mathbf{x} d {\\boldsymbol x} &#39;} f(\\mathbf{x}) =\\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_2^2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix} \\] 2.2.2.1 Key Properties: The Hessian is symmetric if the second-order partial derivatives are continuous (by Clairaut’s theorem, also called Schwarz’s theorem). It provides important information about the local behavior of the function, particularly around critical points where the gradient is zero. Eigenvalues of the Hessian matrix determine the type of critical points: If all eigenvalues are positive, the function has a local minimum. If all eigenvalues are negative, the function has a local maximum. If some eigenvalues are positive and others are negative, the function has a saddle point. 2.2.2.2 Example: For a function \\(f(x, y) = x^2 + xy + y^2\\), the Hessian matrix is: \\[ {\\boldsymbol H} (f) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x^2} &amp; \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x} &amp; \\frac{\\partial^2 f}{\\partial y^2} \\end{bmatrix} = \\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 2 \\end{bmatrix} \\] 2.2.3 Applications: In optimization, the Hessian is used to assess the convexity or concavity of a function, which helps in identifying the nature of critical points. In machine learning, it is used to optimize loss functions and can be part of second-order optimization methods like Newton’s method. In economics and engineering, the Hessian helps in analyzing systems involving multiple variables and understanding how they interact with each other. 2.2.4 Matrix Calculus You need to know the following matrix calculus operations: \\[ \\frac{d}{d \\mathbf{x}} \\left( {\\boldsymbol c} &#39; {\\boldsymbol x} \\right) \\] \\[ \\frac{d}{d \\mathbf{x}} \\left( {\\boldsymbol x} &#39; {\\boldsymbol A} {\\boldsymbol x} \\right) \\] \\[ \\frac{d}{d \\mathbf{x} d {\\boldsymbol x} &#39;} \\left( {\\boldsymbol x} &#39; {\\boldsymbol A} {\\boldsymbol x} \\right) \\] Let \\(\\mathbf{c}\\) be a constant vector and \\(\\mathbf{x}\\) be a variable vector, both of size \\(n \\times 1\\). We want to compute the derivative of the product: \\[ f(\\mathbf{x}) = \\mathbf{c}&#39; \\mathbf{x} \\] Where: \\[ \\mathbf{c}&#39; \\mathbf{x} = \\sum_{i=1}^{n} c_i x_i \\] To differentiate \\(f(\\mathbf{x}) = \\mathbf{c}&#39; \\mathbf{x}\\) with respect to the variable vector \\(\\mathbf{x}\\), we take the derivative of each component separately: \\[ \\nabla f = \\frac{d}{d \\mathbf{x}} f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1} (\\mathbf{c}&#39; \\mathbf{x}) \\\\ \\frac{\\partial }{\\partial x_2} (\\mathbf{c}&#39; \\mathbf{x}) \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n} (\\mathbf{c}&#39; \\mathbf{x}) \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1} \\left(\\sum_{i=1}^{n} c_i x_i\\right) \\\\ \\frac{\\partial }{\\partial x_2} \\left(\\sum_{i=1}^{n} c_i x_i\\right) \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n} \\left(\\sum_{i=1}^{n} c_i x_i\\right) \\end{bmatrix} \\] Since \\(\\mathbf{c}\\) is a constant vector, the derivative of each term \\(c_i x_i\\) is simply \\(c_i\\), that is: \\[ \\frac{d}{d x_j} \\left(\\sum_{i=1}^{n} c_i x_i\\right) = c_j \\] Thus, the derivative of the entire sum is the vector: \\[ \\frac{d}{d \\mathbf{x}} \\left( \\mathbf{c}&#39; \\mathbf{x} \\right) = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{bmatrix} = \\mathbf{c} \\] Now, let’s go through the derivative of the quadratic form \\(f(\\mathbf{x}) = \\mathbf{x}&#39; {\\boldsymbol A} \\mathbf{x}\\), where: \\(\\mathbf{x}\\) is a variable vector of size \\(n \\times 1\\), \\( {\\boldsymbol A} \\) is a constant, symmetric matrix of size \\(n \\times n\\). \\[ f(\\mathbf{x}) = \\mathbf{x}&#39; {\\boldsymbol A} \\mathbf{x} \\] First, expand the quadratic form: \\[ f(\\mathbf{x}) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j \\] Then \\[ \\nabla f = \\frac{d}{d \\mathbf{x}} f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1} (\\mathbf{x}&#39; {\\boldsymbol A} \\mathbf{x}) \\\\ \\frac{\\partial }{\\partial x_2} (\\mathbf{x}&#39; {\\boldsymbol A} \\mathbf{x}) \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n} (\\mathbf{x}&#39; {\\boldsymbol A} \\mathbf{x}) \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1} \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j\\right) \\\\ \\frac{\\partial }{\\partial x_2} \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j\\right) \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n} \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j\\right) \\end{bmatrix} \\] For each component \\(x_k\\) in the vector \\(\\mathbf{x}\\), the derivative of \\(f(\\mathbf{x})\\) is: \\[ \\frac{\\partial}{\\partial x_k} f(\\mathbf{x}) = \\frac{\\partial}{\\partial x_k} \\left( \\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j \\right) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\frac{\\partial}{\\partial x_k} x_i a_{ij} x_j \\] Each term \\(x_i a_{ij} x_j\\) has two components that depend on \\( {\\boldsymbol x} \\): If \\(i = j = k\\), the derivative with respect to \\(x_k\\) is: \\[ \\frac{\\partial}{\\partial x_k} (x_i a_{ij} x_j) = 2 a_{kk} x_k \\] If \\(i \\neq j\\) and \\(i = k\\), the derivative with respect to \\(x_k\\) is: \\[ \\frac{\\partial}{\\partial x_k} (x_i a_{ij} x_j) = a_{kj} x_j \\] - Similarly, if \\(i \\neq j\\) and \\(j = k\\), the derivative with respect to \\(x_k\\) is: \\[ \\frac{\\partial}{\\partial x_k} (x_i a_{ij} x_j) = a_{ik} x_i \\] - Finally, if \\(i \\neq k\\) and \\(j \\neq k\\), then: \\[ \\frac{\\partial}{\\partial x_k} (x_i a_{ij} x_j) = 0 \\] Then \\[ \\frac{\\partial}{\\partial x_k} f(\\mathbf{x}) = 2 a_{kk} x_k + \\sum_{i \\neq k} a_{ik} x_i + \\sum_{j \\neq k} a_{kj} x_j \\] Now since \\( {\\boldsymbol A} \\) is symmetric (\\(a_{ij} = a_{ji}\\)), then: \\[\\begin{align*} \\frac{\\partial}{\\partial x_k} f(\\mathbf{x}) &amp;= 2 a_{kk} x_k + \\sum_{i \\neq k} a_{ik} x_i + \\sum_{i \\neq k} a_{ik} x_i \\\\ &amp;= 2 a_{kk} x_k + 2\\sum_{i \\neq k} a_{ik} x_i \\\\ &amp;= 2 \\left(\\sum_{i \\neq k} a_{ik} x_i + a_{kk}x_k \\right) \\\\ &amp;= 2 \\left(\\sum_{i = 1}^n a_{ki} x_i\\right) \\end{align*}\\] Then: \\[ \\nabla f = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1} \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j\\right) \\\\ \\frac{\\partial }{\\partial x_2} \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j\\right) \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n} \\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} x_i a_{ij} x_j\\right) \\end{bmatrix} = \\begin{bmatrix} 2 \\sum_{i = 1}^n a_{1i} x_i \\\\ 2 \\sum_{i = 1}^n a_{2i} x_i \\\\ \\vdots \\\\ 2 \\sum_{i = 1}^n a_{ni} x_i \\end{bmatrix} = 2 {\\boldsymbol A} {\\boldsymbol x} \\] Finally for the second derivative we have that: In general, the Hessian matrix of a scalar function \\(f(\\mathbf{x})\\), where \\(\\mathbf{x} \\in \\mathbb{R}^n\\) is a vector of variables, is a matrix that contains all the second-order partial derivatives of the function. It is defined as: \\[ {\\boldsymbol H} (f) = \\frac{d^2 f}{d {\\boldsymbol x} d {\\boldsymbol x} &#39;} = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_2^2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1}\\left(\\frac{d f}{d {\\boldsymbol x} }\\right)&#39; \\\\ \\frac{\\partial }{\\partial x_2}\\left(\\frac{d f}{d {\\boldsymbol x} }\\right)&#39; \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n}\\left(\\frac{d f}{d {\\boldsymbol x} }\\right)&#39; \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial }{\\partial x_1}\\left(2 {\\boldsymbol A} {\\boldsymbol x} \\right)&#39; \\\\ \\frac{\\partial }{\\partial x_2}\\left(2 {\\boldsymbol A} {\\boldsymbol x} \\right)&#39; \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_n}\\left(2 {\\boldsymbol A} {\\boldsymbol x} \\right)&#39; \\end{bmatrix} \\] Now \\[ \\frac{\\partial }{\\partial x_k}\\left(2 {\\boldsymbol A} {\\boldsymbol x} \\right) = 2\\frac{\\partial }{\\partial x_k}\\begin{bmatrix} \\sum_{i = 1}^n a_{1i} x_i \\\\ \\sum_{i = 1}^n a_{2i} x_i \\\\ \\vdots \\\\ \\sum_{i = 1}^n a_{ni} x_i \\end{bmatrix} = 2 \\begin{bmatrix} \\frac{\\partial }{\\partial x_k} \\left(\\sum_{i = 1}^n a_{1i} x_i \\right)\\\\ \\frac{\\partial }{\\partial x_k} \\left(\\sum_{i = 1}^n a_{2i} x_i \\right)\\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_k} \\left(\\sum_{i = 1}^n a_{ni} x_i \\right) \\end{bmatrix} = 2 \\begin{bmatrix} a_{k1} \\\\ a_{k2} \\\\ \\vdots \\\\ a_{kn} \\end{bmatrix} \\] Then \\[ {\\boldsymbol H} (f) = \\frac{d^2 f}{d {\\boldsymbol x} d {\\boldsymbol x} &#39;} = \\begin{bmatrix} 2\\begin{bmatrix} a_{11} \\\\ a_{12} \\\\ \\vdots \\\\ a_{1n} \\end{bmatrix}&#39; \\\\ 2\\begin{bmatrix} a_{21} \\\\ a_{22} \\\\ \\vdots \\\\ a_{2n} \\end{bmatrix}&#39; \\\\ \\vdots \\\\ 2\\begin{bmatrix} a_{n1} \\\\ a_{n2} \\\\ \\vdots \\\\ a_{nn} \\end{bmatrix}&#39; \\end{bmatrix} = 2\\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{nn} \\end{bmatrix} = 2 {\\boldsymbol A} \\] 2.3 Probability Key probability concepts to understand include: Expected Value Variance Cross-Covariance Matrix Multivariate Normal Distribution 2.3.1 Expected Value The expected value (or mean) of a random vector is a fundamental concept in multivariate statistics. Just as the expected value of a random variable provides a measure of the “center” or “average” of the distribution, the expected value of a random vector captures the central location of a multidimensional distribution. 2.3.1.1 Definition Let \\(\\mathbf{X}\\) be a random vector in \\(\\mathbb{R}^n\\), represented as: \\[ \\mathbf{X} = \\begin{bmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{bmatrix}, \\] where \\(X_1, X_2, \\dots, X_n\\) are its components. The expected value of \\(\\mathbf{X}\\), denoted by \\(\\mathbb{E}[\\mathbf{X}]\\), is defined as the vector of the expected values of each component: \\[ \\mathbb{E}[\\mathbf{X}] = \\begin{bmatrix} \\mathbb{E}[X_1] \\\\ \\mathbb{E}[X_2] \\\\ \\vdots \\\\ \\mathbb{E}[X_n] \\end{bmatrix}. \\] This vector \\(\\mathbb{E}[\\mathbf{X}]\\) is also called the mean vector of \\(\\mathbf{X}\\). 2.3.1.2 Key Properties of the Expected Value of a Random Vector Linearity: For any scalar \\(a\\) and random vectors \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\), \\[ \\mathbb{E}[a\\mathbf{X} + \\mathbf{Y}] = a\\mathbb{E}[\\mathbf{X}] + \\mathbb{E}[\\mathbf{Y}]. \\] Expectation of a Constant Vector: If \\(\\mathbf{c}\\) is a constant vector in \\(\\mathbb{R}^n\\), then the expectation is simply the vector itself: \\[ \\mathbb{E}[\\mathbf{c}] = \\mathbf{c}. \\] Expectation of a Linear Transformation: If \\(\\mathbf{A}\\) is an \\(m \\times n\\) constant matrix, then for a random vector \\(\\mathbf{X}\\) in \\(\\mathbb{R}^n\\), \\[ \\mathbb{E}[\\mathbf{A} \\mathbf{X}] = \\mathbf{A} \\mathbb{E}[\\mathbf{X}]. \\] 2.3.2 Variance 2.3.2.1 Definition Let \\(\\mathbf{X}\\) be a random vector in \\(\\mathbb{R}^n\\), represented as: \\[ \\mathbf{X} = \\begin{bmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{bmatrix}, \\] where each \\(X_i\\) is a random variable. The variance-covariance matrix of \\(\\mathbf{X}\\), denoted by \\( {\\mathbb{V}} (\\mathbf{X})\\) or \\(\\boldsymbol{\\Sigma}\\), is defined as: \\[ {\\mathbb{V}} (\\mathbf{X}) = \\mathbb{E}[(\\mathbf{X} - \\mathbb{E}[\\mathbf{X}])(\\mathbf{X} - \\mathbb{E}[\\mathbf{X}])&#39;]. \\] The \\((i, j)\\) entry of \\( {\\mathbb{V}} (\\mathbf{X})\\) is given by \\( {\\mathbb{C}} (X_i, X_j)\\), representing the covariance between components \\(X_i\\) and \\(X_j\\). If \\(\\mathbf{X}\\) has \\(n\\) components, \\( {\\mathbb{V}} (\\mathbf{X})\\) will be an \\(n \\times n\\) symmetric matrix where: - Diagonal entries represent the variances of each component, i.e., \\( {\\mathbb{V}} (X_i) = {\\mathbb{C}} (X_i, X_i)\\). - Off-diagonal entries represent the covariances between different components, i.e., \\( {\\mathbb{C}} (X_i, X_j)\\) for \\(i \\neq j\\). 2.3.2.2 Key Properties of the Variance-Covariance Matrix Symmetry: The variance-covariance matrix is symmetric, meaning: \\[ {\\mathbb{V}} (\\mathbf{X}) = {\\mathbb{V}} (\\mathbf{X})^T. \\] Non-negativity: The variance-covariance matrix is positive semi-definite, which implies: \\[ \\mathbf{z}^T {\\mathbb{V}} (\\mathbf{X}) \\mathbf{z} \\geq 0 \\quad \\text{for any vector } \\mathbf{z} \\in \\mathbb{R}^n. \\] This property indicates that variances (the diagonal entries) are always non-negative. Variance of a Linear Transformation: If \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix, then the variance of the transformed random vector \\(\\mathbf{A} \\mathbf{X}\\) is given by: \\[ {\\mathbb{V}} (\\mathbf{A} \\mathbf{X}) = \\mathbf{A} \\, {\\mathbb{V}} (\\mathbf{X}) \\, \\mathbf{A}^T. \\] Variance of Independent Random Variables: If the components of \\(\\mathbf{X}\\) are mutually independent, the off-diagonal entries of \\( {\\mathbb{V}} (\\mathbf{X})\\) (the covariances) are zero, resulting in a diagonal covariance matrix. Variance of the Sum of Two Arbitrary Random Vectors: If \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) are two arbitrary random vectors in \\(\\mathbb{R}^n\\), the variance of their sum is given by: \\[ {\\mathbb{V}} (\\mathbf{X} + \\mathbf{Y}) = {\\mathbb{V}} (\\mathbf{X}) + {\\mathbb{V}} (\\mathbf{Y}) + 2 \\, {\\mathbb{C}} (\\mathbf{X}, \\mathbf{Y}), \\] where \\( {\\mathbb{C}} (\\mathbf{X}, \\mathbf{Y})\\) is the cross-covariance matrix between \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\): \\[ {\\mathbb{C}} (\\mathbf{X}, \\mathbf{Y}) = \\mathbb{E}\\left[(\\mathbf{X} - \\mathbb{E}[\\mathbf{X}])(\\mathbf{Y} - \\mathbb{E}[\\mathbf{Y}])&#39;\\right]. \\] Variance of the Sum of Two Independent Random Vectors: If \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) are two independent random vectors in \\(\\mathbb{R}^n\\), then the variance of their sum is given by the sum of their individual variances: \\[ {\\mathbb{V}} (\\mathbf{X} + \\mathbf{Y}) = {\\mathbb{V}} (\\mathbf{X}) + {\\mathbb{V}} (\\mathbf{Y}). \\] Since \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) are independent, their covariance \\( {\\mathbb{C}} (\\mathbf{X}, \\mathbf{Y})\\) is zero, simplifying the variance of the sum. Trace of the Variance: A related measure of the variance of a random vector \\(\\mathbf{X}\\), is given by \\[ \\text{tr}( {\\mathbb{V}} [\\mathbf{X}]) = \\mathbb{E}[(\\mathbf{X} - \\mathbb{E}[\\mathbf{X}])&#39;(\\mathbf{X} - \\mathbb{E}[\\mathbf{X}])] \\] 2.3.3 Cross-Covariance Matrix The cross-covariance matrix measures the covariance between two random vectors, providing insights into the linear relationships between different components of these vectors. Given two random vectors \\(\\mathbf{X} \\in \\mathbb{R}^n\\) and \\(\\mathbf{Y} \\in \\mathbb{R}^m\\), the cross-covariance matrix between \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) captures how each component of \\(\\mathbf{X}\\) varies with each component of \\(\\mathbf{Y}\\). 2.3.3.1 Definition Let \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) be two random vectors with mean vectors \\(\\mathbb{E}[\\mathbf{X}] = \\boldsymbol{\\mu}_{\\mathbf{X}}\\) and \\(\\mathbb{E}[\\mathbf{Y}] = \\boldsymbol{\\mu}_{\\mathbf{Y}}\\). The cross-covariance matrix of \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) is defined as: \\[ {\\mathbb{C}} (\\mathbf{X}, \\mathbf{Y}) = \\mathbb{E}[(\\mathbf{X} - \\boldsymbol{\\mu}_{\\mathbf{X}})(\\mathbf{Y} - \\boldsymbol{\\mu}_{\\mathbf{Y}})^T]. \\] This matrix is of dimension \\(n \\times m\\), where each element \\(( {\\mathbb{C}} (\\mathbf{X}, \\mathbf{Y}))_{ij}\\) represents the covariance between the \\(i\\)-th component of \\(\\mathbf{X}\\) and the \\(j\\)-th component of \\(\\mathbf{Y}\\): \\[ ( {\\mathbb{C}} (\\mathbf{X}, \\mathbf{Y}))_{ij} = {\\mathbb{C}} (X_i, Y_j) = \\mathbb{E}[(X_i - \\mu_{X_i})(Y_j - \\mu_{Y_j})]. \\] 2.3.3.2 Key Properties Symmetry in Covariance: If \\(\\mathbf{X} = \\mathbf{Y}\\), then \\( {\\mathbb{C}} (\\mathbf{X}, \\mathbf{Y})\\) reduces to the covariance matrix of \\(\\mathbf{X}\\), which is symmetric. For distinct vectors \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\), the cross-covariance matrix \\( {\\mathbb{C}} (\\mathbf{X}, \\mathbf{Y})\\) is generally not symmetric. Relationship with Joint Covariance Matrix: If \\(\\mathbf{Z} = \\begin{bmatrix} \\mathbf{X} \\\\ \\mathbf{Y} \\end{bmatrix}\\) is a combined random vector, then the covariance matrix of \\(\\mathbf{Z}\\) is: \\[ {\\mathbb{C}} (\\mathbf{Z}) = \\begin{bmatrix} {\\mathbb{C}} (\\mathbf{X}) &amp; {\\mathbb{C}} (\\mathbf{X}, \\mathbf{Y}) \\\\ {\\mathbb{C}} (\\mathbf{Y}, \\mathbf{X}) &amp; {\\mathbb{C}} (\\mathbf{Y}) \\end{bmatrix}. \\] Linearity: If \\(a\\) and \\(b\\) are constants and \\(\\mathbf{X}_1\\) and \\(\\mathbf{X}_2\\) are random vectors of the same dimension as \\(\\mathbf{X}\\), then: \\[ {\\mathbb{C}} (a\\mathbf{X}_1 + b\\mathbf{X}_2, \\mathbf{Y}) = a {\\mathbb{C}} (\\mathbf{X}_1, \\mathbf{Y}) + b {\\mathbb{C}} (\\mathbf{X}_2, \\mathbf{Y}). \\] Furthermore, if \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are matrices of compatible dimensions, and \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) are random vectors then the cross-covariance of \\(\\mathbf{A}\\mathbf{X}\\) and \\(\\mathbf{B}\\mathbf{Y}\\) is given by: \\[ {\\mathbb{C}} (\\mathbf{A}\\mathbf{X}, \\mathbf{B}\\mathbf{Y}) = \\mathbf{A} {\\mathbb{C}} (\\mathbf{X}, \\mathbf{Y}) \\mathbf{B}^T. \\] Zero Cross-Covariance and Independence: If \\( {\\mathbb{C}} (\\mathbf{X}, \\mathbf{Y}) = \\mathbf{0}\\), it implies that \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) are uncorrelated, but it does not necessarily imply independence unless \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) are jointly normally distributed. 2.3.4 Multivariate Normal Distribution The multivariate normal distribution generalizes the concept of the normal distribution to multiple dimensions, describing the behavior of random vectors whose elements are jointly normally distributed. It is widely used in statistics and machine learning due to its well-behaved properties and its applicability to modeling correlations between variables. 2.3.4.1 Definition A random vector \\(\\mathbf{X} = (X_1, X_2, \\dots, X_n)^T\\) is said to follow a multivariate normal distribution if it has a probability density function of the form: \\[ f(\\mathbf{X}) = \\frac{1}{(2\\pi)^{n/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{X} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{X} - \\boldsymbol{\\mu}) \\right), \\] where: \\(\\mathbf{X} \\in \\mathbb{R}^n\\) is the random vector, \\(\\boldsymbol{\\mu} \\in \\mathbb{R}^n\\) is the mean vector, \\(\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{n \\times n}\\) is the covariance matrix, assumed to be symmetric and positive definite. This distribution is denoted as \\(\\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\). 2.3.4.2 Key Properties Marginal Distributions: Any subset of the components of \\(\\mathbf{X}\\) is also normally distributed. If \\(\\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), then any partition of \\(\\mathbf{X}\\) results in a marginal distribution that is also multivariate normal. Affine Transformation: For a matrix \\(\\mathbf{A}\\) and vector \\(\\mathbf{b}\\) of appropriate dimensions, the affine transformation \\(\\mathbf{Y} = \\mathbf{A}\\mathbf{X} + \\mathbf{b}\\) also follows a multivariate normal distribution, \\(\\mathbf{Y} \\sim \\mathcal{N}(\\mathbf{A} \\boldsymbol{\\mu} + \\mathbf{b}, \\mathbf{A} \\boldsymbol{\\Sigma} \\mathbf{A}^T)\\). Conditional Distributions: For a partitioned vector \\(\\mathbf{X} = (\\mathbf{X}_1, \\mathbf{X}_2)^T\\), with corresponding partitions of the mean vector and covariance matrix: \\[ \\boldsymbol{\\mu} = \\begin{pmatrix} \\boldsymbol{\\mu}_1 \\\\ \\boldsymbol{\\mu}_2 \\end{pmatrix}, \\quad \\boldsymbol{\\Sigma} = \\begin{pmatrix} \\boldsymbol{\\Sigma}_{11} &amp; \\boldsymbol{\\Sigma}_{12} \\\\ \\boldsymbol{\\Sigma}_{21} &amp; \\boldsymbol{\\Sigma}_{22} \\end{pmatrix}, \\] the conditional distribution \\(\\mathbf{X}_1 | \\mathbf{X}_2 = \\mathbf{x}_2\\) is also multivariate normal: \\[ \\mathbf{X}_1 | \\mathbf{X}_2 = \\mathbf{x}_2 \\sim \\mathcal{N}(\\boldsymbol{\\mu}_1 + \\boldsymbol{\\Sigma}_{12} \\boldsymbol{\\Sigma}_{22}^{-1} (\\mathbf{x}_2 - \\boldsymbol{\\mu}_2), \\boldsymbol{\\Sigma}_{11} - \\boldsymbol{\\Sigma}_{12} \\boldsymbol{\\Sigma}_{22}^{-1} \\boldsymbol{\\Sigma}_{21}). \\] Independence and Uncorrelatedness: For a multivariate normal distribution, zero covariance implies independence. If two components \\(X_i\\) and \\(X_j\\) (or two subvectors) have a covariance of zero, they are independent. Moment Generating Function: The moment generating function of \\(\\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) is: \\[ M_{\\mathbf{X}}(\\mathbf{t}) = \\exp\\left( \\mathbf{t}^T \\boldsymbol{\\mu} + \\frac{1}{2} \\mathbf{t}^T \\boldsymbol{\\Sigma} \\mathbf{t} \\right). \\] Maximum Entropy: Among all distributions with a given mean vector and covariance matrix, the multivariate normal distribution has the maximum entropy, making it the most “uninformative” or spread out. 2.3.5 \\(\\chi^2\\) Distribution The chi-squared distribution is a probability distribution that arises naturally in statistics, particularly in hypothesis testing and estimation problems. It is defined as the distribution of a sum of the squares of independent standard normal random variables. Formally, let \\(Z_1, Z_2, \\dots, Z_k\\) be \\(k\\) independent random variables, each following a standard normal distribution (mean \\(0\\) and variance \\(1\\)). Then, the sum of their squares: \\[ Q = Z_1^2 + Z_2^2 + \\cdots + Z_k^2 \\] follows a chi-squared distribution with \\(k\\) degrees of freedom. We denote this as: \\[ Q \\sim \\chi^2_k \\] 2.3.5.1 Key Properties of the Chi-Squared Distribution Degrees of Freedom (\\(k\\)): The parameter \\(k\\) determines the shape of the distribution. Larger \\(k\\) values result in a distribution that becomes more symmetric and approaches a normal distribution (as \\(k \\to \\infty\\)). Mean and Variance: Mean: \\(\\mathbb{E}[Q] = k\\) Variance: \\(\\mathbb{V}[Q] = 2k\\) Additivity: If \\(Q_1 \\sim \\chi^2_{k_1}\\) and \\(Q_2 \\sim \\chi^2_{k_2}\\) are independent, then \\(Q_1 + Q_2 \\sim \\chi^2_{k_1 + k_2}\\). Special Case: For \\(k = 1\\), the chi-squared distribution is simply the square of a standard normal variable, \\(Q = Z^2\\). The chi-squared distribution is widely used in statistical tests, such as the chi-squared test for independence or goodness-of-fit, and in the construction of confidence intervals for variances. 2.3.6 \\(t\\) Distribution The t-distribution, also known as Student’s t-distribution, is a probability distribution that arises frequently in hypothesis testing, particularly when the sample size is small, and the population standard deviation is unknown. It is defined as the distribution of a standard normal random variable divided by the square root of an independent chi-squared random variable, scaled by its degrees of freedom. 2.3.6.1 Definition Let: - \\(Z\\) be a standard normal random variable (\\(Z \\sim N(0, 1)\\)), - \\(Q\\) be a chi-squared random variable with \\(k\\) degrees of freedom (\\(Q \\sim \\chi^2_k\\)), independent of \\(Z\\). Then, the random variable: \\[ T = \\frac{Z}{\\sqrt{\\frac{Q}{k}}} \\] follows a t-distribution with \\(k\\) degrees of freedom. We write: \\[ T \\sim t_k \\] 2.3.6.2 Key Properties of the t-Distribution Degrees of Freedom (\\(k\\)): The parameter \\(k\\) determines the shape of the t-distribution. As \\(k\\) increases, the t-distribution approaches the standard normal distribution. Symmetry: The t-distribution is symmetric about zero, similar to the normal distribution. Tails: The t-distribution has heavier tails than the normal distribution, meaning it gives more probability to extreme values. This reflects increased uncertainty when estimating the population mean with small sample sizes. Moments: Mean: \\(\\mathbb{E}[T] = 0\\) for \\(k &gt; 1\\) Variance: \\(\\mathbb{V}[T] = \\frac{k}{k-2}\\) for \\(k &gt; 2\\) Higher moments exist only for \\(k &gt; m\\), where \\(m\\) is the order of the moment. 2.3.6.3 Applications The t-distribution is widely used in: 1. t-tests for hypothesis testing, such as testing means of small samples. 2. Constructing confidence intervals for a population mean when the population standard deviation is unknown. 3. Regression analysis, where it appears in tests for regression coefficients. The t-distribution plays a fundamental role in statistics, bridging the gap between small-sample and large-sample inference. 2.3.7 \\(F\\) Distribution The F-distribution, also known as Fisher-Snedecor distribution, arises frequently in statistics, especially in hypothesis testing and variance analysis (ANOVA). It is defined as the distribution of the ratio of two independent chi-squared random variables, each divided by their respective degrees of freedom. 2.3.7.1 Definition Let: - \\(Q_1 \\sim \\chi^2_{k_1}\\), a chi-squared random variable with \\(k_1\\) degrees of freedom, - \\(Q_2 \\sim \\chi^2_{k_2}\\), a chi-squared random variable with \\(k_2\\) degrees of freedom, - \\(Q_1\\) and \\(Q_2\\) are independent. Then, the random variable: \\[ F = \\frac{\\frac{Q_1}{k_1}}{\\frac{Q_2}{k_2}} \\] follows an F-distribution with \\(k_1\\) and \\(k_2\\) degrees of freedom. We write: \\[ F \\sim F(k_1, k_2) \\] 2.3.7.2 Key Properties of the F-Distribution Degrees of Freedom: The parameters \\(k_1\\) and \\(k_2\\) determine the shape of the F-distribution. \\(k_1\\) (numerator degrees of freedom) is associated with the variability of the first chi-squared variable. \\(k_2\\) (denominator degrees of freedom) is associated with the variability of the second chi-squared variable. Support: \\(F\\) is defined for \\(F \\geq 0\\). Asymmetry: The F-distribution is not symmetric; it is skewed to the right, especially for small degrees of freedom. As \\(k_1\\) and \\(k_2\\) increase, it approaches a normal distribution. Mean: \\(\\mathbb{E}[F] = \\frac{k_2}{k_2 - 2}\\) for \\(k_2 &gt; 2\\). Variance: \\(\\mathbb{V}[F] = \\frac{2k_2^2 (k_1 + k_2 - 2)}{k_1 (k_2 - 2)^2 (k_2 - 4)}\\) for \\(k_2 &gt; 4\\). Special Cases: When \\(k_1 = 1\\), the F-distribution is equivalent to the square of a t-distribution with \\(k_2\\) degrees of freedom: \\(F(1, k_2) = t_{k_2}^2\\). 2.3.7.3 Applications ANOVA (Analysis of Variance): The F-statistic is used to test whether multiple groups have the same variance or means. Model Comparison: In regression, the F-distribution is used to compare nested models, assessing whether additional predictors improve the fit of the model. Hypothesis Testing: The F-distribution appears in tests of equality of variances (Levene’s test or Bartlett’s test). The F-distribution is essential in statistics for comparing variances and testing model adequacy, making it a cornerstone of many inferential procedures. 2.4 Statistics Essential statistical concepts include: Bias of an Estimator Unbiased Estimator Mean Square Error of an Estimator Consistent Minimum Variance Interval Estimation Hypothesis Testing 2.4.1 Bias of an Estimator The bias of an estimator \\(\\hat{\\mathbf{\\theta}}\\) for a vector parameter \\(\\mathbf{\\theta}\\) is defined as the difference between the expected value of the estimator and the true value of the parameter. Formally, if \\(\\mathbf{\\theta} \\in \\mathbb{R}^n\\) is a vector parameter, then the bias of the estimator \\(\\hat{\\mathbf{\\theta}}\\) is: \\[ \\text{Bias}(\\hat{\\mathbf{\\theta}}) = \\mathbb{E}[\\hat{\\mathbf{\\theta}}] - \\mathbf{\\theta}. \\] 2.4.1.1 Key Points Interpretation: Bias measures how far, on average, the estimator \\(\\hat{\\mathbf{\\theta}}\\) is from the true parameter \\(\\mathbf{\\theta}\\). If the bias is zero, the estimator is said to be unbiased. Component-wise Bias: For each component of the estimator \\(\\hat{\\mathbf{\\theta}} = (\\hat{\\theta}_1, \\dots, \\hat{\\theta}_n)^T\\), the bias can be expressed as: \\[ \\text{Bias}(\\hat{\\theta}_i) = \\mathbb{E}[\\hat{\\theta}_i] - \\theta_i, \\quad \\text{for each } i = 1, \\dots, n. \\] This component-wise breakdown is helpful for understanding how each part of the vector estimator deviates from the true values. Total Bias: The total bias can be viewed as a vector, summarizing the systematic error in each dimension of the estimator. 2.4.1.2 Example In practice, if \\(\\hat{\\mathbf{\\theta}}\\) is the sample mean vector of a random vector \\(\\mathbf{X}\\), and \\(\\mathbf{\\theta} = \\mathbb{E}[\\mathbf{X}]\\), then \\(\\text{Bias}(\\hat{\\mathbf{\\theta}})\\) will be zero, making the sample mean an unbiased estimator of the population mean. 2.4.2 Unbiased Estimator An unbiased estimator of a vector parameter is an estimator that, on average, accurately estimates the true value of the parameter. Formally, let \\(\\mathbf{\\theta} \\in \\mathbb{R}^n\\) be a vector parameter of interest, and let \\(\\hat{\\mathbf{\\theta}}\\) be an estimator of \\(\\mathbf{\\theta}\\). Then, \\(\\hat{\\mathbf{\\theta}}\\) is an unbiased estimator of \\(\\mathbf{\\theta}\\) if the following condition holds: \\[ \\mathbb{E}[\\hat{\\mathbf{\\theta}}] = \\mathbf{\\theta} \\quad \\forall {\\boldsymbol \\tau}\\in {\\mathbb{R}} ^n. \\] 2.4.2.1 Key Points Component-wise Unbiasedness: Each component of \\(\\hat{\\mathbf{\\theta}}\\), say \\(\\hat{\\theta}_i\\), must be an unbiased estimator of the corresponding component \\(\\theta_i\\) of \\(\\mathbf{\\theta}\\): \\[ \\mathbb{E}[\\hat{\\theta}_i] = \\theta_i, \\quad \\text{for all } i = 1, \\dots, n. \\] No Systematic Bias: Unbiasedness implies that, on average, the estimator neither overestimates nor underestimates the true value of \\(\\mathbf{\\theta}\\) across repeated sampling. Applications: Unbiased estimators are crucial in statistics, as they provide estimations that are theoretically centered around the true parameter values, though they may vary due to sampling variation. An example is the sample mean \\(\\mathbf{\\bar{X}}\\) as an estimator for the population mean \\(\\mathbf{\\mu}\\) of a random vector \\(\\mathbf{X}\\), where \\(\\mathbb{E}[\\mathbf{\\bar{X}}] = \\mathbf{\\mu}\\). 2.4.3 Mean Square Error of an Estimator The Mean Square Error (MSE) of an estimator \\(\\hat{\\mathbf{\\theta}}\\) for a vector parameter \\(\\mathbf{\\theta}\\) measures the average squared deviation of the estimator from the true parameter. Formally, for a parameter vector \\(\\mathbf{\\theta} \\in \\mathbb{R}^n\\) and an estimator \\(\\hat{\\mathbf{\\theta}}\\), the MSE is defined as: \\[ {\\mathbb{M}} (\\hat{\\mathbf{\\theta}}) = \\mathbb{E} \\left[ \\|\\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}\\|^2 \\right], \\] where \\(\\|\\cdot\\|\\) denotes the Euclidean (or 2-norm) of a vector. Expanding this expression, we can write: \\[ {\\mathbb{M}} (\\hat{\\mathbf{\\theta}}) = \\mathbb{E} \\left[ (\\hat{\\mathbf{\\theta}} - \\mathbf{\\theta})^T (\\hat{\\mathbf{\\theta}} - \\mathbf{\\theta}) \\right]. \\] 2.4.3.1 Key Components The MSE can be decomposed into two main parts: variance and squared bias. \\[ {\\mathbb{M}} (\\hat{\\mathbf{\\theta}}) = \\text{tr}(\\text{Var}(\\hat{\\mathbf{\\theta}})) + \\|\\text{Bias}(\\hat{\\mathbf{\\theta}})\\|^2, \\] where: - Variance: \\(\\text{Var}(\\hat{\\mathbf{\\theta}}) = \\mathbb{E}[(\\hat{\\mathbf{\\theta}} - \\mathbb{E}[\\hat{\\mathbf{\\theta}}])(\\hat{\\mathbf{\\theta}} - \\mathbb{E}[\\hat{\\mathbf{\\theta}}])^T]\\), representing the variability in the estimator. - Bias: \\(\\text{Bias}(\\hat{\\mathbf{\\theta}}) = \\mathbb{E}[\\hat{\\mathbf{\\theta}}] - \\mathbf{\\theta}\\), representing the systematic deviation from the true parameter. 2.4.3.2 Key Properties Unbiased Estimator: If \\(\\hat{\\mathbf{\\theta}}\\) is unbiased, then \\(\\text{Bias}(\\hat{\\mathbf{\\theta}}) = 0\\), and the MSE is simply the trace of the covariance matrix of \\(\\hat{\\mathbf{\\theta}}\\): \\[ {\\mathbb{M}} (\\hat{\\mathbf{\\theta}}) = \\text{tr}(\\text{Var}(\\hat{\\mathbf{\\theta}})). \\] Bias-Variance Trade-off: The MSE quantifies the balance between bias and variance. Lowering bias often increases variance and vice versa, a key concept in statistical estimation. Overall Error Metric: The MSE provides a comprehensive measure of the estimator’s accuracy, taking both variance and bias into account, making it an essential criterion in evaluating estimators. "],["introduction.html", "3 Introduction 3.1 Key Challenges 3.2 Core Concepts 3.3 Applications", " 3 Introduction High-dimensional multivariate statistics deals with the analysis of data where both the number of variables (\\(p\\)) and the number of observations (\\(n\\)) can be large, and often, \\(p\\) is comparable to or even greater than \\(n\\), making traditional methods unavailable or computationally impractical. This setting arises naturally in modern applications like genomics, finance, and machine learning, where data sets contain hundreds or thousands of variables. 3.1 Key Challenges Curse of Dimensionality: As \\(p\\) grows, classical statistical methods (e.g., ordinary least squares, classical covariance estimation) break down, become unstable or are computationally expensive. Multicollinearity: High correlation between variables can lead to singular or nearly singular covariance matrices. As the number of variables increases this scenario becomes more likely, if \\(p \\gg n\\) this have to be the case necessarily. Overfitting: When \\(p \\gg n\\), models tend to fit noise rather than signal. 3.2 Core Concepts Regularized Estimation: Methods like ridge regression, LASSO, and graphical models introduce constraints to stabilize estimation. Dimensionality Reduction: Techniques like Principal Component Analysis (PCA) and Factor Analysis help summarize information in fewer dimensions. High-Dimensional Covariance and Precision Matrices: Classical estimators (e.g., sample covariance) fail when \\(p &gt; n\\), requiring alternatives like shrinkage estimators or sparsity inducing approaches. Multiple Testing and False Discovery Rate (FDR): In high-dimensional settings, multiple hypothesis tests lead to inflated error rates, necessitating corrections like the Benjamini-Hochberg procedure. 3.3 Applications Genomics: Identifying genes associated with diseases from thousands of genetic markers. Finance: Portfolio optimization where the number of assets is large relative to available data. Machine Learning: Feature selection and model regularization in predictive modeling. High-dimensional statistics continues to evolve, with ongoing research in areas like robust estimation, Bayesian methods, and deep learning applications. As more computational power and data becomes availale, methods that were considered High-Dimensional can be approached with traditional methods. "],["linear-regression.html", "4 Linear Regression 4.1 Machine Learning 4.2 Bayesian Linear Regression 4.3 Computational Comparisson 4.4 Efficient Computation", " 4 Linear Regression 4.1 Machine Learning In this section, we will not make any probability assumption, and we will treat the problem only as an optimization problem. 4.1.1 Ordinary Least Squares 4.1.1.1 Model Specification Let \\(y_i \\in \\mathbb{R}\\) be the response variable and \\(x_i \\in \\mathbb{R}^p\\) be the vector of predictors for observation \\(i\\), where \\(i = 1, \\dots, n\\). The multiple linear regression model is given by: \\[ {\\boldsymbol y} = {\\boldsymbol X} {\\boldsymbol \\beta} + {\\boldsymbol e} \\] where: - \\( {\\boldsymbol y} \\in \\mathbb{R}^{n}\\) is the response vector (each entry corresponds to an observation), - \\( {\\boldsymbol X} \\in \\mathbb{R}^{n \\times p}\\) is the design matrix (including predictors, typically with an intercept column of ones), - \\( {\\boldsymbol \\beta} \\in \\mathbb{R}^{p}\\) is the coefficient vector to be estimated, - \\( {\\boldsymbol e} \\in \\mathbb{R}^{n}\\) is the error vector. 4.1.1.2 Minimization Problem The objective is to minimize the sum of squared errors (SSE): \\[ \\min_{ {\\boldsymbol \\beta} } \\mathcal{L}( {\\boldsymbol \\beta} ) = \\min_{ {\\boldsymbol \\beta} } \\| {\\boldsymbol y} - {\\boldsymbol X} {\\boldsymbol \\beta} \\|_2^2. \\] Expanding the loss function: \\[ \\mathcal{L}( {\\boldsymbol \\beta} ) = ( {\\boldsymbol y} - {\\boldsymbol X} {\\boldsymbol \\beta} )&#39; ( {\\boldsymbol y} - {\\boldsymbol X} {\\boldsymbol \\beta} ). \\] 4.1.1.3 Solution To minimize \\(\\mathcal{L}(\\beta)\\), we take the derivative with respect to \\(\\beta\\): \\[ \\frac{\\partial \\mathcal{L}( {\\boldsymbol \\beta} )}{\\partial {\\boldsymbol \\beta} } = -2 {\\boldsymbol X} &#39; ( {\\boldsymbol y} - {\\boldsymbol X} {\\boldsymbol \\beta} ). \\] Setting this to zero, we obtain the normal equation: \\[ {\\boldsymbol X} &#39; {\\boldsymbol X} {\\boldsymbol \\beta} = {\\boldsymbol X} &#39; {\\boldsymbol y} . \\] If \\( {\\boldsymbol X} &#39; {\\boldsymbol X} \\) is invertible (i.e., \\( {\\boldsymbol X} \\) has full column rank), we solve for \\( {\\boldsymbol \\beta} \\): \\[ {\\boldsymbol \\beta} = ( {\\boldsymbol X} &#39; {\\boldsymbol X} )^{-1} {\\boldsymbol X} &#39; {\\boldsymbol y} . \\] To check that this is a minimizer, we compute the Hessian of \\(\\mathcal{L}( {\\boldsymbol \\beta} )\\): \\[ H = \\frac{\\partial^2 \\mathcal{L}( {\\boldsymbol \\beta} )}{\\partial {\\boldsymbol \\beta} \\partial {\\boldsymbol \\beta} &#39;} = 2 {\\boldsymbol X} &#39; {\\boldsymbol X} . \\] Since \\( {\\boldsymbol X} &#39; {\\boldsymbol X} \\) is positive semidefinite and positive definite if \\( {\\boldsymbol X} \\) has full column rank, the function \\(\\mathcal{L}( {\\boldsymbol \\beta} )\\) is convex. Hence, the critical point \\( {\\boldsymbol \\beta} = ( {\\boldsymbol X} &#39; {\\boldsymbol X} )^{-1} {\\boldsymbol X} &#39; {\\boldsymbol y} \\) is the unique global minimum. The OLS solution is often denoted as: \\[ \\boldsymbol{\\beta}_{\\text{OLS}} = (\\mathbf{X}&#39;\\mathbf{X})^{-1} \\mathbf{X}&#39;\\mathbf{y}. \\] Notice that we noted that \\( {\\boldsymbol X} \\) is full column rank, when this condition is not met (or is close to not be met), other approaches are necessary. 4.1.2 Ridge Regression 4.1.2.1 Introduction When the design matrix \\(\\mathbf{X}\\) is not full rank, the matrix \\(\\mathbf{X}&#39;\\mathbf{X}\\) is singular (i.e., not invertible), making it impossible to compute the least squares solution \\[ \\boldsymbol{\\beta}_{\\text{OLS}} = (\\mathbf{X}&#39;\\mathbf{X})^{-1} \\mathbf{X}&#39;\\mathbf{y}. \\] This issue arises when there are more predictors than observations (\\(p &gt; n\\)) or when there is multicollinearity among the predictors. To address this, Ridge Regression introduces a small positive penalty term that regularizes the matrix \\(\\mathbf{X}&#39;\\mathbf{X}\\), making it invertible. This method is also known as Tikhonov regularization or \\(L_2\\) regularization. When \\(p &gt; n\\) then we can approximate \\(\\mathbf{X}&#39;\\mathbf{X}\\) with \\(\\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I}\\), where \\(\\lambda &gt; 0\\) can be as small as necessary. This approximation is helpful since \\(\\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I}\\) is always non-singular. 4.1.2.2 Non-singularity of the Approximation Let \\(\\mathbf{X}\\) be an \\(n \\times p\\) matrix. Its SVD decomposition is \\[ \\mathbf{X} = \\mathbf{U} \\mathbf{D} \\mathbf{V}&#39; \\] where: - \\(\\mathbf{U} \\in \\mathbb{R}^{n \\times n}\\) is an orthonormal matrix (\\(\\mathbf{U}&#39; \\mathbf{U} = \\mathbf{I}_n\\)), - \\(\\mathbf{V} \\in \\mathbb{R}^{p \\times p}\\) is an orthonormal matrix (\\(\\mathbf{V}&#39; \\mathbf{V} = \\mathbf{I}_p\\)), - \\(\\mathbf{D} \\in \\mathbb{R}^{n \\times p}\\) is a diagonal matrix with singular values \\(d_1, d_2, \\dots, d_n \\geq 0\\) along the diagonal. Since \\(p &gt; n\\), the number of singular values is at most \\(n\\), meaning that \\(\\mathbf{X}&#39; \\mathbf{X}\\) has at most rank \\(n\\) and is not invertible when \\(p &gt; n\\). Using the SVD of \\(\\mathbf{X}\\), we can express \\[ \\mathbf{X}&#39; \\mathbf{X} = (\\mathbf{U} \\mathbf{D} \\mathbf{V}&#39;)&#39; (\\mathbf{U} \\mathbf{D} \\mathbf{V}&#39;) = \\mathbf{V} \\mathbf{D}&#39; \\mathbf{U}&#39; \\mathbf{U} \\mathbf{D} \\mathbf{V}&#39; = \\mathbf{V} (\\mathbf{D}&#39; \\mathbf{D}) \\mathbf{V}&#39;. \\] Since \\(\\mathbf{U}\\) is an \\(n \\times n\\) orthogonal matrix, \\(\\mathbf{D}&#39; \\mathbf{D}\\) is a \\(p \\times p\\) diagonal matrix: \\[ \\mathbf{D}&#39; \\mathbf{D} = \\begin{bmatrix} d_1^2 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; d_2^2 &amp; \\dots &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; d_n^2 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ \\end{bmatrix}. \\] The first \\(n\\) diagonal entries are \\(d_i^2\\), corresponding to the squared singular values of \\(\\mathbf{X}\\). The remaining \\(p - n\\) diagonal entries are zero, meaning \\(\\mathbf{X}&#39; \\mathbf{X}\\) has \\(p - n\\) zero eigenvalues and is not full rank. Now, consider the modified matrix: \\[ \\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I}. \\] Using the SVD form of \\(\\mathbf{X}&#39; \\mathbf{X}\\), we have: \\[ \\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I} = \\mathbf{V} (\\mathbf{D}&#39; \\mathbf{D}) \\mathbf{V}&#39; + \\lambda \\mathbf{I}. \\] Since \\(\\mathbf{I} = \\mathbf{V} \\mathbf{I} \\mathbf{V}&#39;\\), we rewrite this as: \\[ \\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I} = \\mathbf{V} (\\mathbf{D}&#39; \\mathbf{D} + \\lambda \\mathbf{I}) \\mathbf{V}&#39;. \\] Since \\(\\mathbf{D}&#39; \\mathbf{D}\\) is diagonal, adding \\(\\lambda \\mathbf{I}\\) results in: \\[ \\mathbf{D}&#39; \\mathbf{D} + \\lambda \\mathbf{I} = \\begin{bmatrix} d_1^2 + \\lambda &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; d_2^2 + \\lambda &amp; \\dots &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; d_n^2 + \\lambda &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; \\dots &amp; 0 &amp; \\lambda &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; \\dots &amp; \\lambda \\\\ \\end{bmatrix}. \\] The first \\(n\\) diagonal entries are \\(d_i^2 + \\lambda\\), all strictly positive because \\(d_i^2 \\geq 0\\) and \\(\\lambda &gt; 0\\). The last \\(p - n\\) diagonal entries are \\(\\lambda\\) (also strictly positive). Thus, all eigenvalues of \\(\\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I}\\) are strictly positive, implying that it is full rank and invertible. Since \\(\\mathbf{V}\\) is an orthogonal matrix, the entire expression \\[ \\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I} = \\mathbf{V} (\\mathbf{D}&#39; \\mathbf{D} + \\lambda \\mathbf{I}) \\mathbf{V}&#39; \\] is invertible, because an orthogonal matrix times an invertible diagonal matrix remains invertible. So, even when \\(p &gt; n\\), adding \\(\\lambda \\mathbf{I}\\) shifts all eigenvalues away from zero, ensuring that \\[ \\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I} \\] is always invertible for \\(\\lambda &gt; 0\\). This guarantees that Ridge Regression always has a unique solution: \\[ \\boldsymbol{\\beta}_{\\text{ridge}} = (\\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\mathbf{X}&#39; \\mathbf{y}. \\] 4.1.2.3 Ridge Regression as a Minimization Problem Instead of minimizing the standard sum of squared errors, Ridge Regression solves the following regularized problem: \\[ \\min_{\\boldsymbol{\\beta}} \\mathcal{L}(\\boldsymbol{\\beta}) = \\min_{\\boldsymbol{\\beta}} \\left\\{ \\| \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|_2^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_2^2 \\right\\}. \\] where \\(\\lambda &gt; 0\\) is a tuning parameter that controls the amount of regularization: - When \\(\\lambda = 0\\): The problem reduces to ordinary least squares (OLS). - When \\(\\lambda \\to \\infty\\): The penalty dominates, forcing \\(\\boldsymbol{\\beta}\\) toward zero, shrinking coefficients. Expanding the loss function: \\[ \\mathcal{L}(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})&#39; (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}) + \\lambda \\boldsymbol{\\beta}&#39; \\boldsymbol{\\beta}. \\] Taking the derivative with respect to \\(\\boldsymbol{\\beta}\\) and setting it to zero: \\[ -2 \\mathbf{X}&#39; (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}) + 2 \\lambda \\boldsymbol{\\beta} = 0. \\] Rearranging: \\[ \\mathbf{X}&#39; \\mathbf{X} \\boldsymbol{\\beta} + \\lambda \\boldsymbol{\\beta} = \\mathbf{X}&#39; \\mathbf{y}. \\] Factoring out \\(\\boldsymbol{\\beta}\\): \\[ (\\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I}) \\boldsymbol{\\beta} = \\mathbf{X}&#39; \\mathbf{y}. \\] Since \\(\\mathbf{X}&#39;\\mathbf{X}\\) may be singular, adding \\(\\lambda \\mathbf{I}\\) ensures that the matrix \\((\\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I})\\) is always invertible for any \\(\\lambda &gt; 0\\). Thus, the Ridge Regression solution is \\[ \\boldsymbol{\\beta}_{\\text{ridge}} = (\\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\mathbf{X}&#39; \\mathbf{y}. \\] 4.1.2.4 Conclusion Matrix Regularization: The term \\(\\lambda \\mathbf{I}\\) ensures that \\(\\mathbf{X}&#39;\\mathbf{X} + \\lambda \\mathbf{I}\\) is always invertible because it shifts the eigenvalues of \\(\\mathbf{X}&#39;\\mathbf{X}\\) away from zero. Bias-Variance Tradeoff: Ridge Regression reduces variance at the cost of introducing some bias, which can improve prediction accuracy when \\(\\mathbf{X}\\) is ill-conditioned or when \\(p &gt; n\\). Shrinkage Effect: Larger \\(\\lambda\\) values shrink the coefficients towards zero, preventing overfitting. When \\(\\lambda = 0\\): The problem reduces to ordinary least squares (OLS). When \\(\\lambda \\to \\infty\\): The penalty dominates, forcing \\(\\boldsymbol{\\beta}\\) toward zero, shrinking coefficients. The only thing that is left is selecting the value of \\(\\lambda\\) 4.1.3 Lasso Regression Lasso regression (Least Absolute Shrinkage and Selection Operator) is a variation of linear regression that adds a penalty (like Ridge Regression) to the loss function to promote sparsity in the coefficients, effectively setting some of them to zero (unulike Ridge Regression). This makes Lasso a useful technique for feature selection, especially when we have many predictors, some of which may be irrelevant or highly correlated. 4.1.3.1 Lasso Regression as an Optimization Problem The Lasso regression formulation is: \\[ \\min_{\\boldsymbol{\\beta}} \\mathcal{L}(\\boldsymbol{\\beta}) = \\min_{\\boldsymbol{\\beta}} \\left\\{ \\| \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|_2^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_1 \\right\\}. \\] Where: - \\(\\|\\boldsymbol{\\beta}\\|_1 = \\sum_{i=1}^p |\\beta_i|\\) is the L1 norm of the coefficients (sum of absolute values of the coefficients), - \\(\\lambda \\geq 0\\) is the regularization parameter controlling the strength of the penalty. The loss function consists of: 1. Residual Sum of Squares (RSS): \\(\\| \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|_2^2\\), which measures the fit of the model (same as in ordinary least squares regression). 2. L1 Penalty: \\(\\lambda \\|\\boldsymbol{\\beta}\\|_1\\), which shrinks the coefficients towards zero and encourages sparsity (i.e., some coefficients are exactly zero). The objective is to minimize the sum of squared residuals along with a penalty term: \\[ \\min_{\\boldsymbol{\\beta}} \\left\\{ \\| \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|_2^2 + \\lambda \\sum_{i=1}^p |\\beta_i| \\right\\}. \\] The key feature of Lasso is that the L1 penalty promotes sparsity by shrinking some coefficients exactly to zero, which results in a simpler, more interpretable model. The parameter \\(\\lambda\\) controls the trade-off between fit and sparsity: - When \\(\\lambda = 0\\): Lasso reduces to ordinary least squares regression (OLS), where no penalty is applied. - When \\(\\lambda\\) is large: The penalty dominates, and more coefficients are shrunk to zero. Unfortunately, unlike Ridge Regression, Lasso has no closed form solution and it is necessary to find the solution numerically. 4.1.4 Elastic Net Elastic Net is a regularization technique that combines the strengths of Lasso and Ridge regression. While Lasso uses an L1 penalty and Ridge uses an L2 penalty, Elastic Net applies a mix of both penalties, giving a balance between sparsity and regularization strength. Elastic Net is particularly useful when there are highly correlated features or when the number of features is larger than the number of observations (\\(p &gt; n\\)). 4.1.5 Elastic Net as a Mixed Penalty The Elastic Net loss function is defined as: \\[ \\min_{\\boldsymbol{\\beta}} \\left\\{ \\| \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|_2^2 + \\lambda_1 \\|\\boldsymbol{\\beta}\\|_1 + \\frac{\\lambda_2}{2} \\|\\boldsymbol{\\beta}\\|_2^2 \\right\\}. \\] Where: - \\(\\mathbf{y}\\) is the \\(n \\times 1\\) vector of observed responses, - \\(\\mathbf{X}\\) is the \\(n \\times p\\) matrix of predictor variables, - \\(\\boldsymbol{\\beta}\\) is the \\(p \\times 1\\) vector of regression coefficients, - \\(\\|\\boldsymbol{\\beta}\\|_1 = \\sum_{i=1}^p |\\beta_i|\\) is the L1 norm (Lasso penalty), - \\(\\|\\boldsymbol{\\beta}\\|_2^2 = \\sum_{i=1}^p \\beta_i^2\\) is the L2 norm (Ridge penalty), - \\(\\lambda_1 \\geq 0\\) is the L1 regularization parameter (controlling the Lasso penalty), - \\(\\lambda_2 \\geq 0\\) is the L2 regularization parameter (controlling the Ridge penalty). L1 Penalty (Lasso term): \\(\\lambda_1 \\|\\boldsymbol{\\beta}\\|_1\\) encourages sparsity, meaning that it drives some coefficients to exactly zero. This is helpful for feature selection and reduces the complexity of the model. L2 Penalty (Ridge term): \\(\\frac{\\lambda_2}{2} \\|\\boldsymbol{\\beta}\\|_2^2\\) shrinks the coefficients toward zero without setting them exactly to zero. This helps with multicollinearity, preventing large fluctuations in the estimated coefficients when predictors are highly correlated. 4.1.5.1 Why Use Elastic Net? Correlation between predictors: When predictors are highly correlated, Lasso tends to select one variable and ignore the others. Elastic Net, by mixing L1 and L2 penalties, can help by including correlated variables in the model but still controlling their coefficients through the L2 penalty. Feature selection with many predictors: In cases where the number of features \\(p\\) is much greater than the number of observations \\(n\\), Lasso can become unstable. Elastic Net helps stabilize the model by adding a Ridge component, which shrinks the coefficients of less important features without forcing them to zero. The Elastic Net can be seen as a weighted sum of the Lasso and Ridge penalties, where: \\[ \\text{Elastic Net Loss} = \\lambda_1 \\|\\boldsymbol{\\beta}\\|_1 + \\frac{\\lambda_2}{2} \\|\\boldsymbol{\\beta}\\|_2^2. \\] You can think of \\(\\lambda_1\\) as controlling the strength of the Lasso penalty (feature selection), and \\(\\lambda_2\\) as controlling the strength of the Ridge penalty (shrinkage). The Elastic Net is useful when you need both sparsity (for feature selection) and regularization (to prevent overfitting). 4.1.5.2 Optimization Problem The Elastic Net optimization problem is: \\[ \\min_{\\boldsymbol{\\beta}} \\left\\{ \\| \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|_2^2 + \\lambda_1 \\|\\boldsymbol{\\beta}\\|_1 + \\frac{\\lambda_2}{2} \\|\\boldsymbol{\\beta}\\|_2^2 \\right\\}. \\] Objective function: The goal is to minimize the sum of squared residuals (RSS) plus the combined penalty terms. The optimal values of \\(\\lambda_1\\) and \\(\\lambda_2\\) are typically chosen via cross-validation. 4.1.5.3 Connections to Lasso and Ridge Regression When \\(\\lambda_2 = 0\\): Elastic Net becomes Lasso regression, as the Ridge term disappears and only the L1 penalty is applied. When \\(\\lambda_1 = 0\\): Elastic Net becomes Ridge regression, as the L1 penalty is removed and only the L2 penalty is applied. When both \\(\\lambda_1\\) and \\(\\lambda_2\\) are non-zero: Elastic Net is a combination of both regularization methods, providing a balanced approach. 4.1.6 Other Options of Regularization In addition to Elastic Net, Ridge, and Lasso, there are other regularization methods used in machine learning and statistical modeling: Group Lasso: Formula: \\[ \\min_{\\boldsymbol{\\beta}} \\left\\{ \\| \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|_2^2 + \\lambda \\sum_{g} \\|\\boldsymbol{\\beta}_g\\|_2 \\right\\}. \\] Penalty: Group Lasso is used when variables are grouped, and the penalty is applied at the group level. It forces entire groups of variables to be either included or excluded from the model. Fused Lasso: Formula: \\[ \\min_{\\boldsymbol{\\beta}} \\left\\{ \\| \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|_2^2 + \\lambda_1 \\|\\boldsymbol{\\beta}\\|_1 + \\lambda_2 \\sum_{i} |\\beta_i - \\beta_{i-1}| \\right\\}. \\] Penalty: Fused Lasso adds a penalty to the differences between adjacent coefficients, encouraging smoothness in the solution. This is useful in time series or spatial data where adjacent coefficients are expected to be similar. Bayesian Regularization: Formula: Bayesian regularization methods, like Bayesian Ridge Regression, assume a probabilistic model for the coefficients and add a prior distribution (often Gaussian) to the coefficients. The regularization comes from the prior’s influence on the model. Penalty: The prior serves as a regularizer, encouraging smaller coefficients with the Gaussian prior. 4.2 Bayesian Linear Regression Bayesian regression provides a probabilistic framework for regression analysis by incorporating prior knowledge about the parameters. It offers a direct connection to regularized regression, by introducing a prior on the regression coefficients. 4.2.1 Basic Bayesian Linear Regression As before, we consider the standard linear regression model: \\[ \\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{e}, \\quad \\mathbf{e} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}). \\] where: - \\(\\mathbf{y}\\) is the \\(n \\times 1\\) vector of observed responses, - \\(\\mathbf{X}\\) is the \\(n \\times p\\) matrix of predictor variables, - \\(\\boldsymbol{\\beta}\\) is the \\(p \\times 1\\) vector of regression coefficients, - \\(\\mathbf{e} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})\\) is the noise term, assumed to follow a normal distribution with variance \\(\\sigma^2\\). The likelihood function follows from the assumption that \\(\\mathbf{y}\\) is normally distributed given \\(\\mathbf{X}\\) and \\(\\boldsymbol{\\beta}\\): \\[ p(\\mathbf{y} | \\boldsymbol{\\beta}) = \\mathcal{N}(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}). \\] In a Bayesian framework, we assume a prior distribution for \\(\\boldsymbol{\\beta}\\). There are several (infinite) alternatives to set a prior, however in this case, we are going to work with a very basic model, in fact in a more gneral setting a prior distribution for \\(\\sigma^2\\) is usually specified. We take a normal prior with mean zero and covariance matrix \\(\\sigma^2 \\mathbf{\\Sigma}_\\beta\\), where \\(\\mathbf{\\Sigma}_\\beta\\) captures prior beliefs about the relationships between the coefficients: \\[ p(\\boldsymbol{\\beta}) = \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{\\Sigma}_\\beta). \\] where \\(\\mathbf{\\Sigma}_\\beta\\) is a positive definite \\(p \\times p\\) covariance matrix. 4.2.1.1 Posterior Distribution of \\(\\boldsymbol{\\beta}\\) Applying Bayes’ theorem, the posterior is proportional to the product of the likelihood and the prior: \\[ p(\\boldsymbol{\\beta} | \\mathbf{y}) \\propto p(\\mathbf{y} | \\boldsymbol{\\beta}) p(\\boldsymbol{\\beta}). \\] Since both the likelihood and prior are Gaussian, the posterior will also be Gaussian. To derive its mean and covariance, we complete the square in the exponent. \\[ p(\\mathbf{y} | \\boldsymbol{\\beta}) \\propto \\exp \\left( -\\frac{1}{2\\sigma^2} \\| \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|_2^2 \\right). \\] Expanding: \\[ \\| \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|_2^2 = (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})&#39; (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}). \\] \\[ p(\\boldsymbol{\\beta}) \\propto \\exp \\left( -\\frac{1}{2\\sigma^2} \\boldsymbol{\\beta}&#39; \\mathbf{\\Sigma}_\\beta^{-1} \\boldsymbol{\\beta} \\right). \\] The posterior distribution is proportional to: \\[ \\exp \\left( -\\frac{1}{2\\sigma^2} \\left[ (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})&#39; (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}) + \\boldsymbol{\\beta}&#39; \\mathbf{\\Sigma}_\\beta^{-1} \\boldsymbol{\\beta} \\right] \\right). \\] Expanding the quadratic term: \\[ \\mathbf{y}&#39; \\mathbf{y} - 2 \\mathbf{y}&#39; \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\beta}&#39; \\mathbf{X}&#39; \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\beta}&#39; \\mathbf{\\Sigma}_\\beta^{-1} \\boldsymbol{\\beta}. \\] Rewriting, \\[ - 2 \\mathbf{y}&#39; \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\beta}&#39; (\\mathbf{X}&#39; \\mathbf{X} + \\mathbf{\\Sigma}_\\beta^{-1}) \\boldsymbol{\\beta}. \\] Completing the square, we identify the posterior mean: \\[ \\boldsymbol{\\beta} | \\mathbf{y} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_\\beta, \\mathbf{\\Sigma}_\\beta^*), \\] where: \\[ \\mathbf{\\Sigma}_\\beta^* = \\left( \\mathbf{X}&#39; \\mathbf{X} + \\mathbf{\\Sigma}_\\beta^{-1} \\right)^{-1} \\sigma^2, \\] \\[ \\boldsymbol{\\mu}_\\beta = \\mathbf{\\Sigma}_\\beta^* \\mathbf{X}&#39; \\mathbf{y}. \\] 4.2.1.2 Connection to Ridge Regression If we assume that the prior covariance is a scaled identity matrix, i.e., \\[ \\mathbf{\\Sigma}_\\beta = \\frac{1}{\\lambda} \\mathbf{I}, \\] then its inverse is: \\[ \\mathbf{\\Sigma}_\\beta^{-1} = \\lambda \\mathbf{I}. \\] Substituting this into the posterior mean formula: \\[ \\boldsymbol{\\mu}_\\beta = \\left( \\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\mathbf{X}&#39; \\mathbf{y}. \\] which is exactly the Ridge estimator: \\[ \\hat{\\boldsymbol{\\beta}}_{\\text{ridge}} = \\left( \\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\mathbf{X}&#39; \\mathbf{y}. \\] Thus, we see that Bayesian regression with a normal prior on \\(\\boldsymbol{\\beta}\\) corresponds to Ridge regression, where the regularization parameter \\(\\lambda\\) is determined by the prior covariance. 4.2.1.3 Behavior of the Posterior Distribution as \\(\\lambda\\) Varies Since we have shown that the posterior mean of \\(\\boldsymbol{\\beta}\\) is: \\[ \\boldsymbol{\\mu}_\\beta = \\left( \\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\mathbf{X}&#39; \\mathbf{y}, \\] and the posterior covariance is: \\[ \\mathbf{\\Sigma}_\\beta^* = \\sigma^2 \\left( \\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1}, \\] we can analyze the behavior of the posterior distribution under extreme values of \\(\\lambda\\). When \\(\\lambda \\to 0\\) (No Regularization, Pure MLE) As \\(\\lambda \\to 0\\), the prior becomes uninformative, meaning we are not imposing any shrinkage on the coefficients. In this case: \\[ \\left( \\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\to (\\mathbf{X}&#39; \\mathbf{X})^{-1}, \\] assuming \\(\\mathbf{X}&#39; \\mathbf{X}\\) is invertible. Then, the posterior mean simplifies to the ordinary least squares (OLS) estimator: \\[ \\boldsymbol{\\mu}_\\beta \\to (\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{X}&#39; \\mathbf{y}. \\] Likewise, the posterior covariance reduces to: \\[ \\mathbf{\\Sigma}_\\beta^* \\to \\sigma^2 (\\mathbf{X}&#39; \\mathbf{X})^{-1}. \\] This shows that, when \\(\\lambda \\to 0\\), Bayesian regression becomes equivalent to the classical maximum likelihood estimate (MLE) from OLS, with high variance when \\(\\mathbf{X}&#39; \\mathbf{X}\\) is ill-conditioned. When \\(\\lambda \\to \\infty\\) (Strong Prior, Heavy Shrinkage) As \\(\\lambda \\to \\infty\\), the prior dominates and strongly shrinks \\(\\boldsymbol{\\beta}\\) toward zero. In this case: \\[ \\left( \\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\approx \\frac{1}{\\lambda} \\mathbf{I}, \\quad \\text{(for large \\( \\lambda \\))}. \\] Thus, the posterior mean behaves as: \\[ \\boldsymbol{\\mu}_\\beta \\approx \\frac{1}{\\lambda} \\mathbf{X}&#39; \\mathbf{y} \\to \\mathbf{0} \\quad \\text{as} \\quad \\lambda \\to \\infty. \\] Similarly, the posterior covariance reduces to: \\[ \\mathbf{\\Sigma}_\\beta^* \\approx \\frac{\\sigma^2}{\\lambda} \\mathbf{I} \\to \\mathbf{0}. \\] This means that, for very large \\(\\lambda\\), the posterior distribution becomes highly concentrated around zero: \\[ \\boldsymbol{\\beta} | \\mathbf{y} \\sim \\mathcal{N}(\\mathbf{0}, 0). \\] Interpretation: - As \\(\\lambda \\to \\infty\\), the prior overwhelms the data and forces all coefficients to shrink to zero. - The posterior variance also vanishes, meaning the uncertainty about \\(\\boldsymbol{\\beta}\\) disappears—everything is shrunk toward the prior mean (which is zero in this case). - This corresponds to extreme regularization, effectively setting all coefficients to zero, similar to a very strong Ridge penalty. Summary of \\(\\lambda\\)-Dependence: \\(\\lambda\\) Posterior Mean \\(\\boldsymbol{\\mu}_\\beta\\) Posterior Covariance \\(\\mathbf{\\Sigma}_\\beta^*\\) Interpretation \\(\\lambda \\to 0\\) OLS estimate: \\((\\mathbf{X}&#39; \\mathbf{X})^{-1} \\mathbf{X}&#39; \\mathbf{y}\\) \\(\\sigma^2 (\\mathbf{X}&#39; \\mathbf{X})^{-1}\\) No regularization (pure MLE). Large variance if \\(\\mathbf{X}&#39; \\mathbf{X}\\) is ill-conditioned. Small \\(\\lambda\\) Close to OLS Slightly shrunk covariance Light regularization. Small shrinkage toward zero. Large \\(\\lambda\\) Strongly shrunk toward zero Shrunk covariance, but still adaptive to data Ridge-like regularization, balances data and prior. \\(\\lambda \\to \\infty\\) \\(\\mathbf{0}\\) \\(\\mathbf{0}\\) Extreme shrinkage; model ignores data and forces coefficients to zero. 4.2.1.4 Conclusion The Bayesian regression model presented here is a basic formulation that assumes a Gaussian likelihood and a normal prior on the regression coefficients. This simple setup already reveals deep connections to Ridge regression, demonstrating how prior beliefs influence parameter estimation through shrinkage. Also note that the Bayesian framework allows you to perform Linear Regression even in the case where \\(p &gt; n\\) without the need to change models, as long as a proper prior for \\( {\\boldsymbol \\beta} \\) is used. The model developed before, is just one possible Bayesian approach to regression. Many alternative priors can be used to encode different assumptions about the regression coefficients, leading to distinct forms of regularization: Laplace prior: Leads to Bayesian Lasso, which promotes sparsity by encouraging some coefficients to be exactly zero. Spike-and-slab prior: A mixture of a point mass at zero and a diffuse normal distribution, allowing for automatic feature selection. Horseshoe prior: A heavy-tailed prior that shrinks small coefficients strongly while allowing large ones to remain, making it useful for sparse models with some large effects. Gaussian Process priors: Used in nonparametric Bayesian regression, allowing for flexible modeling of relationships without assuming a fixed functional form. These richer prior choices allow Bayesian regression to adapt to a variety of settings, from high-dimensional problems to nonlinear relationships. Bayesian approaches also provide full posterior distributions, enabling uncertainty quantification in predictions—a key advantage over standard frequentist methods. 4.2.2 Bayesian Lasso Regression Given the standard regression model: \\[ \\mathbf{y} | \\boldsymbol{\\beta}, \\sigma^2 \\sim \\mathcal{N}(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}), \\] we place a Laplace prior on each coefficient \\(\\beta_j\\), scaled by \\(\\sigma\\), as follows: \\[ p(\\beta_j | \\sigma^2) = \\frac{\\lambda}{2\\sigma} \\exp \\left( - \\frac{\\lambda}{\\sigma} | \\beta_j | \\right). \\] This prior encourages sparsity, shrinking small coefficients toward zero while allowing some large ones. Unlike the basic Bayesian approach in the last section, the Bayesian Lasso does not have a closed form posterior distribution. However, it is easy to sample from following a hierarchical prior approach. 4.2.2.1 Hierarchical Representation of the Laplace Prior The Laplace prior can be rewritten as a hierarchical model using a Gaussian scale mixture representation. Specifically, we introduce auxiliary variance parameters \\(\\tau_j^2\\), where: \\[ \\beta_j | \\tau_j^2, \\sigma^2 \\sim \\mathcal{N}(0, \\sigma^2 \\tau_j^2). \\] The prior on \\(\\tau_j^2\\) follows an exponential distribution: \\[ p(\\tau_j^2 | \\lambda^2) = \\frac{\\lambda^2}{2} \\exp \\left( -\\frac{\\lambda^2}{2} \\tau_j^2 \\right). \\] Thus, the Bayesian Lasso can be interpreted as Bayesian ridge regression with an adaptive prior variance for each coefficient. 4.2.2.2 Posterior Distribution and MAP Estimator The posterior distribution of \\(\\boldsymbol{\\beta}\\) is given by: \\[ p(\\boldsymbol{\\beta} | \\mathbf{y}, \\sigma^2) \\propto p(\\mathbf{y} | \\boldsymbol{\\beta}, \\sigma^2) p(\\boldsymbol{\\beta} | \\sigma^2). \\] Since: - The likelihood is Gaussian: \\[ p(\\mathbf{y} | \\boldsymbol{\\beta}, \\sigma^2) \\propto \\exp \\left( - \\frac{1}{2\\sigma^2} \\|\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|_2^2 \\right), \\] - The prior is Laplace: \\[ p(\\boldsymbol{\\beta} | \\sigma^2) \\propto \\exp \\left( - \\frac{\\lambda}{2\\sigma} \\|\\boldsymbol{\\beta}\\|_1 \\right), \\] then the posterior mode (i.e., the Maximum A Posteriori (MAP) estimator) is obtained by solving: \\[ \\hat{\\boldsymbol{\\beta}} = \\arg \\min_{\\boldsymbol{\\beta}} \\left\\{ \\|\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\|_2^2 + \\sigma \\lambda \\|\\boldsymbol{\\beta}\\|_1 \\right\\}. \\] This exactly recovers the traditional Lasso estimator, where the regularization term depends on \\(\\sigma \\lambda\\). Thus, the Bayesian Lasso provides a probabilistic justification for the Lasso estimator and explains how shrinkage is controlled by both \\(\\lambda\\) and \\(\\sigma^2\\). 4.2.2.3 Behavior of the Posterior as \\(\\lambda\\) and \\(\\sigma^2\\) Vary As \\(\\lambda \\to 0\\): The prior becomes uninformative, and the MAP estimate approaches the MLE (ordinary least squares). As \\(\\lambda \\to \\infty\\): The prior dominates the likelihood, and the posterior distribution becomes highly concentrated at zero (extreme sparsity). 4.3 Computational Comparisson We generate a synthetic dataset where only a subset of the predictors are relevant. We compare: Ridge regression. Lasso regression. Basic Bayesian Regression. Bayesian Lasso Regression. Horseshoe Prior Bayesian Regression. 4.3.1 Set-Up First we need some functions and libraries: library(glmnet) Loads the glmnet package, which is used for Lasso, Ridge, and Elastic Net regression. The glmnet package provides functions like cv.glmnet() and glmnet() to perform penalized regression with cross-validation. library(monomvn) Loads the monomvn package, which provides Bayesian regression models, including Bayesian Lasso and Bayesian Ridge. library(mvtnorm) Loads the mvtnorm package, which allows working with the multivariate normal distribution. Used for simulating correlated predictors in the design matrix (X) and for Bayesian sampling. source(&quot;./horseshoe_sampler.R&quot;) source(&quot;./fast_sampler.R&quot;) source(&quot;./fast_horseshoe.R&quot;) Loads external R scripts (horseshoe_sampler.R, fast_sampler.R, and fast_horseshoe.R), which likely contain Bayesian sampling functions for Horseshoe priors. These scripts implement Markov Chain Monte Carlo (MCMC) algorithms or other methods to generate posterior samples. set.seed(222025) Sets the random seed to ensure that results are reproducible. Ensures that simulated data and stochastic processes (e.g., cross-validation, Bayesian sampling) yield the same results every time the script is run. numTra &lt;- 300 # Training samples numSam &lt;- numTra * 2 # Total samples numVar &lt;- 500 # Number of predictors sizBlo &lt;- 10 # Block size for Correlation Matrix numNze &lt;- 10 # Number of nonzero coefficients sigNoi &lt;- 1 # Signal to Noise Ratio We have that: numTra: The number of samples available for training. numSam: The total number of samples in the dataset, including both training and testing data. It is set to twice the number of training samples. numVar: The number of predictor variables (features) in the dataset. sizBlo: The size of blocks in the correlation structure of the design matrix. This determines how many variables are within each block. numNze: The number of nonzero coefficients in the true regression model. These correspond to the features that actually influence the response variable. sigNoi: The signal-to-noise ratio, which controls the relative strength of the true signal (nonzero coefficients) compared to the noise in the observations. 4.3.2 Simulation # X, b and y simulation disCor &lt;- 0.5 # Correlation decay C &lt;- exp(- disCor * abs(rep(1, sizBlo) %*% t(1:sizBlo) - (1:sizBlo) %*% t(rep(1, sizBlo)))) X &lt;- matrix(NA, nrow = numSam, ncol = 0) for(i in 1:(numVar / sizBlo)){ X &lt;- cbind(X, rmvnorm(n = numSam, sigma = C)) } # Generates Coefficients and y coeDecRat &lt;- 4 b &lt;- rep(0, numVar) b[1:numNze] &lt;- exp(- (1:numNze) / coeDecRat) s2 &lt;- (mean(b[1:numNze]) / sigNoi)^2 y &lt;- X %*% b + rnorm(n = numSam, sd = sqrt(s2)) # Coefficient Values print(b[1:(2 * numNze)]) ## [1] 0.7788008 0.6065307 0.4723666 0.3678794 0.2865048 0.2231302 0.1737739 0.1353353 0.1053992 0.0820850 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 ## [17] 0.0000000 0.0000000 0.0000000 0.0000000 # Response Variable Variance print(var(y)) ## [,1] ## [1,] 4.276878 # Error Standard Deviation print(s2) ## [1] 0.1044457 This chunk of code generates the design matrix \\(\\mathbf{X}\\), regression coefficients \\(\\mathbf{b}\\), and response variable \\(\\mathbf{y}\\) for a simulated linear regression problem. It incorporates a correlated feature structure, meaning some predictors are related to each other. disCor &lt;- 0.5 # Correlation decay disCor controls how quickly correlations decay between predictors in the same block. A higher value means stronger correlations between nearby variables, while a lower value leads to weaker correlations. C &lt;- exp(- disCor * abs(rep(1, sizBlo) %*% t(1:sizBlo) - (1:sizBlo) %*% t(rep(1, sizBlo)))) C is a block correlation matrix of size sizBlo × sizBlo. The matrix is Toeplitz-like, meaning each predictor is correlated with its neighbors, and correlation decays exponentially as you move further away. The function exp(-disCor * distance) ensures that correlations are strongest within blocks and weaken with distance. X &lt;- matrix(NA, nrow = numSam, ncol = 0) for(i in 1:(numVar / sizBlo)){ X &lt;- cbind(X, rmvnorm(n = numSam, sigma = C)) } X is initialized as an empty matrix. The for-loop constructs \\(\\mathbf{X}\\) by generating blocks of correlated features using rmvnorm() (from mvtnorm package), which samples from a multivariate normal distribution with covariance matrix C. Each block of predictors has size sizBlo, and the total number of blocks is numVar / sizBlo. coeDecRat &lt;- 4 # Controls how fast coefficients decay b &lt;- rep(0, numVar) # Initializes all coefficients to zero b[1:numNze] &lt;- exp(-(1:numNze) / coeDecRat) coeDecRat controls the decay rate of the true regression coefficients. Only the first numNze coefficients are nonzero, and they follow an exponential decay pattern: \\[ b_j = \\exp(-j / \\text{coeDecRat}), \\quad \\text{for } j = 1, \\dots, \\text{numNze} \\] This means important variables have larger effects, and their influence decreases exponentially. sigNoi &lt;- mean(b[1:numNze]) / 2 sigNoi represents the signal-to-noise ratio: Higher sigNoi → stronger signal, less noise. Lower sigNoi → weaker signal, more noise. s2 &lt;- (mean(b[1:numNze]) / sigNoi)^2 s2 is the variance of the noise term. It ensures that the ratio of signal to noise remains controlled. y &lt;- X %*% b + rnorm(n = numSam, sd = sqrt(s2)) print(b[1:(2 * numNze)]) # Print the first few coefficients Displays the first 2 * numNze coefficients to confirm they follow the expected decay pattern and zero coefficients afterwards. print(var(y)) # Print the variance of the response variable Checks the total variance of \\(y\\), which is affected by both the signal and the noise. print(s2) # Print error variance Prints the variance of the noise to verify the expected level of randomness in the response variable. Variable Description disCor Decay parameter controlling correlation between predictors. C Correlation matrix defining block-wise correlated predictors. X Design matrix, with correlated predictor blocks. coeDecRat Decay rate for true coefficients, controlling sparsity. b True regression coefficients, only numNze are nonzero. s2 Noise variance, ensuring controlled randomness in \\(y\\). y Response variable, generated from \\(X\\) and \\(b\\) with noise. traInd &lt;- sample(1:numSam, numTra, replace = FALSE) traY &lt;- y[traInd] traX &lt;- X[traInd, ] tesY &lt;- y[-traInd] tesX &lt;- X[-traInd, ] This block splits the dataset into training and testing sets, which is crucial for evaluating model performance. Step-by-Step Explanation: traInd &lt;- sample(1:numSam, numTra, replace = FALSE) sample(1:numSam, numTra, replace = FALSE) Randomly selects numTra indices from 1:numSam (the full dataset). replace = FALSE ensures that no index is selected more than once, maintaining a random subset without duplication. traInd stores the indices of the training samples. traY &lt;- y[traInd] traX &lt;- X[traInd, ] traY: Subset of y containing only the selected training indices → training response values. traX: Corresponding rows from design matrix X → training predictor values. tesY &lt;- y[-traInd] tesX &lt;- X[-traInd, ] tesY: Subset of y excluding training indices → test response values. tesX: Corresponding rows from X excluding training indices → test predictor values. Summary of Variables: Variable Description traInd Indices randomly chosen for training. traX Training set predictors (subset of X). traY Training set response values (subset of y). tesX Test set predictors (remaining rows of X). tesY Test set response values (remaining rows of y). Why This Matters? 1. Prevents overfitting: The model is trained on traX, traY but evaluated on tesX, tesY, ensuring it generalizes to unseen data. 2. Mimics real-world scenarios: In practice, models predict new data points, so testing on unseen data measures true performance. 3. Ensures unbiased evaluation: A random split avoids bias in model evaluation, ensuring the test set represents different feature patterns. 4.3.3 OLS Then we first OLS outOLS &lt;- lm(traY ~ traX - 1) coeOLS &lt;- outOLS$coefficients coeOLS[is.na(coeOLS)] &lt;- 0 mseOLS &lt;- mean((tesX %*% coeOLS - tesY)^2) pr2OLS &lt;- 1 - mseOLS / var(tesY) This block fits an OLS regression model, extracts the estimated coefficients, and evaluates the model’s performance on the test set. outOLS &lt;- lm(traY ~ traX - 1) lm(traY ~ traX - 1): Fits a linear model where traY is the response variable and traX is the design matrix. The -1 removes the default intercept, ensuring that all coefficients correspond directly to the predictors in traX. outOLS stores the fitted model, including coefficients and residuals. coeOLS &lt;- outOLS$coefficients Retrieves the estimated regression coefficients from the fitted model. coeOLS[is.na(coeOLS)] &lt;- 0 Replaces any NA values with 0 (this can happen when certain predictors are collinear, leading to undefined coefficients). mseOLS &lt;- mean((tesX %*% coeOLS - tesY)^2) Computes the Mean Squared Error (MSE) on the test set: \\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} ( \\hat{y}_i - y_i )^2 \\] tesX %*% coeOLS calculates predicted values \\(\\hat{y}\\) on the test data. pr2OLS &lt;- 1 - mseOLS / var(tesY) Computes the predictive R-squared \\(R^2\\): \\[ R^2 = 1 - \\frac{\\text{MSE}}{\\text{Var}(\\text{Test } y)} \\] Interpretation: \\(R^2 \\approx 1\\) → Model explains most of the variance in tesY (good performance). \\(R^2 \\approx 0\\) → Model performs as poorly as simply predicting the mean. \\(R^2 &lt; 0\\) → Model performs worse than a constant predictor (severe overfitting or poor generalization). Summary of Key Variables: Variable Description outOLS OLS model fitted to the training data. coeOLS Estimated coefficients from the OLS model. mseOLS Mean Squared Error on the test set (lower is better). pr2OLS Predictive \\(R^2\\), measuring how well the model generalizes to new data. Why This Matters? OLS assumes no multicollinearity → Since traX contains correlated predictors, OLS may suffer from unstable coefficient estimates. Regularization (e.g., Ridge, Lasso) is often needed to improve performance when predictors are highly correlated. Comparison to Ridge/Lasso/Bayesian models will highlight how regularization techniques handle multicollinearity better than OLS. 4.3.4 Ridge Regression cvRID &lt;- cv.glmnet(x = traX, y = traY, alpha = 0) outRID &lt;- glmnet(x = traX, y = traY, alpha = 0, lambda = cvRID$lambda.min) preRID &lt;- predict(outRID, newx = tesX) mseRID &lt;- mean((preRID - tesY)^2) pr2RID &lt;- 1 - mseRID / var(tesY) This block fits a Ridge Regression model, selects an optimal regularization parameter \\(\\lambda\\) via cross-validation, and evaluates the model’s performance on the test set. cvRID &lt;- cv.glmnet(x = traX, y = traY, alpha = 0) cv.glmnet() performs cross-validation to find the best \\(\\lambda\\) (regularization strength). alpha = 0 specifies Ridge Regression (if alpha = 1, it would perform Lasso instead). The function: Splits the training data into folds. Trains Ridge regression models with different \\(\\lambda\\) values. Selects the best \\(\\lambda\\) by minimizing cross-validated MSE. outRID &lt;- glmnet(x = traX, y = traY, alpha = 0, lambda = cvRID$lambda.min) Fits a Ridge Regression model using the best \\(\\lambda\\) found from cv.glmnet(). Key arguments: x = traX, y = traY: Training data. alpha = 0: Specifies Ridge Regression. lambda = cvRID$lambda.min: Uses the optimal \\(\\lambda\\) from cross-validation. Effect of Ridge Regularization: Shrinks all coefficients (unlike Lasso, which can set some to zero). Reduces variance by stabilizing estimates, especially in highly correlated predictor settings. preRID &lt;- predict(outRID, newx = tesX) Uses the trained Ridge model (outRID) to predict response values on the test set (tesX). mseRID &lt;- mean((preRID - tesY)^2) Computes the Mean Squared Error (MSE) on the test data: \\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} ( \\hat{y}_i - y_i )^2 \\] Lower MSE indicates better predictive performance. pr2RID &lt;- 1 - mseRID / var(tesY) Computes the predictive \\(R^2\\): \\[ R^2 = 1 - \\frac{\\text{MSE}}{\\text{Var}(\\text{Test } y)} \\] Interpretation: \\(R^2 \\approx 1\\) → Model explains most of the variance (good performance). \\(R^2 \\approx 0\\) → Model performs similarly to a constant predictor. \\(R^2 &lt; 0\\) → Model performs worse than predicting the mean (poor generalization). Summary of Key Variables: Variable Description cvRID Performs cross-validation to select optimal \\(\\lambda\\). outRID Trained Ridge Regression model with best \\(\\lambda\\). preRID Predicted values for the test set. mseRID Mean Squared Error on the test set (lower is better). pr2RID Predictive \\(R^2\\), measuring model generalization. Why Use Ridge Regression? Handles multicollinearity: When predictors are highly correlated, OLS estimates become unstable. Ridge stabilizes them. Reduces overfitting: Adding a penalty term \\(\\lambda \\|\\boldsymbol{\\beta}\\|_2^2\\) discourages overly large coefficients. Retains all features: Unlike Lasso, Ridge does not force coefficients to be exactly zero. 4.3.5 Lasso cvLAS &lt;- cv.glmnet(x = traX, y = traY, alpha = 1) plot(cvLAS) plot(cvLAS$glmnet.fit) outLAS &lt;- glmnet(x = traX, y = traY, alpha = 1, lambda = cvLAS$lambda.min) preLAS &lt;- predict(outLAS, newx = tesX) mseLAS &lt;- mean((preLAS - tesY)^2) pr2LAS &lt;- 1 - mseLAS / var(tesY) This block fits a Lasso Regression model, selects an optimal regularization parameter \\(\\lambda\\) via cross-validation, and evaluates the model’s performance on the test set. cvLAS &lt;- cv.glmnet(x = traX, y = traY, alpha = 1) cv.glmnet() performs cross-validation to find the best \\(\\lambda\\) (regularization strength). alpha = 1 specifies Lasso Regression (if alpha = 0, it would perform Ridge Regression). The function: Splits the training data into folds. Trains Lasso models with different \\(\\lambda\\) values. Selects the best \\(\\lambda\\) by minimizing cross-validated MSE. plot(cvLAS) plot(cvLAS$glmnet.fit) plot(cvLAS): Plots the cross-validation error for different values of \\(\\lambda\\). Shows the selected \\(\\lambda\\) (the one that minimizes error). plot(cvLAS$glmnet.fit): Plots the Lasso path: how coefficients change as \\(\\lambda\\) increases. Higher \\(\\lambda\\) values shrink more coefficients to zero, performing feature selection. outLAS &lt;- glmnet(x = traX, y = traY, alpha = 1, lambda = cvLAS$lambda.min) Fits a Lasso Regression model using the best \\(\\lambda\\) found from cv.glmnet(). Key arguments: x = traX, y = traY: Training data. alpha = 1: Specifies Lasso Regression. lambda = cvLAS$lambda.min: Uses the optimal \\(\\lambda\\) from cross-validation. Effect of Lasso Regularization: Shrinks some coefficients to exactly zero, performing automatic feature selection. Helps when only a subset of predictors is relevant. preLAS &lt;- predict(outLAS, newx = tesX) Uses the trained Lasso model (outLAS) to predict response values on the test set (tesX). mseLAS &lt;- mean((preLAS - tesY)^2) Computes the Mean Squared Error (MSE) on the test data: \\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} ( \\hat{y}_i - y_i )^2 \\] Lower MSE indicates better predictive performance. pr2LAS &lt;- 1 - mseLAS / var(tesY) Computes the predictive \\(R^2\\): \\[ R^2 = 1 - \\frac{\\text{MSE}}{\\text{Var}(\\text{Test } y)} \\] Interpretation: \\(R^2 \\approx 1\\) → Model explains most of the variance (good performance). \\(R^2 \\approx 0\\) → Model performs similarly to a constant predictor. \\(R^2 &lt; 0\\) → Model performs worse than predicting the mean (poor generalization). Summary of Key Variables: Variable Description cvLAS Performs cross-validation to select optimal \\(\\lambda\\). outLAS Trained Lasso Regression model with best \\(\\lambda\\). preLAS Predicted values for the test set. mseLAS Mean Squared Error on the test set (lower is better). pr2LAS Predictive \\(R^2\\), measuring model generalization. Why Use Lasso Regression? Feature Selection: Unlike Ridge, Lasso can shrink some coefficients exactly to zero, removing irrelevant predictors. Handles high-dimensional data: When \\(p &gt; n\\), Lasso helps by selecting the most relevant features. Interpretable models: Because some coefficients are set to zero, Lasso produces simpler models. 4.3.6 Basic Bayesian Regression outBAY &lt;- rmvnorm(n = 2000, mean = outRID$beta, sigma = s2 * solve(t(X) %*% X + cvRID$lambda.min * diag(numVar))) coeBAY &lt;- apply(X = outBAY, MARGIN = 2, FUN = median) preBAY &lt;- tesX %*% coeBAY mseBAY &lt;- mean((preBAY - tesY)^2) pr2BAY &lt;- 1 - mseBAY / var(tesY) outBAY &lt;- rmvnorm(n = 2000, mean = outRID$beta, sigma = s2 * solve(t(X) %*% X + cvRID$lambda.min * diag(numVar))) rmvnorm() generates 2,000 samples from a multivariate normal distribution, approximating the posterior distribution of the regression coefficients \\(\\boldsymbol{\\beta}\\). Posterior Mean: Uses the Ridge Regression solution (outRID$beta) as the mean of the distribution. Posterior Covariance: \\[ \\sigma^2 (\\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\] s2 * solve(t(X) %*% X + cvRID$lambda.min * diag(numVar)) is the posterior covariance matrix, incorporating both the observed data and the prior information. The \\(\\lambda \\mathbf{I}\\) term (from Ridge) ensures stability even if \\(\\mathbf{X}&#39; \\mathbf{X}\\) is singular.  Key Idea: Unlike Ridge, which gives a point estimate for \\(\\boldsymbol{\\beta}\\), Bayesian Ridge treats \\(\\boldsymbol{\\beta}\\) as a random variable and samples from its posterior distribution. coeBAY &lt;- apply(X = outBAY, MARGIN = 2, FUN = median) For each coefficient \\(\\beta_j\\), we take the median across the 2,000 posterior samples. This is a Bayesian point estimate, similar to the Ridge solution but incorporating posterior uncertainty. preBAY &lt;- tesX %*% coeBAY Uses the Bayesian posterior median coefficients to predict tesY. mseBAY &lt;- mean((preBAY - tesY)^2) Computes the Mean Squared Error (MSE) on the test data: \\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} ( \\hat{y}_i - y_i )^2 \\] Lower MSE indicates better predictive performance. pr2BAY &lt;- 1 - mseBAY / var(tesY) Computes the predictive \\(R^2\\): \\[ R^2 = 1 - \\frac{\\text{MSE}}{\\text{Var}(\\text{Test } y)} \\] Interpretation: \\(R^2 \\approx 1\\) → Model explains most of the variance (good performance). \\(R^2 \\approx 0\\) → Model performs similarly to a constant predictor. \\(R^2 &lt; 0\\) → Model performs worse than predicting the mean (poor generalization). Summary of Key Variables: Variable Description outBAY Posterior samples of \\(\\boldsymbol{\\beta}\\) from a Bayesian Ridge model. coeBAY Posterior median coefficients (Bayesian point estimate). preBAY Predicted values for the test set. mseBAY Mean Squared Error on the test set (lower is better). pr2BAY Predictive \\(R^2\\), measuring model generalization. Why Use Bayesian Ridge Regression? Incorporates uncertainty: Instead of a single estimate, we get a posterior distribution over \\(\\boldsymbol{\\beta}\\). Handles multicollinearity: Like Ridge, the prior shrinks coefficients to prevent instability. Flexibility: Bayesian methods allow incorporating informative priors when prior knowledge is available. 4.3.7 Bayesian Lasso outBLA &lt;- blasso(X = traX, y = traY, T = 500) ## t=100, m=215 ## t=200, m=209 ## t=300, m=226 ## t=400, m=203 coeBLA &lt;- apply(X = outBLA$beta, MARGIN = 2, FUN = median) preBLA &lt;- tesX %*% coeBLA mseBLA &lt;- mean((preBLA - tesY)^2) pr2BLA &lt;- 1 - mseBLA / var(tesY) This block implements a Bayesian version of Lasso Regression, where instead of solving a convex optimization problem (like traditional Lasso), it samples from the posterior distribution of the regression coefficients using a Bayesian framework. outBLA &lt;- blasso(X = traX, y = traY, T = 50) blasso() performs Bayesian Lasso regression, which introduces a Laplace (double-exponential) prior on \\(\\boldsymbol{\\beta}\\). Key argument: T = 50: Runs 50 iterations of a Markov Chain Monte Carlo (MCMC) sampler to draw posterior samples of \\(\\boldsymbol{\\beta}\\).  Bayesian Lasso vs. Traditional Lasso - Traditional Lasso: Estimates \\(\\boldsymbol{\\beta}\\) by solving an optimization problem with an \\(\\ell_1\\)-penalty. - Bayesian Lasso: Places a Laplace prior on \\(\\boldsymbol{\\beta}\\), then samples from the posterior. - Unlike Traditional Lasso, the Bayesian Lasso doesn’t perform exact variable selection. coeBLA &lt;- apply(X = outBLA$beta, MARGIN = 2, FUN = median) Takes the median of the sampled coefficients across the 50 iterations. This provides a point estimate similar to the Lasso solution but incorporates posterior uncertainty. preBLA &lt;- tesX %*% coeBLA Uses the posterior median coefficients to predict response values on the test set. mseBLA &lt;- mean((preBLA - tesY)^2) Computes the Mean Squared Error (MSE) on the test data: \\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} ( \\hat{y}_i - y_i )^2 \\] Lower MSE indicates better predictive performance. pr2BLA &lt;- 1 - mseBLA / var(tesY) Computes the predictive \\(R^2\\): \\[ R^2 = 1 - \\frac{\\text{MSE}}{\\text{Var}(\\text{Test } y)} \\] Interpretation: \\(R^2 \\approx 1\\) → Model explains most of the variance (good performance). \\(R^2 \\approx 0\\) → Model performs similarly to a constant predictor. \\(R^2 &lt; 0\\) → Model performs worse than predicting the mean (poor generalization). Summary of Key Variables: Variable Description outBLA Posterior samples of \\(\\boldsymbol{\\beta}\\) from the Bayesian Lasso model. coeBLA Posterior median coefficients (Bayesian point estimate). preBLA Predicted values for the test set. mseBLA Mean Squared Error on the test set (lower is better). pr2BLA Predictive \\(R^2\\), measuring model generalization. Why Use Bayesian Lasso Regression? Incorporates Uncertainty: Unlike standard Lasso, it provides posterior distributions instead of just point estimates. More Robust to Noise: Bayesian priors smooth out overfitting issues that can occur in traditional Lasso. 4.3.8 Bayesian Horseshoe Prior outHOR &lt;- horseshoe_sampler(X = X, y = y, S = 500) ## | | | 0% | | | 1% | |= | 1% | |= | 2% | |== | 2% | |== | 3% | |=== | 4% | |=== | 5% | |==== | 5% | |==== | 6% | |===== | 6% | |===== | 7% | |===== | 8% | |====== | 8% | |====== | 9% | |======= | 9% | |======= | 10% | |======== | 11% | |======== | 12% | |========= | 12% | |========= | 13% | |========== | 13% | |========== | 14% | |=========== | 15% | |=========== | 16% | |============ | 16% | |============ | 17% | |============= | 17% | |============= | 18% | |============= | 19% | |============== | 19% | |============== | 20% | |=============== | 20% | |=============== | 21% | |================ | 22% | |================ | 23% | |================= | 23% | |================= | 24% | |================== | 24% | |================== | 25% | |================== | 26% | |=================== | 26% | |=================== | 27% | |==================== | 27% | |==================== | 28% | |===================== | 29% | |===================== | 30% | |====================== | 30% | |====================== | 31% | |======================= | 31% | |======================= | 32% | |======================= | 33% | |======================== | 33% | |======================== | 34% | |========================= | 34% | |========================= | 35% | |========================== | 36% | |========================== | 37% | |=========================== | 37% | |=========================== | 38% | |============================ | 38% | |============================ | 39% | |============================= | 40% | |============================= | 41% | |============================== | 41% | |============================== | 42% | |=============================== | 42% | |=============================== | 43% | |=============================== | 44% | |================================ | 44% | |================================ | 45% | |================================= | 45% | |================================= | 46% | |================================== | 47% | |================================== | 48% | |=================================== | 48% | |=================================== | 49% | |==================================== | 49% | |==================================== | 50% | |==================================== | 51% | |===================================== | 51% | |===================================== | 52% | |====================================== | 52% | |====================================== | 53% | |======================================= | 54% | |======================================= | 55% | |======================================== | 55% | |======================================== | 56% | |========================================= | 56% | |========================================= | 57% | |========================================= | 58% | |========================================== | 58% | |========================================== | 59% | |=========================================== | 59% | |=========================================== | 60% | |============================================ | 61% | |============================================ | 62% | |============================================= | 62% | |============================================= | 63% | |============================================== | 63% | |============================================== | 64% | |=============================================== | 65% | |=============================================== | 66% | |================================================ | 66% | |================================================ | 67% | |================================================= | 67% | |================================================= | 68% | |================================================= | 69% | |================================================== | 69% | |================================================== | 70% | |=================================================== | 70% | |=================================================== | 71% | |==================================================== | 72% | |==================================================== | 73% | |===================================================== | 73% | |===================================================== | 74% | |====================================================== | 74% | |====================================================== | 75% | |====================================================== | 76% | |======================================================= | 76% | |======================================================= | 77% | |======================================================== | 77% | |======================================================== | 78% | |========================================================= | 79% | |========================================================= | 80% | |========================================================== | 80% | |========================================================== | 81% | |=========================================================== | 81% | |=========================================================== | 82% | |=========================================================== | 83% | |============================================================ | 83% | |============================================================ | 84% | |============================================================= | 84% | |============================================================= | 85% | |============================================================== | 86% | |============================================================== | 87% | |=============================================================== | 87% | |=============================================================== | 88% | |================================================================ | 88% | |================================================================ | 89% | |================================================================= | 90% | |================================================================= | 91% | |================================================================== | 91% | |================================================================== | 92% | |=================================================================== | 92% | |=================================================================== | 93% | |=================================================================== | 94% | |==================================================================== | 94% | |==================================================================== | 95% | |===================================================================== | 95% | |===================================================================== | 96% | |====================================================================== | 97% | |====================================================================== | 98% | |======================================================================= | 98% | |======================================================================= | 99% | |========================================================================| 99% | |========================================================================| 100% coeHOR &lt;- apply(X = outHOR$B, MARGIN = 2, FUN = median) preHOR &lt;- tesX %*% coeHOR mseHOR &lt;- mean((preHOR - tesY)^2) pr2HOR &lt;- 1 - mseHOR / var(tesY) This block implements Bayesian regression using the Horseshoe prior, which is particularly useful for sparse models where only a small subset of predictors are truly relevant. The Horseshoe prior is known for strong shrinkage of irrelevant coefficients while allowing large signals to remain unshrunk. In this case custom code is used to perform faster sampling when \\(p &gt; n\\). outHOR &lt;- horseshoe_sampler(X = X, y = y, S = 500) horseshoe_sampler() performs Bayesian regression with a Horseshoe prior, which is an adaptive shrinkage prior. Key argument: S = 500: Runs 500 MCMC iterations to generate posterior samples of the coefficients \\(\\boldsymbol{\\beta}\\).  Horseshoe Prior vs. Other Bayesian Methods - Bayesian Ridge: Uses a Gaussian prior, shrinking all coefficients uniformly. - Bayesian Lasso: Uses a Laplace prior, putting more weight at zero in the prior. - Horseshoe Prior: Uses a hierarchical prior that strongly shrinks small coefficients but allows large coefficients to stay large. coeHOR &lt;- apply(X = outHOR$B, MARGIN = 2, FUN = median) Takes the median of the posterior samples for each coefficient. This provides a point estimate similar to Bayesian Lasso but with adaptive shrinkage. preHOR &lt;- tesX %*% coeHOR Uses the posterior median coefficients to predict response values on the test set. mseHOR &lt;- mean((preHOR - tesY)^2) Computes the Mean Squared Error (MSE) on the test data: \\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} ( \\hat{y}_i - y_i )^2 \\] Lower MSE indicates better predictive performance. pr2HOR &lt;- 1 - mseHOR / var(tesY) Computes the predictive \\(R^2\\): \\[ R^2 = 1 - \\frac{\\text{MSE}}{\\text{Var}(\\text{Test } y)} \\] Interpretation: \\(R^2 \\approx 1\\) → Model explains most of the variance (good performance). \\(R^2 \\approx 0\\) → Model performs similarly to a constant predictor. \\(R^2 &lt; 0\\) → Model performs worse than predicting the mean (poor generalization). Summary of Key Variables: Variable Description outHOR Posterior samples of \\(\\boldsymbol{\\beta}\\) from the Horseshoe prior model. coeHOR Posterior median coefficients (Bayesian point estimate). preHOR Predicted values for the test set. mseHOR Mean Squared Error on the test set (lower is better). pr2HOR Predictive \\(R^2\\), measuring model generalization. Why Use the Horseshoe Prior? Handles Sparse Models Well: Encourages strong shrinkage for irrelevant coefficients while keeping relevant ones intact. Better than Lasso for Small Signals: Lasso tends to shrink all coefficients, while Horseshoe allows some to stay large. Works in High-Dimensional Settings: Performs well when \\(p &gt; n\\), where traditional methods like OLS and Ridge struggle. 4.3.9 Results Comparisson 4.3.9.1 Estimation Comparisson # Plots Both coeInd &lt;- 1 denBAY &lt;- density(x = outBAY[, coeInd]) denBLA &lt;- density(x = outBLA$beta[101:500, coeInd]) denHOR &lt;- density(x = outHOR$B[101:500, coeInd]) xmin &lt;- min(b[coeInd], outRID$beta[coeInd], outLAS$beta[coeInd], outBAY[, coeInd], outBLA$beta[101:500, coeInd], outHOR$B[101:500, coeInd]) xmax &lt;- max(b[coeInd], outRID$beta[coeInd], outLAS$beta[coeInd], outBAY[, coeInd], outBLA$beta[101:500, coeInd], outHOR$B[101:500, coeInd]) ymax &lt;- max(denBAY$y, denBLA$y, denHOR$y) plot(denBAY, col = rgb(1, 0, 0), xlim = c(xmin, xmax), ylim = c( 0, ymax), main = paste0(&quot;Coefficient &quot;, coeInd), xlab = bquote(beta[.(coeInd)])) polygon(denBAY, col = rgb(1, 0, 0, 0.5), border = NA) par(new=TRUE) plot(denBLA, col = rgb(1, 0, 1), xlim = c(xmin, xmax), ylim = c( 0, ymax), main = &quot;&quot;, xaxt = &quot;n&quot;, xlab = &quot;&quot;) polygon(denBLA, col = rgb(1, 0, 1, 0.5), border = NA) par(new=TRUE) plot(denHOR, col = rgb(0, 1, 1), xlim = c(xmin, xmax), ylim = c( 0, ymax), main = &quot;&quot;, xaxt = &quot;n&quot;, xlab = &quot;&quot;) polygon(denHOR, col = rgb(0, 1, 1, 0.5), border = NA) abline(v = b[coeInd], lwd = 2, col = rgb(0, 0, 0)) abline(v = outRID$beta[coeInd], lwd = 2, col = rgb(0, 0, 1)) abline(v = outLAS$beta[coeInd], lwd = 2, col = rgb(0, 1, 0)) This code visualizes the posterior distributions of a single regression coefficient \\(\\beta_{\\text{coeInd}}\\) estimated using Bayesian Ridge, Bayesian Lasso, and Bayesian Horseshoe priors. Additionally, it compares these estimates to those from Ridge and Lasso regression. Why This Matters? Ridge (blue line) shrinks coefficients but never sets them exactly to zero. Lasso (green line) performs feature selection by setting some coefficients exactly to zero. Bayesian Ridge (red curve) resembles Ridge but accounts for uncertainty. Bayesian Lasso (purple curve) is similar to Lasso but includes a distribution over coefficients. Horseshoe (cyan curve) applies strong shrinkage to small coefficients while allowing important ones to remain large. coeInd &lt;- 1 coeInd = 1 selects the first coefficient (\\(\\beta_1\\)) for visualization. The user can change this to plot other coefficients. denBAY &lt;- density(x = outBAY[, coeInd]) denBLA &lt;- density(x = outBLA$beta[101:500, coeInd]) denHOR &lt;- density(x = outHOR$B[101:500, coeInd]) density() estimates the probability density function (PDF) of the sampled coefficients. Bayesian Models: outBAY[, coeInd]: Posterior samples from Bayesian Ridge Regression. outBLA$beta[101:500, coeInd]: Posterior samples from Bayesian Lasso (excluding first 100 as burn-in). outHOR$B[101:500, coeInd]: Posterior samples from Bayesian Horseshoe (excluding first 100 as burn-in).  Why exclude the first 100 samples? - In MCMC sampling, the first few iterations (burn-in period) may not be from the true posterior distribution. xmin &lt;- min(b[coeInd], outRID$beta[coeInd], outLAS$beta[coeInd], outBAY[, coeInd], outBLA$beta[101:500, coeInd], outHOR$B[101:500, coeInd]) xmax &lt;- max(b[coeInd], outRID$beta[coeInd], outLAS$beta[coeInd], outBAY[, coeInd], outBLA$beta[101:500, coeInd], outHOR$B[101:500, coeInd]) ymax &lt;- max(denBAY$y, denBLA$y, denHOR$y) xmin &amp; xmax: Find the minimum and maximum values of the estimated coefficients to set the x-axis range. ymax: Finds the maximum density value across all models to set the y-axis range. plot(denBAY, col = rgb(1, 0, 0), xlim = c(xmin, xmax), ylim = c(0, ymax), main = paste0(&quot;Coefficient &quot;, coeInd), xlab = bquote(beta[.(coeInd)])) polygon(denBAY, col = rgb(1, 0, 0, 0.5), border = NA) Plots the density of Bayesian Ridge estimates (in red). polygon() fills the area under the curve for better visualization. par(new=TRUE) plot(denBLA, col = rgb(1, 0, 1), xlim = c(xmin, xmax), ylim = c(0, ymax), main = &quot;&quot;, xaxt = &quot;n&quot;, xlab = &quot;&quot;) polygon(denBLA, col = rgb(1, 0, 1, 0.5), border = NA) Plots Bayesian Lasso estimates (in purple). par(new=TRUE) allows overlaying this plot on the previous one. par(new=TRUE) plot(denHOR, col = rgb(0, 1, 1), xlim = c(xmin, xmax), ylim = c(0, ymax), main = &quot;&quot;, xaxt = &quot;n&quot;, xlab = &quot;&quot;) polygon(denHOR, col = rgb(0, 1, 1, 0.5), border = NA) Plots Bayesian Horseshoe estimates (in cyan/light blue). The densities overlap, allowing a direct comparison of uncertainty across models. abline(v = b[coeInd], lwd = 2, col = rgb(0, 0, 0)) # True coefficient (black) abline(v = outRID$beta[coeInd], lwd = 2, col = rgb(0, 0, 1)) # Ridge estimate (blue) abline(v = outLAS$beta[coeInd], lwd = 2, col = rgb(0, 1, 0)) # Lasso estimate (green) Black line → True coefficient value. Blue line → Ridge estimate. Green line → Lasso estimate.  Why are Ridge and Lasso shown separately? - Unlike Bayesian methods, Ridge and Lasso provide point estimates instead of full posterior distributions. 4.3.9.2 Performance Metrics # Create a table with rounded values coef_table &lt;- round(cbind( True_Beta = b, OLS = outOLS$coefficients, Ridge = as.numeric(outRID$beta), Lasso = as.numeric(outLAS$beta), B_Ridge = coeBAY, B_Lasso = coeBLA, Horseshoe = coeHOR )[1:15, ], 4) rownames(coef_table) &lt;- paste0(&quot;Coefficient &quot;, 1:15) # Print as a formatted table kable(as.matrix(coef_table), caption = &quot;Comparison of Coefficient Estimates Across Methods&quot;) Table 4.1: Comparison of Coefficient Estimates Across Methods True_Beta OLS Ridge Lasso B_Ridge B_Lasso Horseshoe Coefficient 1 0.7788 1.0284 0.1298 0.7248 0.1291 0.7485 0.7657 Coefficient 2 0.6065 0.8395 0.1416 0.6148 0.1407 0.6366 0.6066 Coefficient 3 0.4724 1.2123 0.1398 0.4795 0.1414 0.4802 0.4812 Coefficient 4 0.3679 -0.0911 0.1192 0.3627 0.1187 0.3661 0.3635 Coefficient 5 0.2865 -0.5125 0.0986 0.2963 0.0993 0.2965 0.2957 Coefficient 6 0.2231 1.1521 0.0778 0.2083 0.0782 0.2129 0.2056 Coefficient 7 0.1738 -0.8806 0.0567 0.1360 0.0564 0.1474 0.1661 Coefficient 8 0.1353 -0.0308 0.0382 0.1173 0.0385 0.1154 0.1175 Coefficient 9 0.1054 0.7145 0.0288 0.0585 0.0285 0.0616 0.0970 Coefficient 10 0.0821 0.3688 0.0261 0.0858 0.0280 0.1142 0.0829 Coefficient 11 0.0000 -0.1746 -0.0186 0.0000 -0.0176 0.0000 -0.0001 Coefficient 12 0.0000 0.5834 -0.0202 0.0000 -0.0208 0.0000 -0.0001 Coefficient 13 0.0000 -0.4630 -0.0106 0.0000 -0.0114 0.0057 0.0000 Coefficient 14 0.0000 -0.3090 -0.0019 0.0000 -0.0022 0.0000 0.0000 Coefficient 15 0.0000 0.0410 -0.0051 0.0000 -0.0045 0.0000 0.0000 Observations: - OLS coefficients tend to be large and unstable due to multicollinearity. - Ridge shrinks all coefficients but keeps them nonzero. - Lasso performs feature selection, setting some coefficients exactly to zero. - Bayesian methods provide uncertainty-aware estimates and adaptive shrinkage. - Horseshoe applies strong shrinkage to irrelevant coefficients while keeping relevant ones unshrunk. Would you like to include a heatmap visualization of these coefficient estimates?  4.3.9.3 Predictive Performance # Create a performance table performance_table &lt;- data.frame( Method = c(&quot;OLS&quot;, &quot;Ridge&quot;, &quot;Lasso&quot;, &quot;Bayesian Ridge&quot;, &quot;Bayesian Lasso&quot;, &quot;Horseshoe&quot;), MSE = round(c(mseOLS, mseRID, mseLAS, mseBAY, mseBLA, mseHOR), 4), Predictive_R2 = round(c(pr2OLS, pr2RID, pr2LAS, pr2BAY, pr2BLA, pr2HOR), 4) ) # Print as a formatted table kable(performance_table, caption = &quot;Model Comparison: MSE and Predictive R²&quot;) Table 4.2: Model Comparison: MSE and Predictive R² Method MSE Predictive_R2 OLS 35.7967 -8.0661 Ridge 2.3529 0.4041 Lasso 0.1065 0.9730 Bayesian Ridge 2.3150 0.4137 Bayesian Lasso 0.1162 0.9706 Horseshoe 0.0939 0.9762 Interpretation Lower MSE → Better predictive performance. Higher \\(R^2\\) → Model explains more variance in the test set. OLS likely performs the worst due to overfitting and instability. Ridge improves stability but retains all predictors. Lasso performs feature selection, possibly improving interpretability. Bayesian methods incorporate uncertainty and adaptive shrinkage. Horseshoe is expected to perform well in sparse settings. 4.3.9.4 Variable Selection Post-Processing # No post-processing necessary estBLA &lt;- apply(X = outBLA$beta, MARGIN = 2, FUN = median) estLAS &lt;- as.numeric(outLAS$beta) # Post-processing for Ridge regression estRID &lt;- outRID$beta kmeRID &lt;- kmeans(x = as.numeric(abs(outRID$beta)), centers = 2) if(kmeRID$centers[1] &gt; kmeRID$centers[2]){ estRID[kmeRID$cluster == 2] &lt;- 0 } else { estRID[kmeRID$cluster == 1] &lt;- 0 } estRID &lt;- as.numeric(estRID) # Post-processing for Basic Bayesian regression kmeBAY &lt;- kmeans(x = as.numeric(abs(outBAY)), centers = 2) if(kmeBAY$centers[1] &gt; kmeBAY$centers[2]){ outBAY[kmeBAY$cluster == 2] &lt;- 0 } else { outBAY[kmeBAY$cluster == 1] &lt;- 0 } estBAY &lt;- apply(X = outBAY, MARGIN = 2, FUN = median) # Post-processing for Horseshoe Regression coeHOR &lt;- outHOR$B kmeHOR &lt;- kmeans(x = as.numeric(abs(coeHOR)), centers = 2) if(kmeHOR$centers[1] &gt; kmeHOR$centers[2]){ coeHOR[kmeHOR$cluster == 2] &lt;- 0 } else { coeHOR[kmeHOR$cluster == 1] &lt;- 0 } estHOR &lt;- apply(X = coeHOR, MARGIN = 2, FUN = median) # Create a table with rounded values coef_table &lt;- round(cbind( True_Beta = b, Ridge = estRID, Lasso = estLAS, B_Ridge = estBAY, B_Lasso = estBLA, Horseshoe = estHOR )[1:15, ], 4) rownames(coef_table) &lt;- paste0(&quot;Coefficient &quot;, 1:15) # Print as a formatted table kable(as.matrix(coef_table), caption = &quot;Comparison of Post-processed Coefficient Across Methods&quot;) Table 4.3: Comparison of Post-processed Coefficient Across Methods True_Beta Ridge Lasso B_Ridge B_Lasso Horseshoe Coefficient 1 0.7788 0.1298 0.7248 0.1291 0.7485 0.7657 Coefficient 2 0.6065 0.1416 0.6148 0.1407 0.6366 0.6066 Coefficient 3 0.4724 0.1398 0.4795 0.1414 0.4802 0.4812 Coefficient 4 0.3679 0.1192 0.3627 0.1187 0.3661 0.3635 Coefficient 5 0.2865 0.0986 0.2963 0.0993 0.2965 0.2957 Coefficient 6 0.2231 0.0778 0.2083 0.0782 0.2129 0.0000 Coefficient 7 0.1738 0.0000 0.1360 0.0564 0.1474 0.0000 Coefficient 8 0.1353 0.0000 0.1173 0.0385 0.1154 0.0000 Coefficient 9 0.1054 0.0000 0.0585 0.0000 0.0616 0.0000 Coefficient 10 0.0821 0.0000 0.0858 0.0000 0.1142 0.0000 Coefficient 11 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 Coefficient 12 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 Coefficient 13 0.0000 0.0000 0.0000 0.0000 0.0057 0.0000 Coefficient 14 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 Coefficient 15 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 This block refines and compares the coefficient estimates across different regression methods. Some methods, like Bayesian Ridge and Horseshoe, require post-processing to identify which coefficients should be set to zero. The final output is a table comparing the first 15 coefficients across methods. estBLA &lt;- apply(X = outBLA$beta, MARGIN = 2, FUN = median) estLAS &lt;- as.numeric(outLAS$beta) estBLA: Extracts posterior median estimates from Bayesian Lasso. estLAS: Converts Lasso coefficients to a numeric vector. No post-processing is required for these methods since Lasso and Bayesian Lasso naturally set some coefficients to zero. estRID &lt;- outRID$beta kmeRID &lt;- kmeans(x = as.numeric(abs(outRID$beta)), centers = 2) if(kmeRID$centers[1] &gt; kmeRID$centers[2]){ estRID[kmeRID$cluster == 2] &lt;- 0 } else { estRID[kmeRID$cluster == 1] &lt;- 0 } estRID &lt;- as.numeric(estRID) Ridge Regression retains all coefficients (i.e., does not set any to zero). Post-processing approach: Uses K-means clustering to separate large and small coefficients into two groups (centers = 2). The group with the smaller average magnitude is assumed to contain insignificant coefficients, so they are set to zero.  Why is this needed? - Ridge does not perform feature selection, so this helps identify zero coefficients. kmeBAY &lt;- kmeans(x = as.numeric(abs(outBAY)), centers = 2) if(kmeBAY$centers[1] &gt; kmeBAY$centers[2]){ outBAY[kmeBAY$cluster == 2] &lt;- 0 } else { outBAY[kmeBAY$cluster == 1] &lt;- 0 } estBAY &lt;- apply(X = outBAY, MARGIN = 2, FUN = median) Bayesian Ridge also does not perform feature selection, so we apply K-means post-processing. The median of the posterior samples is used as the final estimate. coeHOR &lt;- outHOR$B kmeHOR &lt;- kmeans(x = as.numeric(abs(coeHOR)), centers = 2) if(kmeHOR$centers[1] &gt; kmeHOR$centers[2]){ coeHOR[kmeHOR$cluster == 2] &lt;- 0 } else { coeHOR[kmeHOR$cluster == 1] &lt;- 0 } estHOR &lt;- apply(X = coeHOR, MARGIN = 2, FUN = median) The Horseshoe prior naturally encourages sparsity, but we reinforce it using the K-means clustering method. The posterior median is used as the final estimate. # Create a table with rounded values coef_table &lt;- round(cbind( True_Beta = b, Ridge = estRID, Lasso = estLAS, B_Ridge = estBAY, B_Lasso = estBLA, Horseshoe = estHOR )[1:15, ], 4) rownames(coef_table) &lt;- paste0(&quot;Coefficient &quot;, 1:15) # Print as a formatted table kable(as.matrix(coef_table), caption = &quot;Comparison of Post-processed Coefficient Across Methods&quot;) Combines all coefficient estimates (True \\(\\beta\\), Ridge, Lasso, Bayesian Ridge, Bayesian Lasso, Horseshoe). Rounds values to 4 decimal places for readability. Formats the table using kable() for a clean output. Key Insights - OLS and Ridge Regression retain all coefficients → Ridge needs post-processing to remove small values. - Lasso and Bayesian Lasso naturally set some coefficients exactly to zero. - Bayesian Ridge needs post-processing to enforce sparsity. - Horseshoe Prior is adaptive, but post-processing enhances sparsity. 4.4 Efficient Computation When dealing with high-dimensional settings where \\(p &gt; n\\), direct computation of matrix inverses and decompositions becomes computationally expensive. Specifically: Matrix inversion for an \\(n \\times p\\) matrix is typically \\(O(p^3)\\), which is prohibitive when \\(p\\) is large. Cholesky decomposition, commonly used for solving linear systems, is \\(O(p^3)\\) as well. To make computations more efficient, we can express operations in terms of \\(n\\) rather than \\(p\\) whenever possible. A key technique in Machine Learning and Statistics is the Woodbury Matrix Identity, which allows us to rewrite certain matrix inversions in a computationally cheaper form. 4.4.1 Efficient Computation of Ridge Regression using the Woodbury Identity Ridge regression estimates the coefficients \\(\\boldsymbol{\\beta}\\) as: \\[ \\hat{\\boldsymbol{\\beta}}_{\\text{ridge}} = (\\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\mathbf{X}&#39; \\mathbf{y} \\] where: - \\(\\mathbf{X}\\) is an \\(n \\times p\\) design matrix, - \\(\\lambda \\mathbf{I}\\) is a regularization term (where \\(\\lambda &gt; 0\\) ensures invertibility), - \\(\\mathbf{y}\\) is an \\(n \\times 1\\) response vector. Computing \\((\\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I})^{-1}\\) directly is expensive when \\(p\\) is large. Instead, we use the Woodbury Identity, which states: \\[ (\\mathbf{A} + \\mathbf{U} \\mathbf{C} \\mathbf{V})^{-1} = \\mathbf{A}^{-1} - \\mathbf{A}^{-1} \\mathbf{U} (\\mathbf{C}^{-1} + \\mathbf{V} \\mathbf{A}^{-1} \\mathbf{U})^{-1} \\mathbf{V} \\mathbf{A}^{-1} \\] To apply this to Ridge Regression, let’s define: \\(\\mathbf{A} = \\lambda \\mathbf{I}_p\\) (a \\(p \\times p\\) diagonal matrix), \\(\\mathbf{U} = \\mathbf{X}&#39;\\) (a \\(p \\times n\\) matrix), \\(\\mathbf{C} = \\mathbf{I}_n\\) (an \\(n \\times n\\) identity matrix), \\(\\mathbf{V} = \\mathbf{X}\\) (an \\(n \\times p\\) matrix). Applying the Woodbury Identity: \\[ (\\mathbf{X}&#39; \\mathbf{X} + \\lambda \\mathbf{I})^{-1} = \\lambda^{-1} \\mathbf{I} - \\lambda^{-1} \\mathbf{X}&#39; (\\mathbf{I} + \\mathbf{X} \\lambda^{-1} \\mathbf{X}&#39;)^{-1} \\mathbf{X} \\lambda^{-1} \\] Substituting this into the Ridge Regression equation: \\[ \\hat{\\boldsymbol{\\beta}}_{\\text{ridge}} = \\left( \\lambda^{-1} \\mathbf{I} - \\lambda^{-1} \\mathbf{X}&#39; (\\mathbf{I} + \\mathbf{X} \\lambda^{-1} \\mathbf{X}&#39;)^{-1} \\mathbf{X} \\lambda^{-1} \\right) \\mathbf{X}&#39; \\mathbf{y} \\] Rearranging: \\[ \\hat{\\boldsymbol{\\beta}}_{\\text{ridge}} = \\lambda^{-1} \\mathbf{X}&#39; \\mathbf{y} - \\lambda^{-1} \\mathbf{X}&#39; (\\mathbf{I} + \\mathbf{X} \\lambda^{-1} \\mathbf{X}&#39;)^{-1} \\mathbf{X} \\lambda^{-1} \\mathbf{X}&#39; \\mathbf{y} \\] Defining \\(n \\times n\\) matrix inversion instead of \\(p \\times p\\): \\[ \\mathbf{M} = (\\mathbf{I} + \\mathbf{X} \\lambda^{-1} \\mathbf{X}&#39;)^{-1} \\] \\[ \\hat{\\boldsymbol{\\beta}}_{\\text{ridge}} = \\lambda^{-1} \\mathbf{X}&#39; \\mathbf{y} - \\lambda^{-1} \\mathbf{X}&#39; \\mathbf{M} \\mathbf{X} \\lambda^{-1} \\mathbf{X}&#39; \\mathbf{y} \\] Computational Benefits: Direct inversion of \\(\\mathbf{X}&#39; \\mathbf{X} + \\lambda {\\boldsymbol I} \\) (size \\(p \\times p\\)) is \\(O(p^3)\\). Computing \\(\\mathbf{M}\\) involves inverting an \\(n \\times n\\) matrix, which is only \\(O(n^3)\\) (much faster when \\(p \\gg n\\)). Since \\(n \\ll p\\) in high-dimensional settings, this trick significantly reduces computational cost. Conclusion: Using the Woodbury Identity, we reformulate the Ridge Regression inversion in terms of \\(n\\) instead of \\(p\\). This approach is particularly useful in high-dimensional settings where \\(p &gt; n\\). Many Bayesian methods, such as Gaussian Process Regression and Variational Inference, also rely on the Woodbury identity for computational efficiency. 4.4.2 Efficient Bayesian Sampling for Gaussian Scale-Mixture Priors (Based on Bhattacharya et al. (2016), “Fast Sampling with Gaussian Scale-Mixture Priors in High-Dimensional Regression”) 4.4.2.1 Motivation Gaussian scale-mixture priors, such as the Horseshoe prior and Bayesian Lasso, are widely used for high-dimensional regression due to their adaptive shrinkage properties. These priors impose global-local shrinkage on regression coefficients, ensuring that irrelevant predictors are strongly shrunk, while allowing large signals to remain unshrunk. However, standard Markov Chain Monte Carlo (MCMC) sampling for these priors can be computationally expensive when \\(p \\gg n\\), particularly due to the matrix inversion required in Gibbs sampling steps. Existing approaches rely on Cholesky decomposition, which has a computational complexity of \\(O(p^3)\\), making it prohibitive for large-scale problems. Bhattacharya et al. (2016) propose an exact sampling algorithm that avoids direct matrix inversion by leveraging: 1. Latent variable augmentation to restructure the sampling problem. 2. Sherman–Morrison–Woodbury identity to reduce matrix inversion complexity from \\(O(p^3)\\) to \\(O(n^3)\\). 3. Block Gibbs sampling, allowing efficient updates of the regression coefficients. This method scales linearly with \\(p\\), making Bayesian inference feasible in high-dimensional settings. 4.4.2.2 Model Setup: Gaussian Scale-Mixture Priors in Bayesian Regression We consider the standard Bayesian linear regression model: \\[ \\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{e}, \\quad \\mathbf{e} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}) \\] where: - \\(\\mathbf{X}\\) is the \\(n \\times p\\) design matrix. - \\(\\boldsymbol{\\beta}\\) is the \\(p\\)-dimensional vector of regression coefficients. - \\(\\mathbf{y}\\) is the \\(n\\)-dimensional response vector. - \\(\\mathbf{e}\\) represents independent Gaussian noise. A Gaussian scale-mixture prior is imposed on \\(\\boldsymbol{\\beta}\\): \\[ \\beta_j | \\lambda_j, \\tau, \\sigma^2 \\sim \\mathcal{N}(0, \\lambda_j^2 \\tau^2 \\sigma^2) \\] where: - \\(\\lambda_j\\) are local shrinkage parameters, controlling the sparsity of each coefficient. - \\(\\tau\\) is a global shrinkage parameter, controlling overall sparsity. - Different choices of \\(p(\\lambda_j)\\) lead to different priors (e.g., Horseshoe, Bayesian Lasso). The full conditional posterior of \\(\\boldsymbol{\\beta}\\) is given by: \\[ \\boldsymbol{\\beta} | \\mathbf{y}, \\lambda, \\tau, \\sigma^2 \\sim \\mathcal{N} \\left( \\mathbf{A}^{-1} \\mathbf{X}&#39; \\mathbf{y}, \\sigma^2 \\mathbf{A}^{-1} \\right) \\] where: \\[ \\mathbf{A} = \\mathbf{X}&#39; \\mathbf{X} + \\Lambda^{-1}, \\quad \\Lambda = \\tau^2 \\text{diag}(\\lambda_1^2, ..., \\lambda_p^2). \\] Computing \\(\\mathbf{A}^{-1}\\) directly is prohibitively expensive for large \\(p\\), motivating the need for an efficient sampling algorithm. To sample from: \\[ \\boldsymbol{\\beta} \\sim \\mathcal{N} \\left( \\mathbf{A}^{-1} \\mathbf{X}&#39; \\mathbf{y}, \\sigma^2 \\mathbf{A}^{-1} \\right) \\] define: \\[ \\mathbf{Q} = (\\mathbf{X}&#39; \\mathbf{X} + \\Lambda^{-1})^{-1}, \\quad \\mathbf{b} = \\mathbf{X}&#39; \\mathbf{y}. \\] Instead of inverting \\(\\mathbf{Q}\\), introduce an auxiliary variable: \\[ \\mathbf{v} = \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{z}, \\quad \\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) \\] which allows a Woodbury-based reformulation of the problem. 4.4.2.3 Fast Gibbs Sampling Algorithm Step 1: Sample Auxiliary Variables Sample \\(\\mathbf{u} \\sim \\mathcal{N}(\\mathbf{0}, \\Lambda)\\). Sample \\(\\boldsymbol{\\delta} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_n)\\). Step 2: Compute a Latent Variable Compute: \\[ \\mathbf{v} = \\mathbf{X} \\mathbf{u} + \\boldsymbol{\\delta} \\] Step 3: Solve a Linear System Solve for \\(\\mathbf{w}\\) in: \\[ (\\mathbf{X} \\Lambda \\mathbf{X}&#39; + \\mathbf{I}) \\mathbf{w} = (\\mathbf{y} - \\mathbf{v}) \\] This avoids inverting \\(\\mathbf{X}&#39; \\mathbf{X} + \\Lambda^{-1}\\). Step 4: Compute the Updated Sample for \\(\\boldsymbol{\\beta}\\) Compute: \\[ \\boldsymbol{\\beta} = \\mathbf{u} + \\Lambda \\mathbf{X}&#39; \\mathbf{w} \\]  Key Insight: Instead of inverting a \\(p \\times p\\) matrix, the algorithm solves a linear system of size \\(n \\times n\\), reducing computational complexity from \\(O(p^3)\\) to \\(O(n^3)\\). 4.4.2.4 Computational Efficiency Standard MCMC: Requires inverting \\(p \\times p\\) matrices (\\(O(p^3)\\)). Proposed method: Uses matrix multiplications and solves a system of size \\(n \\times n\\) (\\(O(n^3)\\)). When \\(p \\gg n\\), this leads to significant computational savings. For example, the algorithm achieves a 250× speedup when \\(p = 5000\\) compared to standard methods. "],["principal-component-analysis.html", "5 Principal Component Analysis 5.1 PCA as a Low-Rank Approximation of \\(\\mathbf{X}\\) 5.2 Variance Maximization in PCA 5.3 PARAFAC decomposition 5.4 High-Dimensional PCA 5.5 Principal Components Regression", " 5 Principal Component Analysis Principal Component Analysis (PCA) is a widely used dimensionality reduction technique in statistics and machine learning. While it is often introduced as a method for finding orthogonal directions of maximum variance, another fundamental perspective is that PCA provides the best low-rank approximation of the data matrix \\(\\mathbf{X}\\). In many real-world datasets, the observed variables exhibit significant redundancy due to correlations between features. This means that the true intrinsic dimensionality of the data is often much lower than the number of measured variables. PCA exploits this redundancy by representing the data using a lower-dimensional subspace while preserving as much of the original information as possible. 5.1 PCA as a Low-Rank Approximation of \\(\\mathbf{X}\\) Let \\(\\mathbf{X}\\) be an \\(n \\times p\\) data matrix, where \\(n\\) is the number of observations and \\(p\\) is the number of variables. The goal of PCA is to find a low-rank representation of \\(\\mathbf{X}\\) that captures its most important structure. This is achieved through the Singular Value Decomposition (SVD): \\[ \\mathbf{X} = \\sum_{i=1}^{r} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i&#39; \\] where: - \\(\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_r &gt; 0\\) are the singular values of \\(\\mathbf{X}\\), - \\(\\mathbf{u}_i\\) and \\(\\mathbf{v}_i\\) are the corresponding left and right singular vectors, - \\(r = \\text{rank}(\\mathbf{X})\\). PCA provides a rank-\\(k\\) approximation by retaining only the top \\(k\\) singular values and singular vectors, leading to: \\[ \\mathbf{X}_k = \\sum_{i=1}^{k} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i&#39;. \\] By the Eckart–Young–Mirsky theorem, this is the best approximation to \\(\\mathbf{X}\\) under the Frobenius norm, meaning it minimizes the reconstruction error: \\[ \\|\\mathbf{X} - \\mathbf{X}_k\\|_F^2 = \\sum_{i=k+1}^{r} \\sigma_i^2. \\] 5.1.1 Why Low-Rank Approximation Matters Dimensionality Reduction The original data matrix \\(\\mathbf{X}\\) is often high-dimensional and contains redundant information. PCA allows us to approximate \\(\\mathbf{X}\\) with a much smaller representation while preserving most of the variance. Noise Reduction Higher-order singular values \\(\\sigma_{k+1}, \\dots, \\sigma_r\\) are often associated with noise rather than meaningful structure. Truncating these components leads to a denoised version of the data. Compression and Storage Efficiency Instead of storing \\(n \\times p\\) raw data values, we only need to store the top \\(k\\) singular values and vectors, which is much more memory-efficient. Feature Extraction The principal components (columns of \\(\\mathbf{V}_k\\)) define a new basis for the data that is more informative and often interpretable. These features can be used for classification, clustering, and visualization. 5.1.2 The Singular Value Solution is the Best Low-Rank Approximation (Eckart–Young–Mirsky Theorem) We want to prove that the truncated Singular Value Decomposition (SVD) provides the best rank-\\(k\\) approximation to a matrix \\(\\mathbf{X}\\) in the Frobenius norm and spectral norm. This result is known as the Eckart–Young–Mirsky theorem. 5.1.2.1 Problem Statement (Eckart–Young–Mirsky Theorem) Let \\(\\mathbf{X}\\) be an \\(n \\times p\\) matrix with full SVD: \\[ \\mathbf{X} = \\sum_{i=1}^{r} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i&#39; \\] where: - \\(\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_r &gt; 0\\) are the singular values (ordered decreasingly), - \\(\\mathbf{u}_i\\) are the left singular vectors (columns of \\(\\mathbf{U}\\), an \\(n \\times n\\) orthonormal matrix), - \\(\\mathbf{v}_i\\) are the right singular vectors (columns of \\(\\mathbf{V}\\), a \\(p \\times p\\) orthonormal matrix), - \\(r = \\text{rank}(\\mathbf{X})\\). We seek a rank-\\(k\\) matrix \\(\\mathbf{Y}\\) that best approximates \\(\\mathbf{X}\\) by minimizing the Frobenius norm error: \\[ \\min_{\\text{rank}(\\mathbf{Y}) = k} \\|\\mathbf{X} - \\mathbf{Y}\\|_F. \\] The theorem states that the best rank-\\(k\\) approximation is given by the truncated SVD: \\[ \\mathbf{X}_k = \\sum_{i=1}^{k} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i&#39;. \\] We now prove this claim. 5.1.2.2 Proof (Eckart–Young–Mirsky Theorem) We will show this in four steps. Step 1: The Frobenius norm of \\(\\mathbf{X}\\) is: \\[ \\|\\mathbf{X}\\|_F^2 = \\sum_{i=1}^{r} \\sigma_i^2. \\] Step 2: Any rank-\\(k\\) approximation \\(\\mathbf{Y}\\) can be written in terms of some linear combination of singular vectors: \\[ \\mathbf{Y} = \\sum_{i=1}^{k} a_i \\mathbf{u}_i \\mathbf{v}_i&#39;. \\] Step 3: The residual error \\[ \\|\\mathbf{X} - \\mathbf{Y}\\|_F^2 = \\sum_{i=1}^{k} (\\sigma_i - a_i)^2 + \\sum_{i=k+1}^{r} \\sigma_i^2 \\] Step 4: The optimal choice is \\(a_i = \\sigma_i\\) for the first \\(k\\) terms and \\(a_i = 0\\) for the rest. For Step 1, we want to show that the Frobenius norm of a matrix \\(\\mathbf{X}\\) is equal to the sum of the squared singular values: \\[ \\|\\mathbf{X}\\|_F^2 = \\sum_{i=1}^{r} \\sigma_i^2. \\] The Frobenius norm of an \\(n \\times p\\) matrix \\(\\mathbf{X}\\) is defined as: \\[ \\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^{n} \\sum_{j=1}^{p} x_{ij}^2}. \\] Squaring both sides: \\[ \\|\\mathbf{X}\\|_F^2 = \\sum_{i=1}^{n} \\sum_{j=1}^{p} x_{ij}^2. \\] An alternative way to express the Frobenius norm is using the trace function: \\[ \\|\\mathbf{X}\\|_F^2 = \\text{Tr}(\\mathbf{X}&#39; \\mathbf{X}). \\] where \\(\\text{Tr}(\\mathbf{A})\\) denotes the trace (sum of the diagonal elements) of a square matrix \\(\\mathbf{A}\\). To see this note that \\[ \\|\\mathbf{X}\\|_F^2 = \\sum_{i=1}^{n} \\sum_{j=1}^{p} x_{ij}^2 = \\sum_{j=1}^{p} \\sum_{i=1}^{n} x_{ij}^2 = \\sum_{j=1}^{p} {\\boldsymbol x} _j&#39; {\\boldsymbol x} _j = \\text{Tr}(\\mathbf{X}&#39; \\mathbf{X}). \\] From the SVD decomposition, we write: \\[ \\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}&#39; \\] where: - \\(\\mathbf{U}\\) is an \\(n \\times n\\) orthonormal matrix of left singular vectors, - \\(\\mathbf{V}\\) is a \\(p \\times p\\) orthonormal matrix of right singular vectors, - \\(\\mathbf{\\Sigma}\\) is an \\(n \\times p\\) diagonal matrix of singular values: \\[ \\mathbf{\\Sigma} = \\begin{bmatrix} \\sigma_1 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma_2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_3 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; \\sigma_r \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\end{bmatrix}. \\] The rank of \\(\\mathbf{X}\\) is \\(r\\), meaning it has \\(r\\) nonzero singular values. Now, compute \\(\\mathbf{X}&#39; \\mathbf{X}\\): \\[ \\mathbf{X}&#39; \\mathbf{X} = (\\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}&#39;)&#39; (\\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}&#39;). \\] Using the transpose property \\((\\mathbf{A} \\mathbf{B})&#39; = \\mathbf{B}&#39; \\mathbf{A}&#39;\\): \\[ \\mathbf{X}&#39; \\mathbf{X} = \\mathbf{V} \\mathbf{\\Sigma}&#39; \\mathbf{U}&#39; \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}&#39;. \\] Since \\(\\mathbf{U}\\) is orthonormal (\\(\\mathbf{U}&#39; \\mathbf{U} = \\mathbf{I}\\)), this simplifies to: \\[ \\mathbf{X}&#39; \\mathbf{X} = \\mathbf{V} \\mathbf{\\Sigma}&#39; \\mathbf{\\Sigma} \\mathbf{V}&#39;. \\] Since \\(\\mathbf{\\Sigma}&#39; \\mathbf{\\Sigma}\\) is a diagonal matrix with squared singular values \\(\\sigma_i^2\\), we get: \\[ \\mathbf{X}&#39; \\mathbf{X} = \\mathbf{V} \\begin{bmatrix} \\sigma_1^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma_2^2 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 0 &amp; \\ddots &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma_r^2 \\end{bmatrix} \\mathbf{V}&#39;. \\] Taking the trace on both sides: \\[ \\text{Tr}(\\mathbf{X}&#39; \\mathbf{X}) = \\text{Tr}(\\mathbf{V} \\mathbf{\\Sigma}&#39; \\mathbf{\\Sigma} \\mathbf{V}&#39;). \\] Since the trace is invariant under cyclic permutations: \\[ \\text{Tr}(\\mathbf{X}&#39; \\mathbf{X}) = \\text{Tr}( \\mathbf{V}&#39; \\mathbf{V} \\mathbf{\\Sigma}&#39; \\mathbf{\\Sigma}) = \\text{Tr}(\\mathbf{\\Sigma}&#39; \\mathbf{\\Sigma}). \\] Since \\(\\mathbf{\\Sigma}&#39; \\mathbf{\\Sigma}\\) is diagonal, its trace is simply the sum of the diagonal elements: \\[ \\text{Tr}(\\mathbf{\\Sigma}&#39; \\mathbf{\\Sigma}) = \\sum_{i=1}^{r} \\sigma_i^2. \\] Since we previously established that: \\[ \\|\\mathbf{X}\\|_F^2 = \\text{Tr}(\\mathbf{X}&#39; \\mathbf{X}), \\] we conclude: \\[ \\|\\mathbf{X}\\|_F^2 = \\sum_{i=1}^{r} \\sigma_i^2. \\] For Step 2, we claim that any rank-\\(k\\) matrix \\(\\mathbf{Y}\\) can be expressed as: \\[ \\mathbf{Y} = \\sum_{i=1}^{k} a_i \\mathbf{u}_i \\mathbf{v}_i&#39;. \\] This follows from the fact that the outer products of the singular vectors, \\[ \\mathbf{u}_i \\mathbf{v}_i&#39; \\] form an orthonormal basis for matrices in \\(\\mathbb{R}^{n \\times p}\\), meaning that any rank-\\(k\\) matrix can be expressed as a linear combination of at most \\(k\\) of these elements. To see this, consider the inner product between two singular vector outer products, defined as: \\[ \\langle \\mathbf{u}_i \\mathbf{v}_i&#39;, \\mathbf{u}_j \\mathbf{v}_j&#39; \\rangle_F. \\] By the Frobenius inner product, the inner product between two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) is: \\[ \\langle \\mathbf{A}, \\mathbf{B} \\rangle_F = \\text{Tr}(\\mathbf{A}&#39; \\mathbf{B}). \\] Applying this to the singular vectors: \\[ \\langle \\mathbf{u}_i \\mathbf{v}_i&#39;, \\mathbf{u}_j \\mathbf{v}_j&#39; \\rangle_F = \\text{Tr} \\big( (\\mathbf{u}_i \\mathbf{v}_i&#39;)&#39; (\\mathbf{u}_j \\mathbf{v}_j&#39;) \\big). \\] Using the transpose property \\((\\mathbf{A} \\mathbf{B})&#39; = \\mathbf{B}&#39; \\mathbf{A}&#39;\\), we get: \\[ \\langle \\mathbf{u}_i \\mathbf{v}_i&#39;, \\mathbf{u}_j \\mathbf{v}_j&#39; \\rangle_F = \\text{Tr} \\big( \\mathbf{v}_i \\mathbf{u}_i&#39; \\mathbf{u}_j \\mathbf{v}_j&#39; \\big). \\] Since \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are orthonormal, we use: \\[ \\mathbf{u}_i&#39; \\mathbf{u}_j = \\delta_{ij}, \\quad \\mathbf{v}_i&#39; \\mathbf{v}_j = \\delta_{ij}. \\] where \\(\\delta_{ij} = 1\\), if \\(i=j\\) and \\(\\delta_{ij} = 0\\), if \\(i \\neq j\\). Thus, \\[ \\langle \\mathbf{u}_i \\mathbf{v}_i&#39;, \\mathbf{u}_j \\mathbf{v}_j&#39; \\rangle_F = \\delta_{ij}. \\] This proves that \\(\\{ \\mathbf{u}_i \\mathbf{v}_i&#39; \\}_{i=1}^{k}\\) are orthonormal under the Frobenius inner product, therefore linearly independent and forming an spanning bases for a rank \\(k\\) matrix. For Step 3, mote that the error matrix is: \\[ \\mathbf{X} - \\mathbf{Y} = \\sum_{i=1}^{r} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i&#39; - \\sum_{i=1}^{k} a_i \\mathbf{u}_i \\mathbf{v}_i&#39;. \\] Rewriting this sum: \\[ \\mathbf{X} - \\mathbf{Y} = \\sum_{i=1}^{k} (\\sigma_i - a_i) \\mathbf{u}_i \\mathbf{v}_i&#39; + \\sum_{i=k+1}^{r} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i&#39;. \\] Since the singular vector outer products \\(\\mathbf{u}_i \\mathbf{v}_i&#39;\\) form an orthonormal basis under the Frobenius norm, the squared Frobenius norm of a sum of such terms is the sum of the squared coefficients: \\[ \\|\\mathbf{X} - \\mathbf{Y}\\|_F^2 = \\sum_{i=1}^{k} (\\sigma_i - a_i)^2 + \\sum_{i=k+1}^{r} \\sigma_i^2. \\] Finally, in Step 4, we need to minimize the approximation error: \\[ \\sum_{i=1}^{k} (\\sigma_i - a_i)^2 + \\sum_{i=k+1}^{r} \\sigma_i^2, \\] the best choice is clearly: \\[ a_i = \\sigma_i, \\quad \\text{for } i = 1, \\dots, k. \\] Thus, the best rank-\\(k\\) approximation is: \\[ \\mathbf{X}_k = \\sum_{i=1}^{k} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i&#39;. \\] This minimizes the error: \\[ \\|\\mathbf{X} - \\mathbf{X}_k\\|_F^2 = \\sum_{i=k+1}^{r} \\sigma_i^2. \\] which confirms the Eckart–Young–Mirsky theorem. 5.1.2.3 Uniqueness of the Solution The Eckart–Young–Mirsky theorem guarantees that the best rank-\\(k\\) approximation of a matrix \\(\\mathbf{X}\\) in terms of the Frobenius norm is given by the truncated SVD. However, the uniqueness of this solution depends on the singular values. 5.1.2.3.1 Key Condition for Uniqueness The best rank-\\(k\\) approximation is unique if and only if the \\(k\\)-th singular value \\(\\sigma_k\\) is strictly greater than the \\((k+1)\\)-th singular value \\(\\sigma_{k+1}\\). In other words: \\[ \\sigma_k &gt; \\sigma_{k+1} \\implies \\text{Unique solution.} \\] 5.1.2.3.2 When is the Solution Not Unique? If there is a tie or degeneracy in the singular values: \\[ \\sigma_k = \\sigma_{k+1}, \\] then the best rank-\\(k\\) approximation is not unique. Multiple linear combinations of the corresponding singular vectors can yield the same Frobenius norm error, leading to multiple optimal solutions. 5.1.2.3.3 Intuition Unique solution: Singular values decrease strictly, and each singular vector contributes distinctly to the approximation. Non-unique solution: Equal singular values create a subspace of possible solutions, as any linear combination of singular vectors associated with the repeated singular value results in the same approximation quality. 5.1.2.3.4 Example If \\(\\mathbf{X}\\) has singular values \\(\\sigma_1 = 5, \\sigma_2 = 4, \\sigma_3 = 4, \\sigma_4 = 2\\), then: - The best rank-1 approximation is unique because \\(\\sigma_1 &gt; \\sigma_2\\). - The best rank-2 approximation is not unique because \\(\\sigma_2 = \\sigma_3\\). - The best rank-3 approximation is unique because \\(\\sigma_3 &gt; \\sigma_4\\). 5.1.2.3.5 Summary Unique solution: \\(\\sigma_k &gt; \\sigma_{k+1}\\). Not unique solution: \\(\\sigma_k = \\sigma_{k+1}\\). Would you like me to show an example or provide more details?  5.1.3 Eckart–Young–Mirsky Theorem for the Spectral Norm The Eckart–Young–Mirsky theorem is a fundamental result in matrix approximation, stating that the best rank-k approximation of a matrix in terms of the spectral norm is obtained through its singular value decomposition (SVD). However, unlike in the Frobenius norm case, the solution is not always unique in the spectral norm. 5.1.3.1 Spectral Norm Theorem Statement For any matrix \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\) with singular values \\(\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_r &gt; 0\\), the rank-k matrix \\(\\mathbf{X}_k\\) that minimizes the spectral norm error \\(\\|\\mathbf{X} - \\mathbf{Y}\\|_2\\) is given by: \\[ \\mathbf{X}_k = \\sum_{i=1}^k \\sigma_i \\mathbf{u}_i \\mathbf{v}_i&#39;, \\] where \\(\\mathbf{u}_i\\) and \\(\\mathbf{v}_i\\) are the left and right singular vectors corresponding to \\(\\sigma_i\\). The minimal spectral norm error is: \\[ \\|\\mathbf{X} - \\mathbf{X}_k\\|_2 = \\sigma_{k+1}. \\] 5.1.3.2 Spectral Norm Proof Sketch The spectral norm of a matrix is its largest singular value. Given any rank-k matrix \\(\\mathbf{Y}\\) written as before \\(\\mathbf{Y} = \\sum_{i=1}^{k} a_i \\mathbf{u}_i \\mathbf{v}_i&#39;\\), its singular values are \\(a_1, a_2, \\dots, a_k\\). The error matrix \\(\\mathbf{X} - \\mathbf{Y}\\) has singular values: \\[ \\{ |\\sigma_1 - a_1|, |\\sigma_2 - a_2|, \\dots, |\\sigma_k - a_k|, \\sigma_{k+1}, \\dots, \\sigma_r \\}. \\] To minimize the spectral norm of the error, one must ensure that: \\[ \\max \\{ |\\sigma_1 - a_1|, |\\sigma_2 - a_2|, \\dots, \\sigma_{k+1} \\} \\] is as small as possible. A way to achieve the minimal error \\(\\sigma_{k+1}\\) is to set \\(a_i = \\sigma_i\\) for \\(i=1, \\dots, k\\), as with the spectral norm, but we can construct different rank-k matrices that also achieve this value. 5.1.3.3 Non-Uniqueness for the Spectral Norm Unlike in the Frobenius norm case, where the truncated SVD solution is unique, the spectral norm allows for multiple solutions. If we require: \\[ |\\sigma_i - a_i| \\leq \\sigma_{k+1}, \\quad \\forall i=1, \\dots, k, \\] any such choice of \\(a_i\\) will yield the same spectral norm error \\(\\sigma_{k+1}\\). This provides a class of rank-k approximations that achieve the minimal spectral norm error. 5.1.3.3.1 How to Find Alternative Solutions To find alternative rank-k approximations with the same spectral norm error, one can: Perturb Singular Values: Adjust \\(a_i\\) such that \\(|\\sigma_i - a_i| \\leq \\sigma_{k+1}\\), ensuring no singular value difference exceeds \\(\\sigma_{k+1}\\). Linear Combinations of Singular Vectors: Combine singular vectors corresponding to equal singular values to create different approximations. 5.1.3.4 Conclusion for the Spectral Norm The Eckart–Young–Mirsky theorem guarantees the minimal spectral norm error with the truncated SVD solution. However, in cases of spectral norm approximation, the solution is not unique. By carefully choosing singular values and vectors within the specified bounds, one can construct alternative solutions that achieve the same minimal error. This flexibility in spectral norm approximation highlights the subtle difference between approximations in spectral and Frobenius norms, making it an important consideration in applications like low-rank matrix approximation, data compression, and principal component analysis. 5.2 Variance Maximization in PCA Principal Component Analysis (PCA) is traditionally introduced as a method that finds directions (principal components) along which the variance of the data is maximized. Given a data matrix \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\) with centered columns, PCA seeks an orthonormal set of vectors \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_p\\}\\) such that: The first principal component \\(\\mathbf{v}_1\\) maximizes the variance: \\[ \\mathbf{v}_1 = \\arg \\max_{\\|\\mathbf{v}\\|=1} \\text{Var}(\\mathbf{X}\\mathbf{v}) = \\arg \\max_{\\|\\mathbf{v}\\|=1} \\|\\mathbf{X}\\mathbf{v}\\|_2^2. \\] Subsequent principal components are chosen orthogonal to the previous ones and also maximize the variance. Each principal component corresponds to a singular vector from the Singular Value Decomposition (SVD) of \\(\\mathbf{X}\\): \\[ \\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}&#39;, \\] where \\(\\mathbf{\\Sigma}\\) contains the singular values \\(\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r &gt; 0\\). The first \\(k\\) principal components are the first \\(k\\) columns of \\(\\mathbf{V}\\): \\[ \\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_k, \\] with the corresponding principal component scores given by: \\[ \\mathbf{X} \\mathbf{v}_i = \\sigma_i \\mathbf{u}_i, \\quad i = 1, 2, \\dots, k. \\] 5.2.1 First Principal Cmponent We aim to find the direction \\(\\mathbf{v}_1 \\in \\mathbb{R}^p\\) (a unit vector, \\(\\| \\mathbf{v}_1 \\| = 1\\)) that maximizes the variance of the data when projected onto \\(\\mathbf{v}_1\\). The dataset is represented by \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\) (with centered columns), and the projection of \\(\\mathbf{X}\\) onto \\(\\mathbf{v}_1\\) is given by \\(\\mathbf{Xv}_1\\). 5.2.1.1 Variance of the Projection The variance of the projected data \\(\\mathbf{Xv}_1\\) is: \\[ \\text{Var}(\\mathbf{Xv}_1) = \\frac{1}{n} \\| \\mathbf{Xv}_1 \\|_2^2 = \\frac{1}{n} (\\mathbf{Xv}_1)&#39;(\\mathbf{Xv}_1). \\] Expanding this: \\[ \\text{Var}(\\mathbf{Xv}_1) = \\frac{1}{n} \\mathbf{v}_1&#39; \\mathbf{X}&#39; \\mathbf{X} \\mathbf{v}_1. \\] Since \\(\\frac{1}{n} \\mathbf{X}&#39; \\mathbf{X}\\) is the sample covariance matrix \\(\\mathbf{S}\\), we rewrite the objective as: \\[ \\text{maximize } \\mathbf{v}_1&#39; \\mathbf{S} \\mathbf{v}_1 \\quad \\text{subject to} \\quad \\| \\mathbf{v}_1 \\| = 1. \\] 5.2.1.2 Optimization Problem (Rayleigh Quotient) This is a constrained optimization problem that can be solved using the method of Lagrange multipliers: \\[ \\mathcal{L}(\\mathbf{v}_1, \\lambda) = \\mathbf{v}_1&#39; \\mathbf{S} \\mathbf{v}_1 - \\lambda (\\mathbf{v}_1&#39; \\mathbf{v}_1 - 1). \\] Taking the derivative with respect to \\(\\mathbf{v}_1\\) and setting it to zero gives: \\[ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{v}_1} = 2 \\mathbf{S} \\mathbf{v}_1 - 2 \\lambda \\mathbf{v}_1 = 0. \\] Thus: \\[ \\mathbf{S} \\mathbf{v}_1 = \\lambda \\mathbf{v}_1. \\] 5.2.1.3 Eigenvalue Problem This is an eigenvalue problem where \\(\\lambda\\) is an eigenvalue of \\(\\mathbf{S}\\) and \\(\\mathbf{v}_1\\) is the corresponding eigenvector. The direction \\(\\mathbf{v}_1\\) that maximizes the variance is the eigenvector corresponding to the largest eigenvalue \\(\\lambda_1\\). Hence, the first principal component \\(\\mathbf{v}_1\\) is the eigenvector of \\(\\mathbf{S}\\) associated with the largest eigenvalue \\(\\lambda_1\\), and the maximum variance is \\(\\lambda_1\\). 5.2.1.4 Connection to SVD Recall that the SVD of \\(\\mathbf{X}\\) is: \\[ \\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}&#39;. \\] The sample covariance matrix \\(\\mathbf{S} = \\frac{1}{n} \\mathbf{X}&#39; \\mathbf{X}\\) can be written as: \\[ \\mathbf{S} = \\frac{1}{n} \\mathbf{V} \\mathbf{\\Sigma}&#39; \\mathbf{U}&#39; \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}&#39; = \\frac{1}{n} \\mathbf{V} \\mathbf{\\Sigma}^2 \\mathbf{V}&#39;. \\] The eigenvectors of \\(\\mathbf{S}\\) are the columns of \\(\\mathbf{V}\\), and the eigenvalues are the squared singular values \\(\\sigma_i^2\\) divided by \\(n\\). Thus: - The first principal component direction \\(\\mathbf{v}_1\\) is the first column of \\(\\mathbf{V}\\) from the SVD. - The variance explained by the first principal component is \\(\\frac{\\sigma_1^2}{n}\\). 5.2.2 Second Principal Component The second principal component \\(\\mathbf{v}_2\\) is the direction that: - Maximizes the variance of the projected data \\(\\mathbf{Xv}_2\\), - Subject to being orthogonal to the first principal component \\(\\mathbf{v}_1\\). 5.2.2.1 Optimization Problem We want to maximize: \\[ \\text{Var}(\\mathbf{Xv}_2) = \\mathbf{v}_2&#39; \\mathbf{S} \\mathbf{v}_2 \\] subject to: \\[ \\|\\mathbf{v}_2\\| = 1 \\quad \\text{and} \\quad \\mathbf{v}_2&#39; \\mathbf{v}_1 = 0. \\] 5.2.2.2 Solution Using Lagrange Multipliers The constraint \\(\\mathbf{v}_2&#39; \\mathbf{v}_1 = 0\\) ensures orthogonality to the first principal component. Solving this optimization problem leads to: \\[ \\mathbf{S} \\mathbf{v}_2 = \\lambda_2 \\mathbf{v}_2 \\] where \\(\\mathbf{v}_2\\) is the eigenvector associated with the second largest eigenvalue \\(\\lambda_2\\) of \\(\\mathbf{S}\\). 5.2.2.3 Intuition Since the first principal component \\(\\mathbf{v}_1\\) has already captured the maximum variance, the second principal component \\(\\mathbf{v}_2\\) captures the next largest amount of variance while being orthogonal to \\(\\mathbf{v}_1\\). We can continue in this way to find the next Principal Components. 5.3 PARAFAC decomposition The PARAFAC decomposition (also known as CANDECOMP/PARAFAC or CP decomposition) is a natural extension of the concept of Principal Component Analysis (PCA) from matrices to higher-order tensors. 5.3.1 What is a Tensor? A tensor is a multi-dimensional array, extending the concept of vectors (1D arrays) and matrices (2D arrays) to higher dimensions: - A vector is a tensor of order 1. - A matrix is a tensor of order 2. - A tensor of order \\(N\\) has \\(N\\) dimensions (also called modes). For example: - An image is a matrix (2D tensor). - A color video can be represented as a 3D tensor (height × width × time). 5.3.2 Motivation: Why Generalize PCA to Tensors? PCA works well with matrices, but many real-world data are naturally represented as tensors: - Psychology: Data from multiple individuals across multiple tests and times. - Signal Processing: Multi-channel EEG data recorded over time. - Chemometrics: Spectral data collected over different conditions. PCA provides a low-rank approximation for matrices by decomposing them into principal components. The PARAFAC decomposition extends this idea to tensors, finding components along each dimension simultaneously. 5.3.3 PARAFAC Decomposition Definition Given a tensor \\(\\mathcal{X} \\in \\mathbb{R}^{I_1 \\times I_2 \\times \\cdots \\times I_N}\\), the PARAFAC decomposition expresses \\(\\mathcal{X}\\) as a sum of \\(R\\) rank-1 tensors: \\[ \\mathcal{X} \\approx \\sum_{r=1}^{R} \\mathbf{a}_r^{(1)} \\circ \\mathbf{a}_r^{(2)} \\circ \\cdots \\circ \\mathbf{a}_r^{(N)}, \\] where: - \\(R\\) is the rank of the decomposition. - \\(\\mathbf{a}_r^{(n)} \\in \\mathbb{R}^{I_n}\\) are vectors along the \\(n\\)-th mode. - \\(\\circ\\) denotes the outer product. - Each rank-1 tensor \\(\\mathbf{a}_r^{(1)} \\circ \\mathbf{a}_r^{(2)} \\circ \\cdots \\circ \\mathbf{a}_r^{(N)}\\) is analogous to a principal component in PCA. 5.3.4 How is PARAFAC Related to PCA? PCA decomposes a matrix \\(\\mathbf{X}\\) into a sum of rank-1 matrices: \\[ \\mathbf{X} \\approx \\sum_{r=1}^{R} \\sigma_r \\mathbf{u}_r \\mathbf{v}_r&#39;, \\] where \\(\\mathbf{u}_r\\) and \\(\\mathbf{v}_r\\) are left and right singular vectors, and \\(\\sigma_r\\) are singular values. PARAFAC generalizes this concept by decomposing a tensor \\(\\mathcal{X}\\) into a sum of rank-1 tensors. Just like PCA finds directions that capture the most variance in a matrix, PARAFAC finds multi-dimensional factors that capture the most structure in a tensor. 5.3.5 Key Properties of PARAFAC Decomposition Uniqueness: Unlike PCA (where different rotations of principal components can yield the same solution), the PARAFAC decomposition is unique under mild conditions. This uniqueness makes PARAFAC particularly useful for interpretability. Low-rank Approximation: PARAFAC provides a low-rank approximation of tensors, analogous to PCA for matrices. Interpretability: Each component can be interpreted as a factor along each mode, making it valuable in fields like chemometrics and neuroscience. 5.4 High-Dimensional PCA When the number of features or variables is \\(p\\), computing the eigenvalue decomposition or SVD can be prohebitely expensive. We can have some alternatives depending on the 5.4.1 High-Dimensional PCA (\\(p \\gg n\\)) When the number of features \\(p\\) is much larger than the number of samples \\(n\\) (a common scenario in high-dimensional data analysis), directly computing the PCA decomposition from the \\(p \\times p\\) covariance matrix \\(\\mathbf{X}&#39; \\mathbf{X}\\) is computationally expensive. To address this, we can use the matrix \\(\\mathbf{X} \\mathbf{X}&#39;\\) instead, which is an \\(n \\times n\\) matrix, making the computation much more efficient. 5.4.1.1 Key Idea Instead of directly solving the eigenvalue problem: \\[ \\mathbf{X}&#39; \\mathbf{X} \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i, \\] we solve the equivalent problem: \\[ \\mathbf{X} \\mathbf{X}&#39; \\mathbf{u}_i = \\lambda_i \\mathbf{u}_i, \\] where \\(\\mathbf{u}_i\\) are the left singular vectors of \\(\\mathbf{X}\\). 5.4.1.2 Steps for Efficient Computation Form the \\(n \\times n\\) matrix \\(\\mathbf{X} \\mathbf{X}&#39;\\) instead of \\(\\mathbf{X}&#39; \\mathbf{X}\\). Compute the eigenvalues and eigenvectors of \\(\\mathbf{X} \\mathbf{X}&#39;\\): \\(\\mathbf{X} \\mathbf{X}&#39; \\mathbf{u}_i = \\sigma_i^2 \\mathbf{u}_i.\\) Obtain singular values \\(\\sigma_i\\) as the square roots of the eigenvalues. Compute the principal directions \\(\\mathbf{v}_i\\) from the \\(\\mathbf{u}_i\\) vectors: \\[ \\mathbf{v}_i = \\frac{\\mathbf{X}&#39; \\mathbf{u}_i}{\\sigma_i}. \\] Compute the PCA scores directly as: \\[ \\mathbf{X} \\mathbf{v}_i = \\sigma_i \\mathbf{u}_i. \\] 5.4.1.3 Why Does This Work? The matrix \\(\\mathbf{X} \\mathbf{X}&#39;\\) is much smaller when \\(n \\ll p\\), making eigen-decomposition computationally cheaper. Once we have the left singular vectors \\(\\mathbf{u}_i\\) from \\(\\mathbf{X} \\mathbf{X}&#39;\\), we can easily find the right singular vectors \\(\\mathbf{v}_i\\) without ever forming the large \\(p \\times p\\) matrix. This approach is crucial for handling high-dimensional data efficiently, ensuring that PCA remains computationally feasible even when \\(p \\gg n\\). Here’s an explanation with added comments to your provided R Markdown code, keeping the code unchanged: 5.4.1.4 Efficient Computation PCA \\(p \\gg n\\) 5.4.1.4.1 PCA Methods # PCA using X&#39;X (traditional method) pca_traditional &lt;- function(X) { n &lt;- nrow(X) S &lt;- crossprod(X) # X&#39;X eig &lt;- eigen(S, symmetric = TRUE) # Eigen decomposition priCom &lt;- eig$vectors # PC varExp &lt;- eig$values / sum(eig$values) # Explain Variance by PC pcaSco &lt;- X %*% eig$vectors # PCA scores return(list(priCom = priCom, pcaSco = pcaSco, varExp = varExp)) } # PCA using X X&#39; (efficient method) pca_efficient &lt;- function(X) { S &lt;- tcrossprod(X) # X X&#39; eig &lt;- eigen(S, symmetric = TRUE) # Eigen decomposition singular_values &lt;- sqrt(eig$values) priCom &lt;- t(t(t(X) %*% eig$vectors) / singular_values) # Compute V = X&#39;U / Sigma pcaSco &lt;- t(t(eig$vectors) * singular_values) # PCA scores varExp &lt;- eig$values / sum(eig$values) # Explain Variance by PC return(list(priCom = priCom, pcaSco = pcaSco, varExp = varExp)) } Explanation: The traditional method computes PCA from \\(X&#39;X\\), which is large when \\(p \\gg n\\). The efficient method computes PCA from \\(X X&#39;\\), a smaller matrix when \\(p \\gg n\\), then derives the principal components and scores from that decomposition. 5.4.1.4.2 Simulate a Data Set # Simulate high-dimensional data set.seed(123) n &lt;- 100 # Number of samples p &lt;- 1000 # Number of features # Design Matrix X &lt;- matrix(data = NA, nrow = n, ncol = 0) # Block-Correlation Matrix in Blocks sizBlo &lt;- 10 disCor &lt;- 0.5 C &lt;- exp(- disCor * abs(rep(1, sizBlo) %*% t(1:sizBlo) - (1:sizBlo) %*% t(rep(1, sizBlo)))) # Simulates the Design Matrix for(i in 1:(p / sizBlo)){ X &lt;- cbind(X, mvtnorm::rmvnorm(n = n, sigma = C)) } Explanation: Simulates a dataset with \\(n = 100\\) samples and \\(p = 1000\\) features. Correlation is introduced within blocks of features using an exponential decay structure with \\(\\rho = 0.5\\). 5.4.1.4.3 Compare Results # Performs PCA by Both Methods outTra &lt;- pca_traditional(X = X) outEff &lt;- pca_efficient(X = X) # Compare First Two Principal Components cbind(outTra$priCom[1:10, 1:2], outEff$priCom[1:10, 1:2]) ## [,1] [,2] [,3] [,4] ## [1,] -0.001202379 0.004997560 0.001202379 0.004997560 ## [2,] 0.014206244 0.032971374 -0.014206244 0.032971374 ## [3,] 0.017112378 0.030209540 -0.017112378 0.030209540 ## [4,] 0.008336267 0.029518877 -0.008336267 0.029518877 ## [5,] 0.007510180 0.041815584 -0.007510180 0.041815584 ## [6,] -0.013239150 -0.009654624 0.013239150 -0.009654624 ## [7,] 0.008449956 0.042200920 -0.008449956 0.042200920 ## [8,] 0.028501647 0.026011567 -0.028501647 0.026011567 ## [9,] 0.008413915 0.004696628 -0.008413915 0.004696628 ## [10,] 0.018911051 -0.012567287 -0.018911051 -0.012567287 # Compares the First Two Principal Component Scores cbind(outTra$pcaSco[1:10, 1:2], outEff$pcaSco[1:10, 1:2]) ## [,1] [,2] [,3] [,4] ## [1,] -6.5413193 -4.0702069 6.5413193 -4.0702069 ## [2,] -8.5679536 0.3470996 8.5679536 0.3470996 ## [3,] -2.7203524 2.0908395 2.7203524 2.0908395 ## [4,] 2.7429942 3.8261368 -2.7429942 3.8261368 ## [5,] -0.1077707 -6.3139474 0.1077707 -6.3139474 ## [6,] 0.2253885 0.2471944 -0.2253885 0.2471944 ## [7,] -1.3734419 -4.6083631 1.3734419 -4.6083631 ## [8,] 3.2775010 -8.2332778 -3.2775010 -8.2332778 ## [9,] -3.3366402 1.1154150 3.3366402 1.1154150 ## [10,] 8.1907932 -0.2565279 -8.1907932 -0.2565279 # Plots the Variance Explained par(mar = c(2,2,0,0)) par(mfrow = c(2, 1)) plot(outTra$varExp[1:50], xaxt = &quot;n&quot;) par(mar = c(2,2,0,0)) plot(outTra$varExp[1:50]) Explanation: Compares the results from both PCA methods to ensure they are identical. Displays the first two principal components and scores side by side. Plots the variance explained by the top 50 components for both methods. 5.4.1.4.4 Computational Efficiency Comparison # Load necessary library library(microbenchmark) # Benchmark both methods benchmark_results &lt;- microbenchmark( Traditional = pca_traditional(X), Efficient = pca_efficient(X), times = 5 ) # Print the results print(benchmark_results) ## Unit: milliseconds ## expr min lq mean median uq max neval ## Traditional 475.6833 480.4067 482.95812 485.1269 485.9995 487.5742 5 ## Efficient 4.7050 4.7711 6.06798 5.0668 5.3364 10.4606 5 Explanation: Uses microbenchmark to measure and compare the runtime of both PCA methods. Demonstrates the computational advantage of the efficient method when \\(p \\gg n\\). 5.4.1.4.5 Larger Example # Simulate high-dimensional data set.seed(123) n &lt;- 100 # Number of samples p &lt;- 2000 # Number of features # Design Matrix X &lt;- matrix(data = NA, nrow = n, ncol = 0) # Block-Correlation Matrix in Blocks sizBlo &lt;- 10 disCor &lt;- 0.5 C &lt;- exp(- disCor * abs(rep(1, sizBlo) %*% t(1:sizBlo) - (1:sizBlo) %*% t(rep(1, sizBlo)))) # Simulates the Design Matrix for(i in 1:(p / sizBlo)){ X &lt;- cbind(X, mvtnorm::rmvnorm(n = n, sigma = C)) } # Benchmark both methods benchmark_results &lt;- microbenchmark( Traditional = pca_traditional(X), Efficient = pca_efficient(X), times = 5 ) # Print the results print(benchmark_results) ## Unit: milliseconds ## expr min lq mean median uq max neval ## Traditional 3892.6445 3925.5093 4008.5234 3992.7742 4035.0165 4196.6727 5 ## Efficient 9.0702 9.1546 9.5051 9.3315 9.9286 10.0406 5 Explanation: Repeats the PCA and timing comparison on a larger dataset with \\(p = 2000\\) features to demonstrate the increasing efficiency of the \\(X X&#39;\\)-based method as \\(p\\) grows. 5.4.2 Randomized PCA The use of random matrices in the computation of PCA has gained significant attention due to its potential to make high-dimensional computations more efficient. This is particularly important when dealing with large datasets where traditional PCA methods become computationally expensive. 5.4.2.1 Random matrices in PCA When computing PCA, especially in high-dimensional settings, the major computational bottleneck is often the singular value decomposition (SVD) of large matrices. Random matrix methods offer an efficient approximation by projecting the original high-dimensional data into a lower-dimensional subspace, while preserving most of its structure. 5.4.2.2 Randomized PCA (RPCA) Randomized PCA leverages random matrices to approximate the principal components. The key steps are: Random Projection: Instead of working directly with the full data matrix \\(\\mathbf{X}\\), we multiply it by a random matrix \\(\\mathbf{\\Omega}\\) to form a lower-dimensional matrix \\(\\mathbf{Y}\\): \\[ \\mathbf{Y} = \\mathbf{X} \\mathbf{\\Omega} \\] Here, \\(\\mathbf{\\Omega}\\) is typically a Gaussian random matrix, where each entry is drawn from a normal distribution \\(N(0, 1)\\). Compute PCA in Lower Dimension: Perform PCA (or SVD) on \\(\\mathbf{Y}\\) instead of \\(\\mathbf{X}\\), which is much cheaper computationally. Since \\(\\mathbf{Y}\\) has fewer columns, the complexity is greatly reduced. Recover Approximate Components: The principal components of \\(\\mathbf{X}\\) are approximated using the computed components of \\(\\mathbf{Y}\\). 5.4.2.3 Why use random matrices? Efficiency: Reduces the dimensionality before performing SVD, thus significantly speeding up the computation. Scalability: Ideal for large datasets where traditional PCA is computationally intensive. Memory-friendly: Works well in environments with limited memory by avoiding operations on the full data matrix. 5.4.2.4 Theoretical foundations from random matrix theory Random matrix theory provides tools to understand how well the random projections preserve the original structure of the data. Key results such as the Johnson-Lindenstrauss lemma ensure that the distances between points are approximately preserved under random projections. This is crucial because PCA relies on variance and distances within the dataset. 5.4.2.5 When to use random matrix methods in PCA? When the dataset is too large to store or process in full. When approximate principal components are acceptable. In streaming data scenarios where full PCA computations at each step are infeasible. For distributed computing environments where random projections can be easily parallelized. 5.4.2.6 Limitations Approximate results: Although close, they might not match exactly with the traditional PCA results. Hyperparameter tuning: The dimension of the random projection space must be chosen carefully to balance accuracy and computational cost. 5.4.2.7 Conclusion The use of random matrices in PCA allows for: - Faster computations, - Lower memory usage, - Scalable algorithms for large datasets. This technique is particularly powerful in high-dimensional statistics, machine learning, and big data applications. Yes! Using \\(\\mathbf{Y} = \\mathbf{\\Omega} \\mathbf{X}\\) is also a common approach in randomized PCA and can offer computational advantages depending on the dimensions of the data matrix. 5.4.2.7.1 When to use \\(\\mathbf{Y} = \\mathbf{\\Omega} \\mathbf{X}\\) Let \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\) where: - \\(n\\) is the number of samples, - \\(p\\) is the number of features. If \\(n \\gg p\\) (many samples, few features), using: \\[ \\mathbf{Y} = \\mathbf{X} \\mathbf{\\Omega} \\] is typically preferred because it reduces the number of features. However, if \\(p \\gg n\\) (many features, few samples), using: \\[ \\mathbf{Y} = \\mathbf{\\Omega} \\mathbf{X} \\] is advantageous because: - It reduces the number of samples, - The random matrix \\(\\mathbf{\\Omega}\\) is applied to the rows instead of the columns, - This reduces the size of the matrix on which the singular value decomposition (SVD) is performed. 5.4.2.7.2 Advantages of \\(\\mathbf{Y} = \\mathbf{\\Omega} \\mathbf{X}\\) More efficient when \\(p \\gg n\\) because the random projection reduces the dimension from \\(p\\) features to a smaller dimension. Computational savings: Performing SVD on \\(\\mathbf{Y}\\) (which is smaller than \\(\\mathbf{X}\\)) is much faster. Memory efficiency: Storing \\(\\mathbf{Y}\\) requires less memory than \\(\\mathbf{X}\\). 5.4.2.7.3 How it works Random matrix multiplication: \\(\\mathbf{\\Omega} \\in \\mathbb{R}^{r \\times n}\\) is a random Gaussian matrix (or any well-chosen random matrix) with \\(r &lt; n\\). Multiplying \\(\\mathbf{\\Omega}\\) with \\(\\mathbf{X}\\) results in \\(\\mathbf{Y} \\in \\mathbb{R}^{r \\times p}\\). Reduced dimensionality: Now, \\(\\mathbf{Y}\\) has fewer rows, making it more efficient to compute the SVD. Approximate PCA: Perform SVD on \\(\\mathbf{Y}\\) instead of \\(\\mathbf{X}\\), obtaining: \\[ \\mathbf{Y} = \\mathbf{U}_Y \\mathbf{\\Sigma}_Y \\mathbf{V}_Y&#39; \\] The principal components of \\(\\mathbf{X}\\) are then approximated from \\(\\mathbf{U}_Y\\), \\(\\mathbf{\\Sigma}_Y\\), and \\(\\mathbf{V}_Y\\). 5.4.2.7.4 Choosing \\(\\mathbf{Y} = \\mathbf{\\Omega} \\mathbf{X}\\) vs \\(\\mathbf{Y} = \\mathbf{X} \\mathbf{\\Omega}\\) Use \\(\\mathbf{X} \\mathbf{\\Omega}\\) if you want to reduce the feature dimension \\(p\\). Use \\(\\mathbf{\\Omega} \\mathbf{X}\\) if you want to reduce the sample dimension \\(n\\). 5.4.2.7.5 Applications High-dimensional biology data (e.g., genomics with \\(p \\gg n\\)), Text data where the feature space (words) is huge compared to the number of documents, Large image datasets where each image is represented as a high-dimensional vector. 5.5 Principal Components Regression Principal components regression (PCR) is a regression technique that leverages principal component analysis (PCA) to handle high-dimensional data, especially when predictors are highly correlated. 5.5.1 What is Principal Components Regression? PCR combines PCA and linear regression: PCA Step: Reduce the dimensionality of the predictor matrix \\(\\mathbf{X}\\) by transforming it into its principal components. Regression Step: Perform linear regression using the selected principal components as predictors instead of the original variables. 5.5.2 Why Use PCR? Multicollinearity: PCR is particularly useful when predictors are highly correlated, as PCA removes collinearity. Dimensionality Reduction: It reduces the number of predictors by selecting only the most important principal components. Stability: Improves the stability of the regression coefficients by avoiding overfitting. 5.5.3 Mathematical Formulation Given: - \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\) (design matrix), - \\(\\mathbf{y} \\in \\mathbb{R}^n\\) (response vector), 5.5.3.1 Step 1: Perform PCA on \\(\\mathbf{X}\\) The PCA decomposition is: \\[ \\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}&#39; \\] - \\(\\mathbf{V} \\in \\mathbb{R}^{p \\times p}\\) contains the principal directions. - The principal components are given by \\(\\mathbf{Z} = \\mathbf{X} \\mathbf{V}\\). 5.5.3.2 Step 2: Select \\(k\\) Principal Components Choose the top \\(k\\) principal components \\(\\mathbf{Z}_k\\) that explain most of the variance in \\(\\mathbf{X}\\). 5.5.3.3 Step 3: Perform Linear Regression Fit a linear regression model: \\[ \\mathbf{y} = \\mathbf{Z}_k \\mathbf{\\beta}_k + \\mathbf{e}, \\] where \\(\\mathbf{\\beta}_k\\) are the regression coefficients and \\(\\mathbf{e}\\) is the error term. 5.5.3.4 Step 4: Transform Coefficients Back The original regression coefficients \\(\\mathbf{\\hat{\\beta}}\\) are obtained by: \\[ \\mathbf{\\hat{\\beta}} = \\mathbf{V}_k \\mathbf{\\hat{\\beta}}_k. \\] 5.5.4 Choosing the Number of Components The number of principal components \\(k\\) is often chosen using: - Cross-validation: Minimizing the prediction error, - Explained variance: Selecting enough components to capture a large proportion of the variance. 5.5.5 Advantages of PCR Handles multicollinearity well. Reduces overfitting by eliminating noise and less important predictors. Works well in high-dimensional settings where \\(p &gt; n\\). 5.5.6 Disadvantages PCA is unsupervised: It does not consider the response variable when selecting components, potentially excluding important predictors. Interpretation of principal components can be challenging compared to original features. 5.5.7 Randomized PCA and PCR Randomized PCA offers several advantages when used in the context of Principal Components Regression (PCR), particularly for high-dimensional data and large datasets. 5.5.7.1 Advantages of Randomized PCA in PCR Computational Efficiency: In PCR, PCA is performed on the design matrix \\(\\mathbf{X}\\) to obtain principal components. For large datasets, especially when \\(p \\gg n\\), the cost of computing the full SVD is high. Randomized PCA approximates the principal components by projecting \\(\\mathbf{X}\\) onto a lower-dimensional subspace using a random matrix, significantly reducing computation time without a major loss in accuracy. Scalability: Randomized PCA scales well with extremely large datasets, making it feasible to perform PCR on modern, high-dimensional data without running into memory or time constraints. Memory Efficiency: Traditional PCA requires storing and manipulating large matrices. Randomized PCA reduces the effective dimension early, requiring less memory, which is especially beneficial for PCR when \\(p\\) is very large. Handling High-Dimensional Data: PCR is often used when there are many predictors (features). Randomized PCA efficiently reduces the dimension while preserving the structure, making it an ideal preprocessing step before regression. Fast Approximate Solutions: In many practical applications of PCR, an approximate solution is often sufficient. Randomized PCA provides a close approximation to the leading principal components with much less computational effort, which is particularly useful when iterative model fitting is required. Random Projections Preserve Distances: Results from random matrix theory (like the Johnson-Lindenstrauss lemma) ensure that random projections approximately preserve the geometric structure of the data. This means that the key relationships between variables are retained even after dimensionality reduction, ensuring that the regression model does not lose critical information. 5.5.7.2 Why is this Important in PCR? PCR involves two expensive operations: PCA and then regression on the principal components. Randomized PCA accelerates the first step, allowing rapid extraction of the most important components. The resulting principal components are then used in the regression step, maintaining good predictive performance while reducing computational cost. 5.5.7.3 Example Use Case If you’re working with a dataset where: - \\(p = 100,000\\) features, - \\(n = 1,000\\) samples, performing standard PCA can be unfeasible. Randomized PCA quickly approximates the top \\(k\\) components, enabling the PCR model to be trained efficiently without losing significant predictive power. "],["factor-analysis.html", "6 Factor Analysis 6.1 Introduction 6.2 Factor Analysis Model 6.3 Estimation in Factor Analysis 6.4 Estimating Factor Scores in Factor Analysis 6.5 Factor Analysis Example", " 6 Factor Analysis Factor analysis is a statistical method used to explain the relationships among observed variables by identifying a smaller number of latent factors that account for the observed patterns. 6.1 Introduction Imagine you are a psychologist studying personality traits. You conduct a survey where people rate themselves on multiple characteristics like: Talkative Outgoing Shy Prefers being alone At first glance, these seem like four separate traits, but you might suspect that they all relate to an underlying factor—something like “Extroversion”. Factor Analysis (FA) helps us uncover these hidden patterns in data. Instead of treating every survey question as completely independent, FA looks for common factors that explain the relationships between them. 6.1.1 How Factor Analysis Works You Start with a Big Dataset Let’s say you have survey responses from 500 people on 20 personality traits. The goal is to simplify this by identifying a few core personality factors that drive the responses. FA Looks for Hidden Patterns Instead of analyzing 20 separate traits, FA groups them into a smaller set of factors. For example, traits like “Talkative” and “Outgoing” might load onto a factor called “Extraversion”. Each Trait Has a “Loading” FA calculates how strongly each trait relates to a factor. “Talkative” might have a high loading on Extraversion, while “Prefers being alone” has a negative loading. You End Up with Fewer Factors Instead of working with 20 different traits, FA might show that only 3 or 4 key personality factors explain most of the variation in responses. 6.1.2 Why is Factor Analysis Useful? Psychology: Identifying core personality traits (e.g., the “Big Five” personality factors). Marketing: Understanding customer preferences (e.g., grouping product features into themes). Finance: Analyzing stock market trends (e.g., finding common trends among different stocks). Factor Analysis is a powerful tool for simplifying complex datasets and uncovering hidden relationships between variables. 6.2 Factor Analysis Model 6.2.1 Model Specification We assume that each observed variable \\(\\mathbf{x}\\) follows a Factor Analysis (FA) model: \\[ \\mathbf{x} = \\boldsymbol{\\mu} + \\mathbf{\\Lambda} \\mathbf{f} + \\mathbf{e} \\] where: - \\(\\mathbf{x} \\in \\mathbb{R}^{p}\\) is the observed variable (with mean \\(\\boldsymbol{\\mu}\\)). - \\(\\boldsymbol{\\mu} \\in \\mathbb{R}^{p}\\) is the mean vector of the observed variables. - \\(\\mathbf{\\Lambda} \\in \\mathbb{R}^{p \\times k}\\) is the factor loadings matrix, mapping \\(k\\) latent factors to \\(p\\) observed variables. - \\(\\mathbf{f} \\in \\mathbb{R}^{k}\\) is the latent factor vector, capturing the shared variation among the observed variables. - \\(\\mathbf{e} \\in \\mathbb{R}^{p}\\) is the unique error term, representing variation in \\(\\mathbf{x}\\) that is not explained by the factors. 6.2.2 Assumptions Latent Factors \\(\\mathbb{E}[\\mathbf{f}] = 0\\) (factors have zero mean). \\(\\mathbb{C}[\\mathbf{f}] = \\mathbf{I}_k\\) (factors are uncorrelated and have unit variance). Error Terms \\(\\mathbb{E}[\\mathbf{e}] = 0\\) (errors have zero mean). \\(\\mathbb{C}[\\mathbf{e}] = \\mathbf{\\Psi}\\), where \\(\\mathbf{\\Psi}\\) is a diagonal matrix (each error term has its own variance, and errors are uncorrelated across variables). Independence The latent factors and errors are independent: \\[ \\mathbb{C}[\\mathbf{f}, \\mathbf{e}] = 0. \\] 6.2.3 Variance-Covariance Matrix Taking expectations, we get: \\[ \\mathbb{E}[\\mathbf{x}] = \\boldsymbol{\\mu}. \\] The variance-covariance matrix of \\(\\mathbf{x}\\) is: \\[ \\mathbb{C}[\\mathbf{x}] = \\mathbb{C}[\\mathbf{\\Lambda} \\mathbf{f} + \\mathbf{e}]. \\] Expanding using linearity of covariance: \\[ \\mathbb{C}[\\mathbf{x}] = \\mathbb{C}[\\mathbf{\\Lambda} \\mathbf{f}] + \\mathbb{C}[\\mathbf{e}]. \\] Since \\(\\mathbf{f}\\) has covariance \\(\\mathbf{I}_k\\), we get: \\[ \\mathbb{C}[\\mathbf{x}] = \\mathbf{\\Lambda} \\mathbb{C}[\\mathbf{f}] \\mathbf{\\Lambda}&#39; + \\mathbf{\\Psi}. \\] Substituting \\(\\mathbb{C}[\\mathbf{f}] = \\mathbf{I}_k\\): \\[ \\mathbb{C}[\\mathbf{x}] = \\mathbf{\\Lambda} \\mathbf{\\Lambda}&#39; + \\mathbf{\\Psi}. \\] This means: - \\(\\mathbf{\\Lambda} \\mathbf{\\Lambda}&#39;\\) represents the shared variance explained by the factors. - \\(\\mathbf{\\Psi}\\) represents the unique variance that is specific to each observed variable. 6.2.4 Implications Factor Analysis models the covariance, not the data itself Unlike PCA, which focuses on total variance, FA explicitly separates shared variance (factors) from unique variance (errors). Unique variances appear on the diagonal of \\(\\mathbb{C}[\\mathbf{x}]\\) This explains why FA is useful when trying to model relationships between variables rather than just reducing dimensions. The presence of \\(\\boldsymbol{\\mu}\\) If the data is not mean-centered, the first step in estimation is usually to subtract \\(\\boldsymbol{\\mu}\\), so that the analysis focuses only on variance structure. 6.3 Estimation in Factor Analysis The Factor Analysis (FA) model is given by: \\[ \\mathbf{x} = \\boldsymbol{\\mu} + \\mathbf{\\Lambda} \\mathbf{f} + \\mathbf{e} \\] with the variance-covariance structure: \\[ \\mathbb{C}[\\mathbf{x}] = \\mathbf{\\Lambda} \\mathbf{\\Lambda}&#39; + \\mathbf{\\Psi}. \\] Our goal is to estimate \\(\\mathbf{\\Lambda}\\) (factor loadings) and \\(\\mathbf{\\Psi}\\) (unique variances). 6.3.1 Principal Components Method This method estimates \\(\\mathbf{\\Lambda}\\) by performing Principal Component Analysis (PCA) on the sample covariance matrix and approximating the factors using the leading components. 6.3.1.1 Step 1: Compute the Sample Covariance Matrix Given \\(n\\) observations of \\(p\\) variables in the dataset \\(\\mathbf{X}\\), first compute the sample covariance matrix: \\[ \\hat{\\mathbb{C}}[\\mathbf{x}] = \\frac{1}{n-1} (\\mathbf{X} - \\bar{\\mathbf{x}} {\\boldsymbol 1} ) (\\mathbf{X} - \\bar{\\mathbf{x}} {\\boldsymbol 1} )&#39;. \\] 6.3.1.2 Step 2: Perform Eigen-Decomposition Since the Factor Analysis (FA) model assumes: \\[ \\mathbb{C}[\\mathbf{x}] = \\mathbf{\\Lambda} \\mathbf{\\Lambda}&#39; + \\mathbf{\\Psi}, \\] we initially approximate \\(\\mathbf{\\Psi} \\approx 0\\), so that: \\[ \\mathbb{C}[\\mathbf{x}] \\approx \\mathbf{\\Lambda} \\mathbf{\\Lambda}&#39;. \\] Perform an eigenvalue decomposition: \\[ \\hat{\\mathbb{C}}[\\mathbf{x}] = \\mathbf{V} \\mathbf{D} \\mathbf{V}&#39;, \\] where: - \\(\\mathbf{V}\\) contains the eigenvectors (principal directions). - \\(\\mathbf{D}\\) is a diagonal matrix of eigenvalues. 6.3.1.3 Step 3: Select the First \\(k\\) Components Since we assume a low-rank factor model with \\(k\\) factors, we take only the first \\(k\\) largest eigenvalues and their corresponding eigenvectors: \\[ \\mathbf{V}_k = [\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_k]. \\] The corresponding diagonal eigenvalue matrix: \\[ \\mathbf{D}_k = \\text{diag}(\\lambda_1, \\lambda_2, \\dots, \\lambda_k). \\] 6.3.1.4 Step 4: Compute the Factor Loadings \\(\\mathbf{\\Lambda}\\) The estimated factor loadings matrix is: \\[ \\hat{\\mathbf{\\Lambda}} = \\mathbf{V}_k \\mathbf{D}_k^{1/2}. \\] 6.3.1.5 Step 5: Estimate \\(\\mathbf{\\Psi}\\) To refine the model, we estimate \\(\\mathbf{\\Psi}\\) as the difference between the diagonal of the sample covariance matrix and the diagonal elements of \\(\\mathbf{\\Lambda} \\mathbf{\\Lambda}&#39;\\): \\[ \\hat{\\mathbf{\\Psi}} = \\text{diag}(\\hat{\\mathbb{C}}[\\mathbf{x}]) - \\text{diag}(\\hat{\\mathbf{\\Lambda}} \\hat{\\mathbf{\\Lambda}}&#39;). \\] 6.3.2 Principal Factor Method This method improves upon the Principal Components Method by explicitly estimating \\(\\mathbf{\\Psi}\\) before extracting factors. 6.3.2.1 Step 1: Compute the Sample Covariance Matrix As before, compute: \\[ \\hat{\\mathbb{C}}[\\mathbf{x}] = \\frac{1}{n-1} (\\mathbf{X} - \\bar{\\mathbf{X}})&#39; (\\mathbf{X} - \\bar{\\mathbf{X}}). \\] 6.3.2.2 Step 2: Estimate \\(\\mathbf{\\Psi}\\) First Instead of assuming \\(\\mathbf{\\Psi} \\approx 0\\), we estimate it by taking only the diagonal elements of \\(\\hat{\\mathbb{C}}[\\mathbf{x}]\\): \\[ \\hat{\\mathbf{\\Psi}} = \\text{diag}(\\hat{\\mathbb{C}}[\\mathbf{x}]). \\] Then, compute the reduced correlation matrix: \\[ \\mathbf{R} = \\hat{\\mathbb{C}}[\\mathbf{x}] - \\hat{\\mathbf{\\Psi}}. \\] 6.3.2.3 Step 3: Perform Eigen Decomposition We now compute the eigenvalue decomposition of \\(\\mathbf{R}\\): \\[ \\mathbf{R} = \\mathbf{V} \\mathbf{D} \\mathbf{V}&#39;. \\] The first \\(k\\) eigenvectors of \\(\\mathbf{V}\\) correspond to the estimated factor loadings: \\[ \\hat{\\mathbf{\\Lambda}} = \\mathbf{V}_k \\mathbf{D}_k^{1/2}. \\] 6.3.2.4 Step 4: Refine \\(\\mathbf{\\Psi}\\) To refine the model, we estimate \\(\\mathbf{\\Psi}\\) as the difference between the diagonal of the sample covariance matrix and the diagonal elements of \\(\\mathbf{\\Lambda} \\mathbf{\\Lambda}&#39;\\): \\[ \\hat{\\mathbf{\\Psi}} = \\text{diag}(\\hat{\\mathbb{C}}[\\mathbf{x}]) - \\text{diag}(\\hat{\\mathbf{\\Lambda}} \\hat{\\mathbf{\\Lambda}}&#39;). \\] 6.3.3 Maximum Likelihood Estimation (MLE) Instead of relying on the sample covariance, MLE finds \\(\\mathbf{\\Lambda}\\) and \\(\\mathbf{\\Psi}\\) by maximizing the likelihood: \\[ L(\\mathbf{\\Lambda}, \\mathbf{\\Psi}) = -\\frac{n}{2} \\left[ \\log |\\mathbf{\\Lambda} \\mathbf{\\Lambda}&#39; + \\mathbf{\\Psi}| + \\text{tr}(\\hat{\\mathbb{C}}[\\mathbf{x}] (\\mathbf{\\Lambda} \\mathbf{\\Lambda}&#39; + \\mathbf{\\Psi})^{-1}) \\right]. \\] The MLE approach: - Iteratively updates \\(\\mathbf{\\Lambda}\\) and \\(\\mathbf{\\Psi}\\), - Ensures optimal factor separation, - Can be computed using Expectation-Maximization (EM) algorithms. 6.3.4 Bayesian Estimation (Hierarchical Priors) In Bayesian Factor Analysis, we place priors on \\(\\mathbf{\\Lambda}\\) and \\(\\mathbf{\\Psi}\\): \\[ p(\\mathbf{\\Lambda}) \\sim \\mathcal{N}(0, \\tau^2), \\] \\[ p(\\mathbf{\\Psi}) \\sim \\text{Inverse-Gamma}(\\alpha, \\beta). \\] Using MCMC sampling, we estimate posterior distributions for \\(\\mathbf{\\Lambda}\\) and \\(\\mathbf{\\Psi}\\), providing better regularization when data is noisy. 6.3.4.1 Summary Principal Components Method: A quick approximation using PCA. Principal Factor Method: Improves upon PCA by subtracting \\(\\mathbf{\\Psi}\\) before factor extraction. Maximum Likelihood Estimation: Statistically optimal, found via optimization. Bayesian Methods: Regularized approach when priors are available. 6.4 Estimating Factor Scores in Factor Analysis Once we have estimated the factor loadings \\(\\mathbf{\\Lambda}\\) and the unique variances \\(\\mathbf{\\Psi}\\), we can compute the factor scores, which represent the estimated values of the latent factors for each observation. Since the factors \\(\\mathbf{f}_i\\) are unobserved, we estimate them from the observed data \\(\\mathbf{x}_i\\). The factors themselves can be estimated using different methods. 6.4.1 Ordinary Least Squares (OLS) Method The OLS method estimates factor scores by minimizing the sum of squared residuals between the observed variables and their representation through the factor model. The objective is: \\[ \\min_{\\mathbf{f}_i} \\|\\mathbf{x}_i - \\boldsymbol{\\mu} - \\mathbf{\\Lambda} \\mathbf{f}_i \\|^2. \\] Solving for \\(\\mathbf{f}_i\\), the OLS estimator for the factor scores is: \\[ \\hat{\\mathbf{f}}_i = (\\mathbf{\\Lambda}&#39;\\mathbf{\\Lambda})^{-1} \\mathbf{\\Lambda}&#39; (\\mathbf{x}_i - \\boldsymbol{\\mu}). \\] In practice, we use the estimated factor loadings \\(\\hat{\\mathbf{\\Lambda}}\\) and the sample mean \\(\\bar{\\mathbf{x}}\\): \\[ \\hat{\\mathbf{f}}_i = (\\hat{\\mathbf{\\Lambda}}&#39; \\hat{\\mathbf{\\Lambda}})^{-1} \\hat{\\mathbf{\\Lambda}}&#39; (\\mathbf{x}_i - \\bar{\\mathbf{x}}). \\] This method treats the estimation as a regression problem, solving for the factor scores that best reproduce the observed data. 6.4.2 Weighted Least Squares (WLS) Method (Bartlett’s Method) The WLS method, also known as Bartlett’s estimator, adjusts the OLS approach by weighting the residuals by the inverse of their specific variances. This gives more weight to variables with lower unique variances, reflecting greater confidence in those measurements. The objective function is: \\[ \\min_{\\mathbf{f}_i} (\\mathbf{x}_i - \\boldsymbol{\\mu} - \\mathbf{\\Lambda} \\mathbf{f}_i)&#39; \\mathbf{\\Psi}^{-1} (\\mathbf{x}_i - \\boldsymbol{\\mu} - \\mathbf{\\Lambda} \\mathbf{f}_i). \\] Solving for \\(\\mathbf{f}_i\\), the WLS estimator is: \\[ \\hat{\\mathbf{f}}_i = (\\mathbf{\\Lambda}&#39; \\mathbf{\\Psi}^{-1} \\mathbf{\\Lambda})^{-1} \\mathbf{\\Lambda}&#39; \\mathbf{\\Psi}^{-1} (\\mathbf{x}_i - \\boldsymbol{\\mu}). \\] Using the estimated parameters, we have: \\[ \\hat{\\mathbf{f}}_i = (\\hat{\\mathbf{\\Lambda}}&#39; \\hat{\\mathbf{\\Psi}}^{-1} \\hat{\\mathbf{\\Lambda}})^{-1} \\hat{\\mathbf{\\Lambda}}&#39; \\hat{\\mathbf{\\Psi}}^{-1} (\\mathbf{x}_i - \\hat{\\boldsymbol{\\mu}}). \\] This method emphasizes variables with higher reliability (lower unique variances) when estimating the factor scores. 6.4.3 Regression Method (Thompson’s Estimator) The Regression Method, also known as Thompson’s estimator, derives factor scores by considering the conditional expectation of the factors given the observed data. To obtain this result, we first derive the joint distribution of the observed variables \\(\\mathbf{x}_i\\) and the factors \\(\\mathbf{f}_i\\). 6.4.3.1 Joint Distribution of \\(\\mathbf{x}_i\\) and \\(\\mathbf{f}_i\\) We assume that both the observed data \\(\\mathbf{x}_i\\) and the latent factors \\(\\mathbf{f}_i\\) follow a multivariate normal distribution: \\[ \\begin{bmatrix} \\mathbf{x}_i \\\\ \\mathbf{f}_i \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\boldsymbol{\\mu} \\\\ \\mathbf{0} \\end{bmatrix}, \\begin{bmatrix} \\mathbf{\\Sigma}_{xx} &amp; \\mathbf{\\Sigma}_{xf} \\\\ \\mathbf{\\Sigma}_{fx} &amp; \\mathbf{\\Sigma}_{ff} \\end{bmatrix} \\right), \\] where: - \\(\\mathbf{\\Sigma}_{xx} = \\mathbb{C}[\\mathbf{x}] = \\mathbf{\\Lambda} \\mathbf{\\Lambda}&#39; + \\mathbf{\\Psi}\\) (covariance of observed variables). - \\(\\mathbf{\\Sigma}_{ff} = \\mathbb{C}[\\mathbf{f}] = \\mathbf{I}_k\\) (factors are standardized to have identity covariance). - \\(\\mathbf{\\Sigma}_{xf} = \\mathbb{C}[\\mathbf{x}, \\mathbf{f}] = \\mathbf{\\Lambda}\\) (cross-covariance between observed variables and factors). Thus, the joint distribution is: \\[ \\begin{bmatrix} \\mathbf{x}_i \\\\ \\mathbf{f}_i \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\boldsymbol{\\mu} \\\\ \\mathbf{0} \\end{bmatrix}, \\begin{bmatrix} \\mathbf{\\Lambda} \\mathbf{\\Lambda}&#39; + \\mathbf{\\Psi} &amp; \\mathbf{\\Lambda} \\\\ \\mathbf{\\Lambda}&#39; &amp; \\mathbf{I}_k \\end{bmatrix} \\right). \\] 6.4.3.2 Conditional Distribution of \\(\\mathbf{f}_i\\) Given \\(\\mathbf{x}_i\\) Using the standard formula for the conditional expectation of a multivariate normal distribution: \\[ \\mathbb{E}[\\mathbf{f}_i | \\mathbf{x}_i] = \\mathbf{\\Sigma}_{fx} \\mathbf{\\Sigma}_{xx}^{-1} (\\mathbf{x}_i - \\boldsymbol{\\mu}). \\] Substituting \\(\\mathbf{\\Sigma}_{fx} = \\mathbf{\\Lambda}&#39;\\) and \\(\\mathbf{\\Sigma}_{xx} = \\mathbf{\\Lambda} \\mathbf{\\Lambda}&#39; + \\mathbf{\\Psi}\\): \\[ \\hat{\\mathbf{f}}_i = \\mathbb{E}[\\mathbf{f}_i | \\mathbf{x}_i] = \\mathbf{\\Lambda}&#39; (\\mathbf{\\Lambda} \\mathbf{\\Lambda}&#39; + \\mathbf{\\Psi})^{-1} (\\mathbf{x}_i - \\boldsymbol{\\mu}). \\] Thus, the Thompson factor score estimator is: \\[ \\hat{\\mathbf{f}}_i = \\mathbf{\\Lambda}&#39; (\\mathbf{\\Lambda} \\mathbf{\\Lambda}&#39; + \\mathbf{\\Psi})^{-1} (\\mathbf{x}_i - \\boldsymbol{\\mu}). \\] 6.4.3.3 Interpretation of Thompson’s Estimator This estimator balances the influence of observed variables based on both common variance (factor loadings) and unique variances (specific errors). Unlike OLS and Bartlett’s estimator, this method considers the full covariance structure of the observed variables. Since it is derived from the multivariate normal assumption, it provides the minimum mean squared error (MMSE) estimate of the factor scores. 6.4.4 Summary of Factor Score Estimators Method Formula for Factor Scores Key Feature OLS Estimator \\((\\mathbf{\\Lambda}&#39; \\mathbf{\\Lambda})^{-1} \\mathbf{\\Lambda}&#39; (\\mathbf{x}_i - \\boldsymbol{\\mu})\\) Best fit using least squares WLS (Bartlett) Estimator \\((\\mathbf{\\Lambda}&#39; \\mathbf{\\Psi}^{-1} \\mathbf{\\Lambda})^{-1} \\mathbf{\\Lambda}&#39; \\mathbf{\\Psi}^{-1} (\\mathbf{x}_i - \\boldsymbol{\\mu})\\) Ensures factor scores are uncorrelated Regression (Thompson) Estimator \\(\\mathbf{\\Lambda}&#39; (\\mathbf{\\Lambda} \\mathbf{\\Lambda}&#39; + \\mathbf{\\Psi})^{-1} (\\mathbf{x}_i - \\boldsymbol{\\mu})\\) Uses full covariance structure 6.4.5 Key Takeaways Factor scores estimate the latent variables based on the observed data. Different estimation methods lead to different interpretations of factor scores. The choice of estimator depends on the application—some prioritize minimizing error, while others ensure uncorrelated factors. 6.5 Factor Analysis Example Here’s a full implementation of Factor Analysis in R, covering: Estimating the factor loadings \\(\\mathbf{\\Lambda}\\) and unique variances \\(\\mathbf{\\Psi}\\) Principal Components Method Maximum Likelihood Estimation (MLE) Estimating the factor scores using three methods: Ordinary Least Squares (OLS) Weighted Least Squares (Bartlett’s method) Regression (Thompson’s estimator) 6.5.1 Load Required Libraries # Load necessary libraries library(psych) # For factor analysis functions library(MASS) # For matrix operations 6.5.2 Generate Sample Data We generate synthetic data with a predefined factor structure. # Set seed for reproducibility set.seed(123) # Define parameters n &lt;- 500 # Number of observations p &lt;- 6 # Number of observed variables k &lt;- 2 # Number of factors # True factor loadings (simulated) Lambda_true &lt;- matrix(c(0.9, 0.8, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9, 0.8, 0.7), nrow = p, ncol = k) # Unique variances (diagonal matrix) Psi_true &lt;- diag(c(0.2, 0.3, 0.2, 0.2, 0.3, 0.2)) # Generate factor scores (standard normal) F_scores &lt;- matrix(rnorm(n * k), nrow = n, ncol = k) # Generate observed data E_noise &lt;- mvrnorm(n, mu = rep(0, p), Sigma = Psi_true) # Unique variances X &lt;- F_scores %*% t(Lambda_true) + E_noise # Factor model # Standardize data X &lt;- scale(X) 6.5.3 Estimation of Factor Loadings and Unique Variances 6.5.3.1 Principal Components Method # Compute covariance matrix S &lt;- cov(X) # Perform eigen decomposition eig &lt;- eigen(S) # Extract first k eigenvectors and eigenvalues Lambda_pc &lt;- eig$vectors[, 1:k] %*% diag(sqrt(eig$values[1:k])) # Estimate unique variances Psi Psi_pc &lt;- diag(S) - diag(Lambda_pc %*% t(Lambda_pc)) # Display results print(&quot;Factor Loadings (Principal Components Method):&quot;) ## [1] &quot;Factor Loadings (Principal Components Method):&quot; print(round(Lambda_pc, 3)) ## [,1] [,2] ## [1,] 0.659 -0.641 ## [2,] 0.612 -0.651 ## [3,] 0.628 -0.649 ## [4,] -0.689 -0.611 ## [5,] -0.705 -0.572 ## [6,] -0.677 -0.596 print(&quot;Unique Variances (Principal Components Method):&quot;) ## [1] &quot;Unique Variances (Principal Components Method):&quot; print(round(Psi_pc, 3)) ## [1] 0.156 0.203 0.184 0.152 0.175 0.186 6.5.3.2 Maximum Likelihood Estimation (MLE) # Perform MLE Factor Analysis fa_mle &lt;- factanal(X, factors = k, rotation = &quot;none&quot;) # Extract factor loadings Lambda_mle &lt;- fa_mle$loadings[, 1:k] # Estimate unique variances Psi Psi_mle &lt;- diag(S) - diag(Lambda_mle %*% t(Lambda_mle)) # Display results print(&quot;Factor Loadings (MLE Method):&quot;) ## [1] &quot;Factor Loadings (MLE Method):&quot; print(round(Lambda_mle, 3)) ## Factor1 Factor2 ## [1,] -0.575 0.685 ## [2,] -0.500 0.649 ## [3,] -0.528 0.663 ## [4,] 0.726 0.518 ## [5,] 0.716 0.469 ## [6,] 0.686 0.481 print(&quot;Unique Variances (MLE Method):&quot;) ## [1] &quot;Unique Variances (MLE Method):&quot; print(round(Psi_mle, 3)) ## [1] 0.200 0.328 0.281 0.204 0.268 0.298 6.5.4 Estimate Factor Scores 6.5.4.1 Ordinary Least Squares (OLS) Factor Scores # Compute OLS factor scores F_ols &lt;- solve(t(Lambda_mle) %*% Lambda_mle) %*% t(Lambda_mle) %*% t(X) F_ols &lt;- t(F_ols) # Transpose to match dimensions # Display first few factor scores print(&quot;Factor Scores (OLS Method):&quot;) ## [1] &quot;Factor Scores (OLS Method):&quot; print(round(F_ols[1:5, ], 3)) ## Factor1 Factor2 ## [1,] -0.454 -1.093 ## [2,] -0.230 -1.224 ## [3,] -0.039 1.097 ## [4,] 0.909 0.484 ## [5,] -0.952 -0.539 6.5.4.2 Weighted Least Squares (Bartlett’s Method) # Compute WLS factor scores F_wls &lt;- solve(t(Lambda_mle) %*% (Lambda_mle / Psi_mle)) %*% t(Lambda_mle) %*% t(X) / Psi_mle ## Warning in solve(t(Lambda_mle) %*% (Lambda_mle/Psi_mle)) %*% t(Lambda_mle) %*% : longer object length is not a multiple of shorter object length F_wls &lt;- t(F_wls) # Display first few factor scores print(&quot;Factor Scores (Bartlett’s Method):&quot;) ## [1] &quot;Factor Scores (Bartlett’s Method):&quot; print(round(F_wls[1:5, ], 3)) ## Factor1 Factor2 ## [1,] -0.551 -0.838 ## [2,] -0.192 -1.512 ## [3,] -0.048 0.932 ## [4,] 1.128 0.364 ## [5,] -0.841 -0.652 6.5.4.3 Regression (Thompson’s Estimator) # Compute Regression factor scores F_reg &lt;- t(Lambda_mle) %*% solve(Lambda_mle %*% t(Lambda_mle) + diag(Psi_mle), t(X)) F_reg &lt;- t(F_reg) # Display first few factor scores print(&quot;Factor Scores (Regression Method - Thompson’s Estimator):&quot;) ## [1] &quot;Factor Scores (Regression Method - Thompson’s Estimator):&quot; print(round(F_reg[1:5, ], 3)) ## Factor1 Factor2 ## [1,] -0.470 -0.883 ## [2,] -0.187 -1.093 ## [3,] -0.003 0.933 ## [4,] 0.765 0.396 ## [5,] -0.902 -0.412 "],["canonical-correlation-analysis-cca.html", "7 Canonical Correlation Analysis (CCA) 7.1 Motivation 7.2 Mathematical Formulation 7.3 Canonical Directions Estimation 7.4 HD CCA 7.5 Conclusion: Which Method to Use?", " 7 Canonical Correlation Analysis (CCA) Canonical Correlation Analysis (CCA) is a multivariate statistical method used to study the relationships between two sets of variables. It generalizes correlation by finding pairs of linear combinations (called canonical variables) that maximize the correlation between the two datasets. 7.1 Motivation Imagine you have two sets of related variables: - Set 1 (e.g., Psychological traits) → \\(\\mathbf{X}\\) (e.g., intelligence, memory, reasoning) - Set 2 (e.g., Academic performance) → \\(\\mathbf{Y}\\) (e.g., math scores, reading scores, writing scores) Standard correlation measures relationships between individual variables (e.g., intelligence vs. math score). CCA, however, finds pairs of linear combinations that maximize the correlation between the two sets. 7.2 Mathematical Formulation Given: - \\(\\mathbf{X}\\) (a matrix of size \\(n \\times p\\)) representing \\(p\\) variables. - \\(\\mathbf{Y}\\) (a matrix of size \\(n \\times q\\)) representing \\(q\\) variables. CCA finds weight vectors \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) such that the linear combinations: \\[ U = \\mathbf{X} \\mathbf{a}, \\quad V = \\mathbf{Y} \\mathbf{b} \\] maximize the correlation: \\[ \\rho = \\frac{\\mathbb{C}[U, V]}{\\sqrt{\\mathbb{V}[U] \\mathbb{V}[V]}}. \\] where: - \\(U\\) and \\(V\\) are called canonical variables. - \\(\\rho\\) is the canonical correlation. 7.2.1 Key Properties *Finds relationships between two sets of variables**, beyond individual correlations. *Can extract multiple pairs** of canonical variables, each pair capturing a different aspect of the relationship. *Works even when the number of variables in \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) are different**. 7.2.2 Implementation in R We can perform Canonical Correlation Analysis (CCA) in R using the cancor() function. # Load dataset data(&quot;iris&quot;) # Define two variable sets X &lt;- as.matrix(iris[, 1:2]) # Sepal Length and Sepal Width Y &lt;- as.matrix(iris[, 3:4]) # Petal Length and Petal Width # Perform CCA cca_result &lt;- cancor(X, Y) # Print canonical correlations print(cca_result$cor) ## [1] 0.9409690 0.1239369 # Print canonical weight vectors print(&quot;Canonical Weights for X:&quot;) ## [1] &quot;Canonical Weights for X:&quot; print(cca_result$xcoef) ## [,1] [,2] ## Sepal.Length -0.08757435 0.04749411 ## Sepal.Width 0.07004363 0.17582970 print(&quot;Canonical Weights for Y:&quot;) ## [1] &quot;Canonical Weights for Y:&quot; print(cca_result$ycoef) ## [,1] [,2] ## Petal.Length -0.06956302 -0.1571867 ## Petal.Width 0.05683849 0.3940121 7.2.2.1 Interpreting the Results Canonical Correlations (cca_result$cor) These are the highest correlations between linear combinations of \\(X\\) and \\(Y\\). If high (close to 1), the datasets are strongly related. Canonical Coefficients (cca_result$xcoef and cca_result$ycoef) These tell us how each original variable contributes to the canonical variables. Larger absolute values indicate stronger contributions. 7.2.2.2 Visualizing Canonical Correlation Analysis (CCA) Results To better understand Canonical Correlation Analysis (CCA), we can use scatter plots and biplots to visualize the relationships between canonical variables. 7.2.2.2.1 Scatter Plot of Canonical Variables This plot shows the first pair of canonical variables \\(U_1\\) and \\(V_1\\), allowing us to see how well they are correlated. # Load required libraries library(ggplot2) # Compute canonical variables U &lt;- as.matrix(X) %*% cca_result$xcoef[,1] # First canonical variable for X V &lt;- as.matrix(Y) %*% cca_result$ycoef[,1] # First canonical variable for Y # Create data frame for plotting cca_df &lt;- data.frame(U1 = U, V1 = V) # Scatter plot of canonical variables ggplot(cca_df, aes(x = U1, y = V1)) + geom_point(color = &quot;blue&quot;, alpha = 0.6) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, se = FALSE) + theme_minimal() + labs(title = &quot;Scatter Plot of First Canonical Variables&quot;, x = &quot;Canonical Variable 1 (U1 from X)&quot;, y = &quot;Canonical Variable 1 (V1 from Y)&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Interpretation: If the correlation is strong, the points will align closely along a line. A weak correlation will show a dispersed cloud of points. 7.2.2.2.2 Biplot of Canonical Coefficients A biplot shows how the original variables contribute to the canonical variables. # Extract canonical coefficients for X and Y cca_x &lt;- cca_result$xcoef[, 1:2] # First two canonical coefficients for X cca_y &lt;- cca_result$ycoef[, 1:2] # First two canonical coefficients for Y # Create a data frame for the arrows cca_biplot_arrows &lt;- data.frame( Variable = c(rownames(cca_x), rownames(cca_y)), Canonical1 = c(cca_x[,1], cca_y[,1]), Canonical2 = c(cca_x[,2], cca_y[,2]), Set = rep(c(&quot;X Variables&quot;, &quot;Y Variables&quot;), c(nrow(cca_x), nrow(cca_y))) ) # Create the biplot with arrows ggplot(cca_biplot_arrows, aes(x = 0, y = 0, xend = Canonical1, yend = Canonical2, color = Set)) + geom_segment(arrow = arrow(length = unit(0.2, &quot;inches&quot;)), size = 1) + # Draw arrows geom_text(aes(x = Canonical1, y = Canonical2, label = Variable), vjust = 1.5, hjust = 1.5) + # Add labels geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + # Add horizontal line geom_vline(xintercept = 0, linetype = &quot;dashed&quot;) + # Add vertical line theme_minimal() + labs(title = &quot;Biplot of Canonical Coefficients&quot;, x = &quot;First Canonical Variable&quot;, y = &quot;Second Canonical Variable&quot;) + theme(legend.position = &quot;bottom&quot;) Interpretation: Arrows represent original variables and their influence on canonical variables. Longer arrows indicate variables that contribute more to the canonical correlation. Variables pointing in the same direction are highly correlated. 7.2.2.2.3 Canonical Correlation Bar Plot A simple bar plot can be used to visualize the strength of each canonical correlation. # Create bar plot of canonical correlations barplot(cca_result$cor, names.arg = paste(&quot;Canonical Pair&quot;, 1:length(cca_result$cor)), col = &quot;steelblue&quot;, main = &quot;Canonical Correlations&quot;, ylab = &quot;Correlation&quot;, ylim = c(0,1)) Interpretation: High correlation values suggest a strong relationship between the two datasets. If only the first few pairs have high correlations, then only those are meaningful. 7.2.2.2.4 Summary of Visualizations Scatter plot → Shows correlation between the first pair of canonical variables. Biplot → Shows how the original variables influence the canonical variables. Bar plot → Displays the strength of canonical correlations. 7.3 Canonical Directions Estimation 7.3.1 Problem Definition with Random Variables Let \\(\\mathbf{x} \\in \\mathbb{R}^{p}\\) and \\(\\mathbf{y} \\in \\mathbb{R}^{q}\\) be two sets of random variables with zero mean and the following covariance matrices: \\[ \\mathbf{\\Sigma}_{XX} = \\mathbb{E}[\\mathbf{x} \\mathbf{x}&#39;] \\quad \\text{(Covariance of \\( \\mathbf{x} \\))} \\] \\[ \\mathbf{\\Sigma}_{YY} = \\mathbb{E}[\\mathbf{y} \\mathbf{y}&#39;] \\quad \\text{(Covariance of \\( \\mathbf{y} \\))} \\] \\[ \\mathbf{\\Sigma}_{XY} = \\mathbb{C}[\\mathbf{x}, \\mathbf{y}] \\quad \\text{(Cross-covariance between \\( \\mathbf{x} \\) and \\( \\mathbf{y} \\))} \\] We seek canonical directions \\(\\mathbf{a} \\in \\mathbb{R}^{p}\\) and \\(\\mathbf{b} \\in \\mathbb{R}^{q}\\) such that the transformed random variables: \\[ U = \\mathbf{a}&#39; \\mathbf{x}, \\quad V = \\mathbf{b}&#39; \\mathbf{y} \\] are maximally correlated. 7.3.2 Canonical Correlation Maximization Problem Mathematically, we solve: \\[ \\max_{\\mathbf{a}, \\mathbf{b}} \\quad \\frac{\\mathbb{C}[U, V]}{\\sqrt{\\mathbb{V}[U] \\mathbb{V}[V]}}. \\] Expanding in terms of covariances: \\[ \\max_{\\mathbf{a}, \\mathbf{b}} \\quad \\frac{\\mathbf{a}&#39; \\mathbf{\\Sigma}_{XY} \\mathbf{b}}{\\sqrt{\\mathbf{a}&#39; \\mathbf{\\Sigma}_{XX} \\mathbf{a} \\cdot \\mathbf{b}&#39; \\mathbf{\\Sigma}_{YY} \\mathbf{b}}}. \\] To ensure identifiability, we impose the normalization constraints: \\[ \\mathbf{a}&#39; \\mathbf{\\Sigma}_{XX} \\mathbf{a} = 1, \\quad \\mathbf{b}&#39; \\mathbf{\\Sigma}_{YY} \\mathbf{b} = 1. \\] This normalization ensures that the denominator is fixed at 1, so the objective function measures a valid correlation. 7.3.3 Solving for the Canonical Directions Using Lagrange Multipliers We introduce Lagrange multipliers \\(\\lambda\\) and \\(\\mu\\) and define the Lagrangian function: \\[ \\mathcal{L}(\\mathbf{a}, \\mathbf{b}, \\lambda, \\mu) = \\mathbf{a}&#39; \\mathbf{\\Sigma}_{XY} \\mathbf{b} - \\frac{\\lambda}{2} (\\mathbf{a}&#39; \\mathbf{\\Sigma}_{XX} \\mathbf{a} - 1) - \\frac{\\mu}{2} (\\mathbf{b}&#39; \\mathbf{\\Sigma}_{YY} \\mathbf{b} - 1). \\] 7.3.3.1 First-order conditions Taking the derivative with respect to \\(\\mathbf{a}\\) and setting it to zero: \\[ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}} = \\mathbf{\\Sigma}_{XY} \\mathbf{b} - \\lambda \\mathbf{\\Sigma}_{XX} \\mathbf{a} = 0. \\] \\[ \\mathbf{\\Sigma}_{XY} \\mathbf{b} = \\lambda \\mathbf{\\Sigma}_{XX} \\mathbf{a}. \\] Similarly, differentiating with respect to \\(\\mathbf{b}\\) and setting it to zero: \\[ \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}} = \\mathbf{\\Sigma}_{YX} \\mathbf{a} - \\mu \\mathbf{\\Sigma}_{YY} \\mathbf{b} = 0. \\] \\[ \\mathbf{\\Sigma}_{YX} \\mathbf{a} = \\mu \\mathbf{\\Sigma}_{YY} \\mathbf{b}. \\] 7.3.3.2 Reformulation as a Generalized Eigenvalue Problem Multiplying the first equation on the left by \\(\\mathbf{\\Sigma}_{YX}\\) gives: \\[ \\mathbf{\\Sigma}_{YX} \\mathbf{\\Sigma}_{XY} \\mathbf{b} = \\lambda \\mathbf{\\Sigma}_{YX} \\mathbf{\\Sigma}_{XX} \\mathbf{a}. \\] Substituting \\(\\mathbf{\\Sigma}_{YX} \\mathbf{a} = \\mu \\mathbf{\\Sigma}_{YY} \\mathbf{b}\\): \\[ \\mathbf{\\Sigma}_{YX} \\mathbf{\\Sigma}_{XY} \\mathbf{b} = \\lambda \\mu \\mathbf{\\Sigma}_{YY} \\mathbf{b}. \\] Similarly, multiplying the second equation on the left by \\(\\mathbf{\\Sigma}_{XY}\\): \\[ \\mathbf{\\Sigma}_{XY} \\mathbf{\\Sigma}_{YX} \\mathbf{a} = \\lambda \\mu \\mathbf{\\Sigma}_{XX} \\mathbf{a}. \\] Thus, the canonical directions are found by solving the generalized eigenvalue problems: \\[ \\mathbf{\\Sigma}_{XX}^{-1} \\mathbf{\\Sigma}_{XY} \\mathbf{\\Sigma}_{YY}^{-1} \\mathbf{\\Sigma}_{YX} \\mathbf{a} = \\lambda \\mu \\mathbf{a}. \\] \\[ \\mathbf{\\Sigma}_{YY}^{-1} \\mathbf{\\Sigma}_{YX} \\mathbf{\\Sigma}_{XX}^{-1} \\mathbf{\\Sigma}_{XY} \\mathbf{b} = \\lambda \\mu \\mathbf{b}. \\] 7.3.4 Showing That the Matrices Have the Same Eigenvalues Define: \\[ \\mathbf{M} = \\mathbf{\\Sigma}_{YY}^{-1} \\mathbf{\\Sigma}_{YX}, \\quad \\mathbf{N} = \\mathbf{\\Sigma}_{XX}^{-1} \\mathbf{\\Sigma}_{XY}. \\] Then we rewrite: \\[ \\mathbf{\\Sigma}_{YY}^{-1} \\mathbf{\\Sigma}_{YX} \\mathbf{\\Sigma}_{XX}^{-1} \\mathbf{\\Sigma}_{XY} = \\mathbf{M} \\mathbf{N}, \\] \\[ \\mathbf{\\Sigma}_{XX}^{-1} \\mathbf{\\Sigma}_{XY} \\mathbf{\\Sigma}_{YY}^{-1} \\mathbf{\\Sigma}_{YX} = \\mathbf{N} \\mathbf{M}. \\] To show that \\(\\mathbf{M} \\mathbf{N}\\) and \\(\\mathbf{N} \\mathbf{M}\\) have the same eigenvalues, consider the eigenvalue equation: \\[ \\mathbf{M} \\mathbf{N} \\mathbf{v} = \\lambda \\mathbf{v}. \\] Multiplying both sides by \\(\\mathbf{N}\\): \\[ \\mathbf{N} \\mathbf{M} \\mathbf{N} \\mathbf{v} = \\lambda \\mathbf{N} \\mathbf{v}. \\] Defining \\(\\mathbf{w} = \\mathbf{N} \\mathbf{v}\\), we obtain: \\[ \\mathbf{N} \\mathbf{M} \\mathbf{w} = \\lambda \\mathbf{w}. \\] Thus, every eigenvalue of \\(\\mathbf{M} \\mathbf{N}\\) is also an eigenvalue of \\(\\mathbf{N} \\mathbf{M}\\), which proves that these two matrices have the same eigenvalues. 7.3.5 Showing That the Largest Eigenvalue Maximizes the Objective Function For the estimated canonical directions \\(\\mathbf{a}_1, \\mathbf{b}_1\\), the value of the objective function is: \\[ \\rho_1 = \\frac{\\mathbf{a}_1&#39; \\mathbf{\\Sigma}_{XY} \\mathbf{b}_1}{\\sqrt{\\mathbf{a}_1&#39; \\mathbf{\\Sigma}_{XX} \\mathbf{a}_1 \\cdot \\mathbf{b}_1&#39; \\mathbf{\\Sigma}_{YY} \\mathbf{b}_1}}. \\] Since the eigenvectors of: \\[ \\mathbf{\\Sigma}_{XX}^{-1} \\mathbf{\\Sigma}_{XY} \\mathbf{\\Sigma}_{YY}^{-1} \\mathbf{\\Sigma}_{YX} \\] are sorted in decreasing order of eigenvalues, the first eigenvalue corresponds to the maximum value of \\(\\rho\\). This justifies why the largest eigenvalue gives the first canonical correlation, which achieves the maximum correlation. 7.3.6 Conclusion CCA maximization leads to a generalized eigenvalue problem. The two matrices involved have the same eigenvalues, ensuring consistency. The largest eigenvalue corresponds to the maximum canonical correlation, justifying why the eigenvalue problem provides the optimal canonical directions. 7.4 HD CCA When \\(p \\gg n\\) or \\(q \\gg n\\), the covariance matrices \\(\\mathbf{\\Sigma}_{XX}\\) and \\(\\mathbf{\\Sigma}_{YY}\\) are singular (non-invertible). This makes standard Canonical Correlation Analysis (CCA) infeasible because it relies on inverting these matrices. To address this issue, high-dimensional CCA methods have been developed. Below are the most effective alternatives: 7.4.1 Regularized Canonical Correlation Analysis (Ridge CCA) 7.4.1.1 Idea Instead of solving the standard CCA problem: \\[ \\mathbf{\\Sigma}_{XX}^{-1} \\mathbf{\\Sigma}_{XY} \\mathbf{\\Sigma}_{YY}^{-1} \\mathbf{\\Sigma}_{YX} \\mathbf{a} = \\lambda \\mathbf{a}, \\] add a ridge regularization term: \\[ (\\mathbf{\\Sigma}_{XX} + \\lambda \\mathbf{I})^{-1} \\mathbf{\\Sigma}_{XY} (\\mathbf{\\Sigma}_{YY} + \\lambda \\mathbf{I})^{-1} \\mathbf{\\Sigma}_{YX} \\mathbf{a} = \\lambda \\mathbf{a}. \\] 7.4.1.2 Advantages Avoids singularity by shrinking the covariance matrices. Works well when \\(p, q \\gg n\\). Can be solved efficiently using Cholesky decomposition. 7.4.1.3 Computational Complexity \\(O(p^2 n)\\) if using Woodbury identity for inversion. Faster than classical CCA for large \\(p, q\\). 7.4.2 Sparse Canonical Correlation Analysis (Sparse CCA)** 7.4.2.1 Idea Instead of using all variables, enforce sparsity in \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) by solving: \\[ \\max_{\\mathbf{a}, \\mathbf{b}} \\quad \\mathbf{a}&#39; \\mathbf{\\Sigma}_{XY} \\mathbf{b} \\quad \\text{s.t.} \\quad \\|\\mathbf{a}\\|_1 \\leq c_1, \\quad \\|\\mathbf{b}\\|_1 \\leq c_2. \\] where \\(\\|\\cdot\\|_1\\) is the L1 norm (sum of absolute values), enforcing sparsity. 7.4.2.2 Advantages Selects only the most relevant variables. Reduces overfitting. Works well when \\(p, q \\gg n\\). 7.4.2.3 Computational Complexity Uses LASSO-like optimization (\\(O(n p)\\)), making it efficient for large \\(p, q\\). 7.4.3 Low-Rank Approximation CCA (Randomized SVD Approach) 7.4.3.1 Idea Instead of computing full covariance matrices, compress the data via a low-rank approximation: Compute randomized SVD of \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\): \\[ \\mathbf{X} \\approx \\mathbf{U}_X \\mathbf{\\Sigma}_X \\mathbf{V}_X&#39; \\] \\[ \\mathbf{Y} \\approx \\mathbf{U}_Y \\mathbf{\\Sigma}_Y \\mathbf{V}_Y&#39; \\] Perform CCA on reduced-dimension data \\(\\mathbf{U}_X\\) and \\(\\mathbf{U}_Y\\). 7.4.3.2 Advantages Reduces dimensionality before computing canonical directions. Works well when \\(p, q \\gg n\\). Computationally efficient. 7.4.3.3 Computational Complexity \\(O(n^2 p)\\) instead of \\(O(p^3)\\). 7.4.4 Factor Model-Based CCA 7.4.4.1 Idea Instead of estimating full covariance matrices, assume that data follows a factor model: \\[ \\mathbf{X} = \\mathbf{\\Lambda}_X \\mathbf{F} + \\mathbf{\\epsilon}_X \\] \\[ \\mathbf{Y} = \\mathbf{\\Lambda}_Y \\mathbf{F} + \\mathbf{\\epsilon}_Y \\] where: - \\(\\mathbf{F}\\) is a low-dimensional latent factor. - \\(\\mathbf{\\Lambda}_X\\) and \\(\\mathbf{\\Lambda}_Y\\) are factor loadings. Solve CCA on factor scores instead of raw data. 7.4.4.2 Advantages Works well when \\(p, q \\gg n\\). Reduces dimensionality by modeling latent structure. 7.4.4.3 Computational Complexity \\(O(n k^2)\\) if using PCA for factor estimation. 7.4.5 Comparison of Methods for High-Dimensional CCA Method Handles \\(p, q \\gg n\\)? Avoids Inversion? Captures Nonlinearity? Computational Cost Ridge CCA ✅ Yes ✅ Regularized ❌ No \\(O(p^2 n)\\) Sparse CCA ✅ Yes ✅ Sparse Estimation ❌ No \\(O(n p)\\) Low-Rank SVD CCA ✅ Yes ✅ Uses SVD ❌ No \\(O(n^2 p)\\) Factor Model CCA ✅ Yes ✅ Uses PCA/FA ❌ No \\(O(n k^2)\\) 7.5 Conclusion: Which Method to Use? If you want a direct fix for singularity → Use Ridge CCA. If you suspect only a few variables matter → Use Sparse CCA. If you want a computationally efficient approach → Use Low-Rank SVD CCA. If data follows latent structures → Use Factor Model CCA. "],["k-means-clustering.html", "8 k-Means Clustering 8.1 Introduction 8.2 Problem Statement 8.3 Naive Solution 8.4 The k-Means Algorithm (Lloyd’s algorithm)", " 8 k-Means Clustering 8.1 Introduction k-Means clustering is one of the most widely used unsupervised learning algorithms for partitioning data into distinct groups based on similarity. It is a simple yet powerful method that minimizes the within-cluster variation. 8.1.1 Intuition Behind k-Means Imagine you are tasked with grouping points on a map. k-Means aims to achieve this by: ✅ Placing \\(k\\) “centers” (called centroids) on the map. ✅ Assigning each point to the closest centroid. ✅ Adjusting the centroid positions to minimize the distance between points and their assigned centroid. ✅ Repeating this process until the clusters are stable. The algorithm tries to minimize the within-cluster sum of squares (WCSS) — essentially grouping points that are closer to each other than to points in other clusters. 8.1.2 Applications of k-Means k-Means is highly versatile and used in various domains: Business and Marketing Customer Segmentation: Group customers based on purchasing behavior. Market Segmentation: Identify distinct user profiles for targeted marketing. Image and Video Processing Color Quantization: Compress images by reducing the number of colors. Object Detection: Cluster pixel intensities for image segmentation. Anomaly Detection Identify outliers by clustering data points and flagging those farthest from the centroids. Bioinformatics Cluster gene expression data for identifying gene functions. 8.1.3 Choosing the Number of Clusters \\(k\\) Choosing \\(k\\) is critical for performance. Common techniques include: ✅ Elbow Method: Plot WCSS against \\(k\\) and identify the point where the reduction in WCSS slows down (the “elbow”). ✅ Silhouette Score: Measures how well each point fits into its cluster (higher is better). ✅ Gap Statistic: Compares WCSS to that expected under random data. 8.1.4 R Implementation of k-Means 8.1.4.1 Step 1: Load Libraries and Simulate Data # Load necessary library library(ggplot2) # Simulate example data set.seed(123) n &lt;- 300 # Number of points X &lt;- data.frame( x = c(rnorm(n/3, 0, 1), rnorm(n/3, 4, 1), rnorm(n/3, 8, 1)), y = c(rnorm(n/3, 0, 1), rnorm(n/3, 4, 1), rnorm(n/3, 8, 1)) ) 8.1.4.2 Step 2: Perform k-Means Clustering # Perform k-Means clustering set.seed(123) # Ensures reproducibility k &lt;- 3 # Number of clusters kmeans_result &lt;- kmeans(X, centers = k, nstart = 25) # Add cluster labels to the dataset X$cluster &lt;- as.factor(kmeans_result$cluster) 8.1.4.3 Step 3: Visualize the Results # Visualize the result ggplot(X, aes(x = x, y = y, color = cluster)) + geom_point(size = 2) + geom_point(data = as.data.frame(kmeans_result$centers), aes(x = x, y = y), color = &quot;black&quot;, size = 4, shape = 8) + labs(title = &quot;k-Means Clustering&quot;, subtitle = &quot;k = 3&quot;) 8.1.4.4 Step 4: Determine the Optimal Number of Clusters Using the Elbow Method # Elbow method to find optimal k wcss &lt;- numeric(10) for (k in 1:10) { wcss[k] &lt;- kmeans(X, centers = k, nstart = 25)$tot.withinss } # Plot the WCSS to identify the &#39;elbow&#39; plot(1:10, wcss, type = &quot;b&quot;, pch = 19, frame = FALSE, xlab = &quot;Number of Clusters (k)&quot;, ylab = &quot;WCSS (Within-Cluster Sum of Squares)&quot;) 8.1.5 Strengths and Weaknesses of k-Means ✅ Strengths: - Efficient for large datasets. - Simple and intuitive. - Fast convergence with \\(O(n k d i)\\) complexity. ❗️Weaknesses: - Assumes spherical clusters. - Sensitive to initialization (use k-Means++ to improve). - Struggles with non-convex shapes or clusters with varying densities. - May converge to local minima — multiple runs improve stability. 8.1.6 When to Use k-Means ✅ Use k-Means when: - The data has compact, well-separated clusters. - The feature space is low to moderate-dimensional. - Speed is a priority (k-Means is fast). ❗️Avoid k-Means when: - Clusters are non-convex or overlapping (consider DBSCAN or Spectral Clustering). - The data has significant outliers. 8.1.7 Conclusion k-means k-Means is a powerful yet simple clustering algorithm that performs well in many practical applications. While its efficiency is attractive, thoughtful parameter tuning and multiple runs are essential for achieving optimal results. 8.2 Problem Statement Given a dataset with \\(n\\) observations and \\(d\\) features: \\[ \\mathbf{X} = \\{ \\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_n \\} \\quad \\text{where} \\quad \\mathbf{x}_i \\in \\mathbb{R}^d \\] The goal is to partition the data into \\(k\\) clusters \\(\\{ C_1, C_2, \\dots, C_k \\}\\) such that the total within-cluster sum of squares (WCSS) is minimized: \\[ \\min_{C_1, \\dots, C_k} \\sum_{j=1}^{k} \\sum_{\\mathbf{x}_i \\in C_j} \\| \\mathbf{x}_i - \\boldsymbol{\\mu}_j \\|^2 \\] where: - \\(\\boldsymbol{\\mu}_j = \\frac{1}{|C_j|} \\sum_{\\mathbf{x}_i \\in C_j} \\mathbf{x}_i\\) is the centroid of cluster \\(C_j\\). - The objective minimizes the sum of squared distances between points and their respective cluster centroids. 8.3 Naive Solution Why Can We Solve the Partition Problem by Evaluating Every Possible Partition? To understand why a “naive” method — where we evaluate all possible groupings of elements — can correctly compute the number of partitions, we need to analyze the combinatorial structure of partitions. 8.3.1 Why Evaluating All Possible Partitions Works? To see why the naive approach is valid, we can build partitions in an incremental way: 8.3.1.1 Step 1: Start with an Empty Set Suppose we have a set \\(\\{1, 2, \\dots, n \\}\\). 8.3.1.2 Step 2: Add Elements One by One When adding the \\(i\\)-th element to an existing partition: - It can join any existing subset. - Or it can form a new subset. Thus, at each step, there are two choices: - Place the element in one of the existing clusters. - Start a new cluster with the element alone. This combinatorial logic corresponds directly to the recurrence relation for Stirling numbers of the second kind: \\[ S(n, k) = k \\cdot S(n-1, k) + S(n-1, k-1) \\] The first term represents placing the new element in one of the existing \\(k\\) groups. The second term represents starting a new group, increasing the cluster count from \\(k-1\\) to \\(k\\). 8.3.2 Why Does This Work for All Possible Partitions? By systematically generating all possible combinations: - Every valid partition is explored. - Each combination adheres to the non-overlapping, non-empty subset condition. - The method ensures no duplicates are counted by the incremental logic. 8.3.3 Efficiency of the Naive Approach The naive method effectively explores the lattice of set partitions, where each state corresponds to a valid grouping. While correct, this method has exponential complexity: \\(O(n!)\\) for brute force generation. This is why the recursive Stirling number relation or dynamic programming is preferable for larger \\(n\\). 8.3.4 Key Intuition ✅ Each element has multiple valid placements (existing subset or new subset). ✅ Exploring all combinations inherently builds all valid partitions. ✅ The recurrence relation mirrors this exact branching structure. 8.4 The k-Means Algorithm (Lloyd’s algorithm) The k-Means algorithm uses an iterative refinement approach. The steps are: Step 1: Initialize \\(k\\) cluster centroids. - Common methods include: - Random Initialization: Randomly select \\(k\\) points as centroids. - k-Means++ Initialization (recommended): Selects initial centroids to maximize cluster separation. Step 2: Assign points to the nearest centroid. - For each observation \\(\\mathbf{x}_i\\), assign it to the cluster with the closest centroid: \\[ \\text{Cluster}(\\mathbf{x}_i) = \\arg \\min_{j} \\|\\mathbf{x}_i - \\boldsymbol{\\mu}_j\\|^2 \\] Step 3: Update the centroids. - Recalculate each centroid as the mean of its assigned points: \\[ \\boldsymbol{\\mu}_j = \\frac{1}{|C_j|} \\sum_{\\mathbf{x}_i \\in C_j} \\mathbf{x}_i \\] Step 4: Repeat Steps 2 and 3 until convergence. - Convergence occurs when cluster assignments no longer change or centroids stabilize. 8.4.1 Why does k-Means works? We need to show that each iteration of the k-Means algorithm decreases the within-cluster sum of squares (WCSS), ensuring convergence. The k-Means algorithm alternates between two steps: Cluster Assignment Step: Assign each point to the closest centroid. Centroid Update Step: Recalculate the centroid as the mean of points in each cluster. We’ll prove that each of these steps decreases the WCSS. 8.4.1.1 Cluster Assignment Step Decreases WCSS Suppose we are in the middle of the k-Means algorithm with centroids \\(\\{ \\boldsymbol{\\mu}_1, \\dots, \\boldsymbol{\\mu}_k \\}\\). Each point \\(\\mathbf{x}_i\\) is assigned to the nearest centroid. For a point originally assigned to cluster \\(C_j\\), assume it is reassigned to \\(C_\\ell\\). Since the assignment rule is: \\[ \\|\\mathbf{x}_i - \\boldsymbol{\\mu}_\\ell \\|^2 \\leq \\|\\mathbf{x}_i - \\boldsymbol{\\mu}_j \\|^2 \\] the WCSS can only decrease or remain the same. ✅ Conclusion: The assignment step never increases WCSS. 8.4.1.2 Centroid Update Step Decreases WCSS Next, consider the effect of updating the centroids. Suppose the updated centroid for cluster \\(C_j\\) is: \\[ \\boldsymbol{\\mu}_j = \\frac{1}{|C_j|} \\sum_{\\mathbf{x}_i \\in C_j} \\mathbf{x}_i \\] We want to show that this new centroid minimizes: \\[ \\sum_{\\mathbf{x}_i \\in C_j} \\|\\mathbf{x}_i - \\boldsymbol{\\mu}_j\\|^2 \\] From properties of the sample mean, the mean is the point that minimizes the sum of squared distances to all points in the cluster: \\[ \\boldsymbol{\\mu}_j = \\arg\\min_{\\mathbf{y}} \\sum_{\\mathbf{x}_i \\in C_j} \\|\\mathbf{x}_i - \\mathbf{y} \\|^2 \\] ✅ Conclusion: The centroid update step strictly decreases WCSS unless the centroid is already optimal. 8.4.1.3 Combined Effect The Cluster Assignment Step decreases WCSS or leaves it unchanged. The Centroid Update Step strictly decreases WCSS unless centroids are optimal. Since WCSS is lower-bounded by zero, the algorithm must eventually converge. 8.4.1.4 Why Does k-Means Converge? Each iteration reduces WCSS. WCSS is bounded and non-negative. Therefore, by the monotone convergence theorem, the algorithm must eventually reach a local minimum. ❗️ Important: k-Means may converge to a local minimum, not necessarily the global minimum — this is why multiple initializations (like k-Means++) are recommended. 8.4.1.5 Summary ✅ Each iteration of k-Means reduces the WCSS. ✅ Convergence is guaranteed, though the final solution may only be locally optimal. ✅ Using improved initialization techniques like k-Means++ helps achieve better results. "],["expectation-maximization.html", "9 Expectation Maximization 9.1 Introduction 9.2 Model Framework 9.3 Algorithm 9.4 Proof of EM Convergence 9.5 Conclusion", " 9 Expectation Maximization 9.1 Introduction The Expectation-Maximization (EM) algorithm is a method for maximum likelihood estimation in models with latent variables or incomplete data. Rather than maximizing the log-likelihood directly, EM alternates between estimating the missing data and optimizing the parameters. Common applications include: Gaussian Mixture Models Hidden Markov Models Factor Analysis Handling missing values 9.2 Model Framework We have: \\( {\\boldsymbol x} \\): observed variables \\( {\\boldsymbol z} \\): unobserved variables \\( {\\boldsymbol \\theta} \\): parameters to be estimated We want to: \\[ \\max_ {\\boldsymbol \\theta} \\log p( {\\boldsymbol x} \\mid {\\boldsymbol \\theta} ) \\] Note that \\(\\log p( {\\boldsymbol x} \\mid {\\boldsymbol \\theta} )\\) is a function of \\( {\\boldsymbol \\theta} \\) given that data is observed. Sometimes we cannot maximize \\(\\log p(x \\mid \\theta)\\) directly. As a workaround, we proceed via EM. 9.3 Algorithm Initialize \\(\\boldsymbol{\\theta}^{(0)}\\) While EM alternates between \\(\\boldsymbol{\\theta}^{(t)}\\) and \\(\\boldsymbol{\\theta}^{(t+1)}\\), do: Expectation Step (E-step): Compute \\[ Q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}^{(t)}) = \\mathbb{E}_{\\mathbf{z} \\sim p(\\mathbf{z} \\mid \\mathbf{x}, \\boldsymbol{\\theta}^{(t)})} [ \\log p(\\mathbf{x}, \\mathbf{z} \\mid \\boldsymbol{\\theta}) ] \\] Maximization Step: Maximize \\[ \\boldsymbol{\\theta}^{(t+1)} = \\arg\\max_{\\boldsymbol{\\theta}} Q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}^{(t)}) \\] This algorithm produces a sequence \\(\\boldsymbol{\\theta}^{(0)}, \\boldsymbol{\\theta}^{(1)}, \\dots\\), which also produces a sequence: \\[ \\log p(\\mathbf{x} \\mid \\boldsymbol{\\theta}^{(0)}), \\log p(\\mathbf{x} \\mid \\boldsymbol{\\theta}^{(1)}), \\dots \\] Let us call: \\[ \\mathcal{L}^{(t)} = \\log p(\\mathbf{x} \\mid \\boldsymbol{\\theta}^{(t)}) \\] We will now show the EM algorithm produces a non-decreasing sequence \\(\\mathcal{L}^{(t)}\\). So we need to show that: \\[ \\mathcal{L}^{(t+1)} \\geq \\mathcal{L}^{(t)} \\quad \\text{at every iteration of the algorithm} \\] 9.4 Proof of EM Convergence Now note that: \\[\\begin{align*} p(\\mathbf{z} \\mid \\mathbf{x}, \\boldsymbol{\\theta}) &amp;= \\frac{p(\\mathbf{x}, \\mathbf{z} \\mid \\boldsymbol{\\theta})}{p(\\mathbf{x} \\mid \\boldsymbol{\\theta})} &amp; \\text{by def. of conditional probability} \\\\ \\Rightarrow p(\\mathbf{x} \\mid \\boldsymbol{\\theta}) &amp;= \\frac{p(\\mathbf{x}, \\mathbf{z} \\mid \\boldsymbol{\\theta})}{p(\\mathbf{z} \\mid \\mathbf{x}, \\boldsymbol{\\theta})} \\\\ \\Rightarrow \\log p(\\mathbf{x} \\mid \\boldsymbol{\\theta}) &amp;= \\log \\left( \\frac{p(\\mathbf{x}, \\mathbf{z} \\mid \\boldsymbol{\\theta})}{p(\\mathbf{z} \\mid \\mathbf{x}, \\boldsymbol{\\theta})} \\right) \\\\ \\Rightarrow \\mathcal{L}(\\boldsymbol{\\theta}) &amp;= \\log p(\\mathbf{x}, \\mathbf{z} \\mid \\boldsymbol{\\theta}) - \\log p(\\mathbf{z} \\mid \\mathbf{x}, \\boldsymbol{\\theta}) \\tag{1} \\end{align*}\\] Now take \\(\\mathbb{E}_{\\mathbf{z} \\sim p(\\mathbf{z} \\mid \\mathbf{x}, \\boldsymbol{\\theta}^{(t)})}\\) of both sides: \\[ \\mathcal{L}(\\boldsymbol{\\theta}) = \\mathbb{E}_t[\\log p(\\mathbf{x}, \\mathbf{z} \\mid \\boldsymbol{\\theta})] - \\mathbb{E}_t[\\log p(\\mathbf{z} \\mid \\mathbf{x}, \\boldsymbol{\\theta})] = Q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}^{(t)}) + H_t(\\boldsymbol{\\theta}) \\tag{2} \\] Since, \\(\\mathcal{L}(\\boldsymbol{\\theta})\\) doesn’t depend on \\( {\\boldsymbol z} \\), and we use \\(t\\) the sub-index of the expectation to indicate that we are taking expectation with respect of \\(p(\\mathbf{z} \\mid \\mathbf{x}, \\boldsymbol{\\theta}^{(t)})\\), and \\(H_t(\\boldsymbol{\\theta}) = - \\mathbb{E}_t[\\log p(\\mathbf{z} \\mid \\mathbf{x}, \\boldsymbol{\\theta})]\\). Now, let us show that \\(H_t(\\boldsymbol{\\theta}) \\geq H_t(\\boldsymbol{\\theta}^{(t)})\\), \\(\\forall \\boldsymbol{\\theta}\\) Note that: \\[\\begin{align*} H_t(\\boldsymbol{\\theta}) - H_t(\\boldsymbol{\\theta}^{(t)}) &amp;= - \\mathbb{E}_t[\\log p(\\mathbf{z} \\mid \\mathbf{x}, \\boldsymbol{\\theta})] + \\mathbb{E}_t[\\log p(\\mathbf{z} \\mid \\mathbf{x}, \\boldsymbol{\\theta}^{(t)})] \\\\ &amp;= \\mathbb{E}_t[\\log p(\\mathbf{z} \\mid \\mathbf{x}, \\boldsymbol{\\theta}^{(t)}) - \\log p(\\mathbf{z} \\mid \\mathbf{x}, \\boldsymbol{\\theta})] \\\\ &amp;= \\mathbb{E}_t \\left[ \\log \\left( \\frac{p(\\mathbf{z} \\mid \\mathbf{x}, \\boldsymbol{\\theta}^{(t)})}{p(\\mathbf{z} \\mid \\mathbf{x}, \\boldsymbol{\\theta})} \\right) \\right] \\\\ &amp;= D_{\\text{KL}}(p(\\mathbf{z} \\mid \\mathbf{x}, \\boldsymbol{\\theta}^{(t)}) \\,\\|\\, p(\\mathbf{z} \\mid \\mathbf{x}, \\boldsymbol{\\theta})) \\\\ &amp;\\geq 0 \\end{align*}\\] So: \\[ H_t(\\boldsymbol{\\theta}) \\geq H_t(\\boldsymbol{\\theta}^{(t)}) \\tag{4} \\] Then: \\[\\begin{align*} \\mathcal{L}(\\boldsymbol{\\theta}^{(t)}) &amp;= Q(\\boldsymbol{\\theta^{(t)}} \\mid \\boldsymbol{\\theta}^{(t)}) + H_t(\\boldsymbol{\\theta}) &amp; \\text{from (1)} \\\\ &amp;\\leq Q(\\boldsymbol{\\theta^{(t+1)}} \\mid \\boldsymbol{\\theta}^{(t)}) + H_t(\\boldsymbol{\\theta}) &amp; \\text{from the maximization step} \\\\ &amp;\\leq Q(\\boldsymbol{\\theta^{(t+1)}} \\mid \\boldsymbol{\\theta}^{(t+1)}) + H_t(\\boldsymbol{\\theta}) &amp; \\text{from (4)} \\\\ &amp;=\\mathcal{L}(\\boldsymbol{\\theta}^{(t+1)}) &amp; \\text{from (2)} \\\\ \\end{align*}\\] So: \\[ \\mathcal{L}(\\boldsymbol{\\theta}) \\text{ is a non-decreasing function} \\] Therefore, if \\(\\mathcal{L}(\\boldsymbol{\\theta})\\) has a maximum, \\(\\mathcal{L}^{(t)}\\) converges by the Monotone Convergence Theorem. 9.5 Conclusion EM produces a sequence \\(\\mathcal{L}(\\theta^{(t)})\\) that is non-decreasing The proof uses the decomposition into \\(Q\\), entropy, and KL divergence EM converges to a local maximum, but it is not guaranteed it will converge to a global maximum. Different starting points of EM have to be tried to compare performance. "],["gaussian-mixture-models.html", "10 Gaussian Mixture Models 10.1 Introduction 10.2 EM on GMM", " 10 Gaussian Mixture Models 10.1 Introduction Gaussian Mixture Models (GMMs) are probabilistic models used to represent data as a mixture of several Gaussian distributions. Each component of the mixture corresponds to a cluster or subpopulation within the overall data distribution. GMMs are widely used in clustering, density estimation, and as building blocks in more complex generative models. 10.1.1 Model Definition Suppose \\(\\mathbf{x}_1, \\dots, \\mathbf{x}_n \\in \\mathbb{R}^p\\) are observed data points. A GMM models the distribution of each \\(\\mathbf{x}_i\\) as arising from one of \\(K\\) components: \\[ p(\\mathbf{x}) = \\sum_{k=1}^{K} \\pi_k \\cdot \\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) \\] where: \\(\\pi_k \\in (0, 1)\\) are the mixing proportions, with \\(\\sum_{k=1}^{K} \\pi_k = 1\\) \\(\\boldsymbol{\\mu}_k \\in \\mathbb{R}^p\\) is the mean of the \\(k\\)-th Gaussian \\(\\boldsymbol{\\Sigma}_k \\in \\mathbb{R}^{p \\times p}\\) is the covariance matrix of the \\(k\\)-th Gaussian \\(\\mathcal{N}(\\mathbf{x} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) denotes the multivariate Gaussian density 10.1.2 Latent Variable Interpretation GMMs introduce an unobserved categorical variable \\(z_i \\in \\{1, \\dots, K\\}\\) indicating which component generated \\(\\mathbf{x}_i\\). The full generative process is: Sample component: \\[ z_i \\sim \\text{Categorical}(\\pi_1, \\dots, \\pi_K) \\] Given \\(z_i = k\\), sample data: \\[ \\mathbf{x}_i \\sim \\mathcal{N}(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) \\] The observed data likelihood becomes: \\[ p(\\mathbf{x}_i) = \\sum_{k=1}^{K} \\pi_k \\cdot \\mathcal{N}(\\mathbf{x}_i \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) \\] 10.1.3 Parameters to Estimate The unknown parameters are: Mixing weights: \\(\\pi_1, \\dots, \\pi_K\\) Component means: \\(\\boldsymbol{\\mu}_1, \\dots, \\boldsymbol{\\mu}_K\\) Covariances: \\(\\boldsymbol{\\Sigma}_1, \\dots, \\boldsymbol{\\Sigma}_K\\) These are typically estimated via the Expectation-Maximization (EM) algorithm. 10.1.4 Summary GMMs generalize k-means by allowing soft assignments and modeling cluster shape. Each data point has a probabilistic association to each cluster. Inference is typically performed via EM, which alternates between computing responsibilities and updating parameters. 10.2 EM on GMM 10.2.1 Original model \\[ p(\\mathbf{x}_i \\mid \\boldsymbol{\\theta}) = \\sum_{k=1}^{K} \\pi_k \\, \\mathcal{N}(\\mathbf{x}_i \\mid \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) \\] where \\(\\boldsymbol{\\theta} = \\{ \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k, \\pi_k \\}_{k=1}^K\\) Then the likelihood is: \\[ p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) = \\prod_{i=1}^{n} \\left( \\sum_{k=1}^{K} \\pi_k \\, \\mathcal{N}(\\mathbf{x}_i \\mid \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) \\right) \\] 10.2.2 New model \\[ \\mathbf{x}_i \\mid z_i = k \\sim \\mathcal{N}(\\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) \\] \\[ z_i \\mid \\boldsymbol{\\theta} \\sim \\text{Categorical}(\\pi_1, \\dots, \\pi_K) \\] Both, independent across observations. 10.2.3 Model Equivalency Now let us check that the marginal distribution of the new model is the same as the original model. Note that: \\[\\begin{align*} p(\\mathbf{x}_i, z_i \\mid \\boldsymbol{\\theta}) &amp;= p(\\mathbf{x}_i, \\mid z_i \\boldsymbol{\\theta}) \\\\ &amp;= \\prod_{k=1}^{K} \\left[ \\pi_k \\mathcal{N}(\\mathbf{x}_i \\mid \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) \\right]^{\\mathbb{I}[z_i = k]} \\\\ \\end{align*}\\] Then: \\[ p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) = \\prod_{i=1}^{n} \\prod_{k=1}^{K} \\left[ \\pi_k \\, \\mathcal{N}(\\mathbf{x}_i \\mid \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) \\right]^{\\mathbb{I}[z_i = k]} \\] Then: \\[\\begin{align*} p(\\mathbf{X} \\mid \\boldsymbol{\\theta}) &amp;= \\sum_{ {\\boldsymbol Z} } p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) \\\\ &amp;= \\sum_{z_1} \\ldots \\sum_{z_n} \\prod_{i=1}^{n} \\prod_{k=1}^{K} \\left[ \\pi_k \\, \\mathcal{N}(\\mathbf{x}_i \\mid \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) \\right]^{\\mathbb{I}[z_i = k]} \\\\ &amp;= \\sum_{z_1} \\ldots \\sum_{z_{n-1}} \\prod_{i=1}^{n-1} \\prod_{k=1}^{K} \\left[ \\pi_k \\, \\mathcal{N}(\\mathbf{x}_i \\mid \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) \\right]^{\\mathbb{I}[z_i = k]} \\sum_{z_{n}} \\prod_{k=1}^{K} \\left[ \\pi_k \\, \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) \\right]^{\\mathbb{I}[z_n = k]} \\\\ &amp;= \\sum_{z_1} \\ldots \\sum_{z_{n-1}} \\prod_{i=1}^{n-1} \\prod_{k=1}^{K} \\left[ \\pi_k \\, \\mathcal{N}(\\mathbf{x}_i \\mid \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) \\right]^{\\mathbb{I}[z_i = k]} \\left( \\sum_{k=1}^{K} \\left[ \\pi_k \\, \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) \\right]^{\\mathbb{I}[z_n = k]} \\right) \\\\ &amp;= \\prod_{i=1}^{n} \\left( \\sum_{k=1}^{K} \\pi_k \\, \\mathcal{N}(\\mathbf{x}_i \\mid \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) \\right) \\end{align*}\\] So the marginal distribution of the new model is the same as the original model. 10.2.4 Expectation Computation First we compute \\(p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta})\\), the distribution we use to compute the expectation. \\[\\begin{align*} p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}) &amp;= \\frac{p(\\mathbf{Z}, \\mathbf{X} \\mid \\boldsymbol{\\theta})}{p(\\mathbf{X} \\mid \\boldsymbol{\\theta})} \\\\ &amp;= \\frac{p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}) p(\\mathbf{Z} \\mid \\boldsymbol{\\theta})}{p(\\mathbf{X} \\mid \\boldsymbol{\\theta})} \\\\ &amp;= \\frac{ \\prod_{i=1}^n p(z_i \\mid {\\boldsymbol x} _i, \\boldsymbol{\\theta}) \\prod_{i=1}^n p(z_i \\mid \\boldsymbol{\\theta})}{\\prod_{i=1}^n p( {\\boldsymbol x} _i \\mid \\boldsymbol{\\theta})} \\\\ &amp;= \\prod_{i=1}^n \\frac{ p(z_i \\mid {\\boldsymbol x} _i, \\boldsymbol{\\theta}) p(z_i \\mid \\boldsymbol{\\theta})}{ p( {\\boldsymbol x} _i \\mid \\boldsymbol{\\theta})} \\\\ &amp;= \\prod_{i=1}^n p( {\\boldsymbol x} _i \\mid z_i, \\boldsymbol{\\theta}) \\end{align*}\\] Now: \\[\\begin{align*} p(z_i = k \\mid \\mathbf{x}_i, \\boldsymbol{\\theta}) &amp;= \\frac{p(\\mathbf{x}_i, z_i = k \\mid \\boldsymbol{\\theta})}{p(\\mathbf{x}_i \\mid \\boldsymbol{\\theta})} \\\\ &amp;= \\frac{p(\\mathbf{x}_i \\mid z_i = k, \\boldsymbol{\\theta}) p(z_i = k \\mid \\boldsymbol{\\theta})}{\\sum_{l=1}^{K} \\pi_l \\, \\mathcal{N}(\\mathbf{x}_i \\mid \\boldsymbol{\\mu}_l, \\mathbf{\\Sigma}_l)} \\\\ &amp;= \\frac{\\pi_k \\, \\mathcal{N}(\\mathbf{x}_i \\mid \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k)}{\\sum_{l=1}^{K} \\pi_l \\, \\mathcal{N}(\\mathbf{x}_i \\mid \\boldsymbol{\\mu}_l, \\mathbf{\\Sigma}_l)} \\\\ &amp;= \\gamma_{ik} \\end{align*}\\] Finally, let us compute: \\[ \\mathbb{E}_{\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{(t)}}[\\log p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})] \\] First, note that: \\[\\begin{align*} \\log p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta}) &amp;= \\log \\left( \\prod_{i=1}^{n} \\prod_{k=1}^{K} \\left[ \\pi_k \\, \\mathcal{N}(\\mathbf{x}_i \\mid \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) \\right]^{\\mathbb{I}[z_i = k]} \\right) \\\\ &amp;= \\sum_{i=1}^{n} \\sum_{k=1}^{K} \\mathbb{I}[z_i = k] \\log \\left( \\pi_k \\, \\mathcal{N}(\\mathbf{x}_i \\mid \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) \\right) \\\\ &amp;= w^{n} \\end{align*}\\] And note that \\(w^n\\) can be decomposed as follows: \\[ w^n = w^{n-1} + \\sum_{k=1}^{K} \\mathbb{I}[z_n = k] \\log \\left( \\pi_k \\, \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) \\right) \\] Then: \\[\\begin{align*} \\mathbb{E}_{\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{(t)}} &amp;[\\log p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\theta})] = \\mathbb{E}_{\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}^{(t)}}[w^n] \\\\ &amp;= \\sum_{ {\\boldsymbol Z} } w^n p(\\mathbf{Z} \\mid \\mathbf{X}, \\boldsymbol{\\theta}) \\\\ &amp;= \\sum_{z_1} \\ldots \\sum_{z_n} \\left( w^{n-1} + \\sum_{k=1}^{K} \\mathbb{I}[z_n = k] \\log \\left( \\pi_k \\, \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) \\right) \\right) \\prod_{i=1}^n p(z_i \\mid {\\boldsymbol x} _i, \\boldsymbol{\\theta}) \\\\ &amp;= \\sum_{z_1} \\ldots \\sum_{z_{n-1}} w^{n-1} \\prod_{i=1}^{n-1} p(z_i \\mid {\\boldsymbol x} _i, \\boldsymbol{\\theta}) \\\\ &amp;\\quad + \\sum_{z_1} \\ldots \\sum_{z_{n-1}} \\prod_{i=1}^{n-1} p(z_i \\mid {\\boldsymbol x} _i, \\boldsymbol{\\theta}) \\sum_{z_{n}} \\sum_{k=1}^{K} \\mathbb{I}[z_n = k] \\log \\left( \\pi_k \\, \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) \\right) p(z_n \\mid {\\boldsymbol x} _n, \\boldsymbol{\\theta}) \\\\ &amp;= \\sum_{z_1} \\ldots \\sum_{z_{n-1}} w^{n-1} \\prod_{i=1}^{n-1} p(z_i \\mid {\\boldsymbol x} _i, \\boldsymbol{\\theta}) \\\\ &amp;\\quad + \\sum_{z_1} \\ldots \\sum_{z_{n-1}} \\prod_{i=1}^{n-1} p(z_i \\mid {\\boldsymbol x} _i, \\boldsymbol{\\theta}) \\sum_{k=1}^{K} \\log \\left( \\pi_k \\, \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) \\right) \\gamma_{n k} \\\\ &amp;= \\sum_{z_1} \\ldots \\sum_{z_{n-1}} w^{n-1} \\prod_{i=1}^{n-1} p(z_i \\mid {\\boldsymbol x} _i, \\boldsymbol{\\theta}) \\\\ &amp;\\quad + \\sum_{k=1}^{K} \\log \\left( \\pi_k \\, \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) \\right) \\gamma_{n k} \\sum_{z_1} \\ldots \\sum_{z_{n-1}} \\prod_{i=1}^{n-1} p(z_i \\mid {\\boldsymbol x} _i, \\boldsymbol{\\theta}) \\\\ &amp;= \\sum_{z_1} \\ldots \\sum_{z_{n-1}} w^{n-1} \\prod_{i=1}^{n-1} p(z_i \\mid {\\boldsymbol x} _i, \\boldsymbol{\\theta}) + \\sum_{k=1}^{K} \\log \\left( \\pi_k \\, \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) \\right) \\gamma_{n k} \\\\ &amp;= \\sum_{i=1}^n \\sum_{k=1}^{K} \\log \\left( \\pi_k \\, \\mathcal{N}(\\mathbf{x}_i \\mid \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) \\right) \\gamma_{i k} \\\\ \\end{align*}\\] Then: \\[\\begin{align*} Q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}^{(t)}) &amp;= \\sum_{i=1}^{n} \\sum_{k=1}^{K} \\gamma_{ik} \\log \\left( \\pi_k \\, \\mathcal{N}(\\mathbf{x}_i \\mid \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) \\right) \\\\ &amp;= \\sum_{i=1}^{n} \\sum_{k=1}^{K} \\gamma_{ik} \\left[ \\log \\pi_k + \\log \\mathcal{N}(\\mathbf{x}_i \\mid \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) \\right] \\end{align*}\\] 10.2.5 Maximization Step We need to maximize: \\[ \\max_{ {\\boldsymbol \\pi} , {\\boldsymbol \\mu} , {\\boldsymbol \\Sigma} } Q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}^{(t)}) \\\\ \\text{s.t.} \\sum_{k=1}^{K} \\pi_k = 1 \\] then, the Lagrange multiplier is: \\[ \\mathcal{L} = \\sum_{i=1}^{n} \\sum_{k=1}^{K} \\gamma_{ik} \\left[ \\log \\pi_k + \\log \\mathcal{N}(\\mathbf{x}_i \\mid \\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) \\right] + \\lambda \\left( \\sum_{k=1}^{K} \\pi_k - 1 \\right) \\] 10.2.5.1 Update \\(\\pi_k\\) \\[\\begin{align*} &amp; \\frac{\\partial \\mathcal{L}}{\\partial \\pi_k} = \\sum_{i=1}^{n} \\frac{\\gamma_{ik}}{\\pi_k} + \\lambda = 0 \\\\ \\Rightarrow &amp; \\pi_k = - \\frac{1}{\\lambda} \\sum_{i=1}^{n} \\gamma_{ik} \\\\ \\Rightarrow &amp; \\sum_{k=1}^K \\pi_k = - \\frac{1}{\\lambda} \\sum_{k=1}^K \\sum_{i=1}^{n} \\gamma_{ik} \\\\ \\Rightarrow &amp; 1 = - \\frac{1}{\\lambda} n \\\\ \\Rightarrow &amp; \\lambda = -n \\\\ \\end{align*}\\] Then: \\[ \\pi_k = \\frac{1}{n} \\sum_{i=1}^{n} \\gamma_{ik} \\] 10.2.5.2 Update \\(\\boldsymbol{\\mu}_k\\) \\[\\begin{align*} \\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\mu}_k} &amp;= -\\sum_{i=1}^{n} \\gamma_{ik} \\mathbf{\\Sigma}_k^{-1} (\\mathbf{x}_i - \\boldsymbol{\\mu}_k) = 0 \\\\ &amp;\\Rightarrow \\sum_{i=1}^{n} \\gamma_{ik} \\mathbf{\\Sigma}_k^{-1} (\\mathbf{x}_i - \\boldsymbol{\\mu}_k) = 0 \\\\ &amp;\\Rightarrow \\sum_{i=1}^{n} \\gamma_{ik} \\mathbf{x}_i = \\sum_{i=1}^{n} \\gamma_{ik} \\boldsymbol{\\mu}_k \\\\ &amp;\\Rightarrow \\boldsymbol{\\mu}_k = \\frac{\\sum_{i=1}^{n} \\gamma_{ik} \\mathbf{x}_i}{\\sum_{i=1}^{n} \\gamma_{ik}} \\end{align*}\\] 10.2.5.3 Update \\(\\mathbf{\\Sigma}_k\\) \\[\\begin{align*} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{\\Sigma}_k} &amp;= \\frac{\\partial}{\\partial \\mathbf{\\Sigma}_k} \\sum_{i=1}^{n} \\gamma_{ik} \\left( -\\frac{1}{2} \\log |\\mathbf{\\Sigma}_k| - \\frac{1}{2} (\\mathbf{x}_i - \\boldsymbol{\\mu}_k)^\\top \\mathbf{\\Sigma}_k^{-1} (\\mathbf{x}_i - \\boldsymbol{\\mu}_k) \\right) \\\\ &amp;= \\sum_{i=1}^{n} \\gamma_{ik} \\left( -\\frac{1}{2} \\mathbf{\\Sigma}_k^{-1} + \\frac{1}{2} \\mathbf{\\Sigma}_k^{-1} (\\mathbf{x}_i - \\boldsymbol{\\mu}_k) (\\mathbf{x}_i - \\boldsymbol{\\mu}_k)&#39; \\mathbf{\\Sigma}_k^{-1} \\right) \\\\ \\end{align*}\\] Then \\[\\begin{align*} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{\\Sigma}_k} &amp;= 0 \\\\ &amp;\\Rightarrow \\sum_{i=1}^{n} \\gamma_{ik} \\left( \\Sigma_k - (\\mathbf{x}_i - \\boldsymbol{\\mu}_k) (\\mathbf{x}_i - \\boldsymbol{\\mu}_k)&#39; \\right) = 0 \\\\ &amp;\\Rightarrow \\sum_{i=1}^{n} \\gamma_{ik} \\Sigma_k = \\sum_{i=1}^{n} \\gamma_{ik} (\\mathbf{x}_i - \\boldsymbol{\\mu}_k) (\\mathbf{x}_i - \\boldsymbol{\\mu}_k)&#39; \\\\ &amp;\\Rightarrow \\Sigma_k \\sum_{i=1}^{n} \\gamma_{ik} = \\sum_{i=1}^{n} \\gamma_{ik} (\\mathbf{x}_i - \\boldsymbol{\\mu}_k) (\\mathbf{x}_i - \\boldsymbol{\\mu}_k)&#39; \\\\ &amp;\\Rightarrow \\Sigma_k = \\frac{\\sum_{i=1}^{n} \\gamma_{ik} (\\mathbf{x}_i - \\boldsymbol{\\mu}_k) (\\mathbf{x}_i - \\boldsymbol{\\mu}_k)&#39;}{\\sum_{i=1}^{n} \\gamma_{ik}} \\\\ \\end{align*}\\] "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
