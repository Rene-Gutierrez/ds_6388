<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Linear Regression | DS 6388 Spring 2025</title>
  <meta name="description" content="4 Linear Regression | DS 6388 Spring 2025" />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Linear Regression | DS 6388 Spring 2025" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Linear Regression | DS 6388 Spring 2025" />
  
  
  

<meta name="author" content="Rene Gutierrez" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>
<link rel="next" href="principal-component-analysis.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> DS 6388: Multivariate Statistical Methods for High-dimensional Data</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#calendar"><i class="fa fa-check"></i><b>1.1</b> Calendar</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#importatnt-dates"><i class="fa fa-check"></i><b>1.1.1</b> Importatnt Dates</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#class-schedule"><i class="fa fa-check"></i><b>1.1.2</b> Class Schedule</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>2</b> Prerequisites</a>
<ul>
<li class="chapter" data-level="2.1" data-path="prerequisites.html"><a href="prerequisites.html#linear-algebra"><i class="fa fa-check"></i><b>2.1</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="prerequisites.html"><a href="prerequisites.html#linear-independence"><i class="fa fa-check"></i><b>2.1.1</b> Linear Independence</a></li>
<li class="chapter" data-level="2.1.2" data-path="prerequisites.html"><a href="prerequisites.html#column-space-of-a-matrix"><i class="fa fa-check"></i><b>2.1.2</b> Column Space of a Matrix</a></li>
<li class="chapter" data-level="2.1.3" data-path="prerequisites.html"><a href="prerequisites.html#rank-of-a-matrix"><i class="fa fa-check"></i><b>2.1.3</b> Rank of a Matrix</a></li>
<li class="chapter" data-level="2.1.4" data-path="prerequisites.html"><a href="prerequisites.html#full-rank-matrix"><i class="fa fa-check"></i><b>2.1.4</b> Full Rank Matrix</a></li>
<li class="chapter" data-level="2.1.5" data-path="prerequisites.html"><a href="prerequisites.html#inverse-matrix"><i class="fa fa-check"></i><b>2.1.5</b> Inverse Matrix</a></li>
<li class="chapter" data-level="2.1.6" data-path="prerequisites.html"><a href="prerequisites.html#positive-definite-matrix"><i class="fa fa-check"></i><b>2.1.6</b> Positive Definite Matrix</a></li>
<li class="chapter" data-level="2.1.7" data-path="prerequisites.html"><a href="prerequisites.html#singular-value-decomposition"><i class="fa fa-check"></i><b>2.1.7</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="2.1.8" data-path="prerequisites.html"><a href="prerequisites.html#eigendecomposition"><i class="fa fa-check"></i><b>2.1.8</b> Eigendecomposition</a></li>
<li class="chapter" data-level="2.1.9" data-path="prerequisites.html"><a href="prerequisites.html#idempotent-matrix"><i class="fa fa-check"></i><b>2.1.9</b> Idempotent Matrix</a></li>
<li class="chapter" data-level="2.1.10" data-path="prerequisites.html"><a href="prerequisites.html#determinant-of-a-matrix"><i class="fa fa-check"></i><b>2.1.10</b> Determinant of a Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="prerequisites.html"><a href="prerequisites.html#calculus"><i class="fa fa-check"></i><b>2.2</b> Calculus</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="prerequisites.html"><a href="prerequisites.html#gradient"><i class="fa fa-check"></i><b>2.2.1</b> Gradient</a></li>
<li class="chapter" data-level="2.2.2" data-path="prerequisites.html"><a href="prerequisites.html#hessian-matrix"><i class="fa fa-check"></i><b>2.2.2</b> Hessian Matrix</a></li>
<li class="chapter" data-level="2.2.3" data-path="prerequisites.html"><a href="prerequisites.html#applications-1"><i class="fa fa-check"></i><b>2.2.3</b> Applications:</a></li>
<li class="chapter" data-level="2.2.4" data-path="prerequisites.html"><a href="prerequisites.html#matrix-calculus"><i class="fa fa-check"></i><b>2.2.4</b> Matrix Calculus</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="prerequisites.html"><a href="prerequisites.html#probability"><i class="fa fa-check"></i><b>2.3</b> Probability</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="prerequisites.html"><a href="prerequisites.html#expected-value"><i class="fa fa-check"></i><b>2.3.1</b> Expected Value</a></li>
<li class="chapter" data-level="2.3.2" data-path="prerequisites.html"><a href="prerequisites.html#variance"><i class="fa fa-check"></i><b>2.3.2</b> Variance</a></li>
<li class="chapter" data-level="2.3.3" data-path="prerequisites.html"><a href="prerequisites.html#cross-covariance-matrix"><i class="fa fa-check"></i><b>2.3.3</b> Cross-Covariance Matrix</a></li>
<li class="chapter" data-level="2.3.4" data-path="prerequisites.html"><a href="prerequisites.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>2.3.4</b> Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="2.3.5" data-path="prerequisites.html"><a href="prerequisites.html#chi2-distribution"><i class="fa fa-check"></i><b>2.3.5</b> <span class="math inline">\(\chi^2\)</span> Distribution</a></li>
<li class="chapter" data-level="2.3.6" data-path="prerequisites.html"><a href="prerequisites.html#t-distribution"><i class="fa fa-check"></i><b>2.3.6</b> <span class="math inline">\(t\)</span> Distribution</a></li>
<li class="chapter" data-level="2.3.7" data-path="prerequisites.html"><a href="prerequisites.html#f-distribution"><i class="fa fa-check"></i><b>2.3.7</b> <span class="math inline">\(F\)</span> Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="prerequisites.html"><a href="prerequisites.html#statistics"><i class="fa fa-check"></i><b>2.4</b> Statistics</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="prerequisites.html"><a href="prerequisites.html#bias-of-an-estimator"><i class="fa fa-check"></i><b>2.4.1</b> Bias of an Estimator</a></li>
<li class="chapter" data-level="2.4.2" data-path="prerequisites.html"><a href="prerequisites.html#unbiased-estimator"><i class="fa fa-check"></i><b>2.4.2</b> Unbiased Estimator</a></li>
<li class="chapter" data-level="2.4.3" data-path="prerequisites.html"><a href="prerequisites.html#mean-square-error-of-an-estimator"><i class="fa fa-check"></i><b>2.4.3</b> Mean Square Error of an Estimator</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>3</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introduction.html"><a href="introduction.html#key-challenges"><i class="fa fa-check"></i><b>3.1</b> Key Challenges</a></li>
<li class="chapter" data-level="3.2" data-path="introduction.html"><a href="introduction.html#core-concepts"><i class="fa fa-check"></i><b>3.2</b> Core Concepts</a></li>
<li class="chapter" data-level="3.3" data-path="introduction.html"><a href="introduction.html#applications-4"><i class="fa fa-check"></i><b>3.3</b> Applications</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>4</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="linear-regression.html"><a href="linear-regression.html#machine-learning"><i class="fa fa-check"></i><b>4.1</b> Machine Learning</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="linear-regression.html"><a href="linear-regression.html#ordinary-least-squares"><i class="fa fa-check"></i><b>4.1.1</b> Ordinary Least Squares</a></li>
<li class="chapter" data-level="4.1.2" data-path="linear-regression.html"><a href="linear-regression.html#ridge-regression"><i class="fa fa-check"></i><b>4.1.2</b> Ridge Regression</a></li>
<li class="chapter" data-level="4.1.3" data-path="linear-regression.html"><a href="linear-regression.html#lasso-regression"><i class="fa fa-check"></i><b>4.1.3</b> Lasso Regression</a></li>
<li class="chapter" data-level="4.1.4" data-path="linear-regression.html"><a href="linear-regression.html#elastic-net"><i class="fa fa-check"></i><b>4.1.4</b> Elastic Net</a></li>
<li class="chapter" data-level="4.1.5" data-path="linear-regression.html"><a href="linear-regression.html#elastic-net-as-a-mixed-penalty"><i class="fa fa-check"></i><b>4.1.5</b> <strong>Elastic Net as a Mixed Penalty</strong></a></li>
<li class="chapter" data-level="4.1.6" data-path="linear-regression.html"><a href="linear-regression.html#other-options-of-regularization"><i class="fa fa-check"></i><b>4.1.6</b> Other Options of Regularization</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-linear-regression"><i class="fa fa-check"></i><b>4.2</b> Bayesian Linear Regression</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="linear-regression.html"><a href="linear-regression.html#basic-bayesian-linear-regression"><i class="fa fa-check"></i><b>4.2.1</b> Basic Bayesian Linear Regression</a></li>
<li class="chapter" data-level="4.2.2" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-lasso-regression"><i class="fa fa-check"></i><b>4.2.2</b> Bayesian Lasso Regression</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="linear-regression.html"><a href="linear-regression.html#computational-comparisson"><i class="fa fa-check"></i><b>4.3</b> Computational Comparisson</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="linear-regression.html"><a href="linear-regression.html#set-up"><i class="fa fa-check"></i><b>4.3.1</b> Set-Up</a></li>
<li class="chapter" data-level="4.3.2" data-path="linear-regression.html"><a href="linear-regression.html#simulation"><i class="fa fa-check"></i><b>4.3.2</b> Simulation</a></li>
<li class="chapter" data-level="4.3.3" data-path="linear-regression.html"><a href="linear-regression.html#ols"><i class="fa fa-check"></i><b>4.3.3</b> OLS</a></li>
<li class="chapter" data-level="4.3.4" data-path="linear-regression.html"><a href="linear-regression.html#ridge-regression-1"><i class="fa fa-check"></i><b>4.3.4</b> Ridge Regression</a></li>
<li class="chapter" data-level="4.3.5" data-path="linear-regression.html"><a href="linear-regression.html#lasso"><i class="fa fa-check"></i><b>4.3.5</b> Lasso</a></li>
<li class="chapter" data-level="4.3.6" data-path="linear-regression.html"><a href="linear-regression.html#basic-bayesian-regression"><i class="fa fa-check"></i><b>4.3.6</b> Basic Bayesian Regression</a></li>
<li class="chapter" data-level="4.3.7" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-lasso"><i class="fa fa-check"></i><b>4.3.7</b> Bayesian Lasso</a></li>
<li class="chapter" data-level="4.3.8" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-horseshoe-prior"><i class="fa fa-check"></i><b>4.3.8</b> Bayesian Horseshoe Prior</a></li>
<li class="chapter" data-level="4.3.9" data-path="linear-regression.html"><a href="linear-regression.html#results-comparisson"><i class="fa fa-check"></i><b>4.3.9</b> Results Comparisson</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="linear-regression.html"><a href="linear-regression.html#efficient-computation"><i class="fa fa-check"></i><b>4.4</b> Efficient Computation</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="linear-regression.html"><a href="linear-regression.html#efficient-computation-of-ridge-regression-using-the-woodbury-identity"><i class="fa fa-check"></i><b>4.4.1</b> Efficient Computation of Ridge Regression using the Woodbury Identity</a></li>
<li class="chapter" data-level="4.4.2" data-path="linear-regression.html"><a href="linear-regression.html#efficient-bayesian-sampling-for-gaussian-scale-mixture-priors"><i class="fa fa-check"></i><b>4.4.2</b> Efficient Bayesian Sampling for Gaussian Scale-Mixture Priors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>5</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="5.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#pca-as-a-low-rank-approximation-of-mathbfx"><i class="fa fa-check"></i><b>5.1</b> PCA as a Low-Rank Approximation of <span class="math inline">\(\mathbf{X}\)</span></a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#why-low-rank-approximation-matters"><i class="fa fa-check"></i><b>5.1.1</b> Why Low-Rank Approximation Matters</a></li>
<li class="chapter" data-level="5.1.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#the-singular-value-solution-is-the-best-low-rank-approximation-eckartyoungmirsky-theorem"><i class="fa fa-check"></i><b>5.1.2</b> The Singular Value Solution is the Best Low-Rank Approximation (Eckart–Young–Mirsky Theorem)</a></li>
<li class="chapter" data-level="5.1.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#eckartyoungmirsky-theorem-for-the-spectral-norm"><i class="fa fa-check"></i><b>5.1.3</b> Eckart–Young–Mirsky Theorem for the Spectral Norm</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#variance-maximization-in-pca"><i class="fa fa-check"></i><b>5.2</b> Variance Maximization in PCA</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#first-principal-cmponent"><i class="fa fa-check"></i><b>5.2.1</b> First Principal Cmponent</a></li>
<li class="chapter" data-level="5.2.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#second-principal-component"><i class="fa fa-check"></i><b>5.2.2</b> Second Principal Component</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">DS 6388 Spring 2025</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">4</span> Linear Regression<a href="linear-regression.html#linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="machine-learning" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Machine Learning<a href="linear-regression.html#machine-learning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, we will not make any probability assumption, and we will treat
the problem only as an optimization problem.</p>
<hr />
<div id="ordinary-least-squares" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Ordinary Least Squares<a href="linear-regression.html#ordinary-least-squares" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="model-specification" class="section level4 hasAnchor" number="4.1.1.1">
<h4><span class="header-section-number">4.1.1.1</span> Model Specification<a href="linear-regression.html#model-specification" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let <span class="math inline">\(y_i \in \mathbb{R}\)</span> be the response variable and <span class="math inline">\(x_i \in \mathbb{R}^p\)</span> be the vector of predictors for observation <span class="math inline">\(i\)</span>, where <span class="math inline">\(i = 1, \dots, n\)</span>. The multiple linear regression model is given by:</p>
<p><span class="math display">\[
{\boldsymbol y} =  {\boldsymbol X}  {\boldsymbol \beta} +  {\boldsymbol e}
\]</span></p>
<p>where:<br />
- <span class="math inline">\( {\boldsymbol y} \in \mathbb{R}^{n}\)</span> is the <strong>response vector</strong> (each entry corresponds to an observation),<br />
- <span class="math inline">\( {\boldsymbol X} \in \mathbb{R}^{n \times p}\)</span> is the <strong>design matrix</strong> (including predictors, typically with an intercept column of ones),<br />
- <span class="math inline">\( {\boldsymbol \beta} \in \mathbb{R}^{p}\)</span> is the <strong>coefficient vector</strong> to be estimated,<br />
- <span class="math inline">\( {\boldsymbol e} \in \mathbb{R}^{n}\)</span> is the <strong>error vector</strong>.</p>
<hr />
</div>
<div id="minimization-problem" class="section level4 hasAnchor" number="4.1.1.2">
<h4><span class="header-section-number">4.1.1.2</span> Minimization Problem<a href="linear-regression.html#minimization-problem" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The objective is to minimize the sum of squared errors (SSE):</p>
<p><span class="math display">\[
\min_{ {\boldsymbol \beta} } \mathcal{L}( {\boldsymbol \beta} ) = \min_{ {\boldsymbol \beta} } \|  {\boldsymbol y} -  {\boldsymbol X}  {\boldsymbol \beta} \|_2^2.
\]</span></p>
<p>Expanding the loss function:</p>
<p><span class="math display">\[
\mathcal{L}( {\boldsymbol \beta} ) = ( {\boldsymbol y} -  {\boldsymbol X}  {\boldsymbol \beta} )&#39; ( {\boldsymbol y} -  {\boldsymbol X}  {\boldsymbol \beta} ).
\]</span></p>
<hr />
</div>
<div id="solution" class="section level4 hasAnchor" number="4.1.1.3">
<h4><span class="header-section-number">4.1.1.3</span> Solution<a href="linear-regression.html#solution" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To minimize <span class="math inline">\(\mathcal{L}(\beta)\)</span>, we take the derivative with respect to <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[
\frac{\partial \mathcal{L}( {\boldsymbol \beta} )}{\partial  {\boldsymbol \beta} } = -2  {\boldsymbol X} &#39; ( {\boldsymbol y} -  {\boldsymbol X}  {\boldsymbol \beta} ).
\]</span></p>
<p>Setting this to zero, we obtain the <strong>normal equation</strong>:</p>
<p><span class="math display">\[
{\boldsymbol X} &#39;  {\boldsymbol X}  {\boldsymbol \beta} =  {\boldsymbol X} &#39;  {\boldsymbol y} .
\]</span></p>
<p>If <span class="math inline">\( {\boldsymbol X} &#39;  {\boldsymbol X} \)</span> is invertible (i.e., <span class="math inline">\( {\boldsymbol X} \)</span> has full column rank), we solve for <span class="math inline">\( {\boldsymbol \beta} \)</span>:</p>
<p><span class="math display">\[
{\boldsymbol \beta} = ( {\boldsymbol X} &#39;  {\boldsymbol X} )^{-1}  {\boldsymbol X} &#39;  {\boldsymbol y} .
\]</span></p>
<p>To check that this is a minimizer, we compute the Hessian of <span class="math inline">\(\mathcal{L}( {\boldsymbol \beta} )\)</span>:</p>
<p><span class="math display">\[
H = \frac{\partial^2 \mathcal{L}( {\boldsymbol \beta} )}{\partial  {\boldsymbol \beta} \partial  {\boldsymbol \beta} &#39;} = 2  {\boldsymbol X} &#39;  {\boldsymbol X} .
\]</span></p>
<p>Since <span class="math inline">\( {\boldsymbol X} &#39;  {\boldsymbol X} \)</span> is positive semidefinite and positive definite if <span class="math inline">\( {\boldsymbol X} \)</span> has full column rank, the function <span class="math inline">\(\mathcal{L}( {\boldsymbol \beta} )\)</span> is convex. Hence, the critical point <span class="math inline">\( {\boldsymbol \beta} = ( {\boldsymbol X} &#39;  {\boldsymbol X} )^{-1}  {\boldsymbol X} &#39;  {\boldsymbol y} \)</span> is the unique global minimum.</p>
<p>The OLS solution is often denoted as:</p>
<p><span class="math display">\[
\boldsymbol{\beta}_{\text{OLS}} = (\mathbf{X}&#39;\mathbf{X})^{-1} \mathbf{X}&#39;\mathbf{y}.
\]</span></p>
<p>Notice that we noted that <span class="math inline">\( {\boldsymbol X} \)</span> is full column rank, when this condition is not
met (or is close to not be met), other approaches are necessary.</p>
<hr />
</div>
</div>
<div id="ridge-regression" class="section level3 hasAnchor" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Ridge Regression<a href="linear-regression.html#ridge-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="introduction-1" class="section level4 hasAnchor" number="4.1.2.1">
<h4><span class="header-section-number">4.1.2.1</span> Introduction<a href="linear-regression.html#introduction-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>When the design matrix <span class="math inline">\(\mathbf{X}\)</span> is not full rank, the matrix <span class="math inline">\(\mathbf{X}&#39;\mathbf{X}\)</span> is singular (i.e., not invertible), making it impossible to compute the least squares solution</p>
<p><span class="math display">\[
\boldsymbol{\beta}_{\text{OLS}} = (\mathbf{X}&#39;\mathbf{X})^{-1} \mathbf{X}&#39;\mathbf{y}.
\]</span></p>
<p>This issue arises when there are <strong>more predictors than observations</strong> (<span class="math inline">\(p &gt; n\)</span>) or when there is <strong>multicollinearity</strong> among the predictors.</p>
<p>To address this, <strong>Ridge Regression</strong> introduces a small positive <strong>penalty term</strong> that <strong>regularizes</strong> the matrix <span class="math inline">\(\mathbf{X}&#39;\mathbf{X}\)</span>, making it invertible. This method is also known as <strong>Tikhonov regularization</strong> or <strong><span class="math inline">\(L_2\)</span> regularization</strong>.</p>
<p>When <span class="math inline">\(p &gt; n\)</span> then we can approximate <span class="math inline">\(\mathbf{X}&#39;\mathbf{X}\)</span> with <span class="math inline">\(\mathbf{X}&#39; \mathbf{X} + \lambda \mathbf{I}\)</span>, where <span class="math inline">\(\lambda &gt; 0\)</span> can be as small as necessary. This approximation is helpful since <span class="math inline">\(\mathbf{X}&#39; \mathbf{X} + \lambda \mathbf{I}\)</span> is always non-singular.</p>
<hr />
</div>
<div id="non-singularity-of-the-approximation" class="section level4 hasAnchor" number="4.1.2.2">
<h4><span class="header-section-number">4.1.2.2</span> Non-singularity of the Approximation<a href="linear-regression.html#non-singularity-of-the-approximation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let <span class="math inline">\(\mathbf{X}\)</span> be an <span class="math inline">\(n \times p\)</span> matrix. Its SVD decomposition is</p>
<p><span class="math display">\[
\mathbf{X} = \mathbf{U} \mathbf{D} \mathbf{V}&#39;
\]</span></p>
<p>where:<br />
- <span class="math inline">\(\mathbf{U} \in \mathbb{R}^{n \times n}\)</span> is an <strong>orthonormal matrix</strong> (<span class="math inline">\(\mathbf{U}&#39; \mathbf{U} = \mathbf{I}_n\)</span>),<br />
- <span class="math inline">\(\mathbf{V} \in \mathbb{R}^{p \times p}\)</span> is an <strong>orthonormal matrix</strong> (<span class="math inline">\(\mathbf{V}&#39; \mathbf{V} = \mathbf{I}_p\)</span>),<br />
- <span class="math inline">\(\mathbf{D} \in \mathbb{R}^{n \times p}\)</span> is a <strong>diagonal matrix</strong> with singular values <span class="math inline">\(d_1, d_2, \dots, d_n \geq 0\)</span> along the diagonal.</p>
<p>Since <span class="math inline">\(p &gt; n\)</span>, the number of singular values is at most <span class="math inline">\(n\)</span>, meaning that <span class="math inline">\(\mathbf{X}&#39; \mathbf{X}\)</span> has at most <strong>rank <span class="math inline">\(n\)</span></strong> and is <strong>not invertible</strong> when <span class="math inline">\(p &gt; n\)</span>.</p>
<p>Using the SVD of <span class="math inline">\(\mathbf{X}\)</span>, we can express</p>
<p><span class="math display">\[
\mathbf{X}&#39; \mathbf{X} = (\mathbf{U} \mathbf{D} \mathbf{V}&#39;)&#39; (\mathbf{U} \mathbf{D} \mathbf{V}&#39;)
= \mathbf{V} \mathbf{D}&#39; \mathbf{U}&#39; \mathbf{U} \mathbf{D} \mathbf{V}&#39;
= \mathbf{V} (\mathbf{D}&#39; \mathbf{D}) \mathbf{V}&#39;.
\]</span></p>
<p>Since <span class="math inline">\(\mathbf{U}\)</span> is an <span class="math inline">\(n \times n\)</span> orthogonal matrix, <span class="math inline">\(\mathbf{D}&#39; \mathbf{D}\)</span> is a <span class="math inline">\(p \times p\)</span> diagonal matrix:</p>
<p><span class="math display">\[
\mathbf{D}&#39; \mathbf{D} =
\begin{bmatrix}
d_1^2 &amp; 0 &amp; \dots &amp; 0 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; d_2^2 &amp; \dots &amp; 0 &amp; 0 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \dots &amp; d_n^2 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; 0 &amp; \dots &amp; 0 &amp; 0 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \dots &amp; 0 &amp; 0 &amp; \dots &amp; 0 \\
\end{bmatrix}.
\]</span></p>
<ul>
<li>The first <span class="math inline">\(n\)</span> diagonal entries are <span class="math inline">\(d_i^2\)</span>, corresponding to the squared singular values of <span class="math inline">\(\mathbf{X}\)</span>.<br />
</li>
<li>The remaining <span class="math inline">\(p - n\)</span> diagonal entries are <strong>zero</strong>, meaning <span class="math inline">\(\mathbf{X}&#39; \mathbf{X}\)</span> has <strong><span class="math inline">\(p - n\)</span> zero eigenvalues</strong> and is <strong>not full rank</strong>.</li>
</ul>
<p>Now, consider the modified matrix:</p>
<p><span class="math display">\[
\mathbf{X}&#39; \mathbf{X} + \lambda \mathbf{I}.
\]</span></p>
<p>Using the SVD form of <span class="math inline">\(\mathbf{X}&#39; \mathbf{X}\)</span>, we have:</p>
<p><span class="math display">\[
\mathbf{X}&#39; \mathbf{X} + \lambda \mathbf{I} = \mathbf{V} (\mathbf{D}&#39; \mathbf{D}) \mathbf{V}&#39; + \lambda \mathbf{I}.
\]</span></p>
<p>Since <span class="math inline">\(\mathbf{I} = \mathbf{V} \mathbf{I} \mathbf{V}&#39;\)</span>, we rewrite this as:</p>
<p><span class="math display">\[
\mathbf{X}&#39; \mathbf{X} + \lambda \mathbf{I} = \mathbf{V} (\mathbf{D}&#39; \mathbf{D} + \lambda \mathbf{I}) \mathbf{V}&#39;.
\]</span></p>
<p>Since <span class="math inline">\(\mathbf{D}&#39; \mathbf{D}\)</span> is diagonal, adding <span class="math inline">\(\lambda \mathbf{I}\)</span> results in:</p>
<p><span class="math display">\[
\mathbf{D}&#39; \mathbf{D} + \lambda \mathbf{I} =
\begin{bmatrix}
d_1^2 + \lambda &amp; 0 &amp; \dots &amp; 0 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; d_2^2 + \lambda &amp; \dots &amp; 0 &amp; 0 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \dots &amp; d_n^2 + \lambda &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; 0 &amp; \dots &amp; 0 &amp; \lambda &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \dots &amp; 0 &amp; 0 &amp; \dots &amp; \lambda \\
\end{bmatrix}.
\]</span></p>
<ul>
<li>The first <span class="math inline">\(n\)</span> diagonal entries are <span class="math inline">\(d_i^2 + \lambda\)</span>, all <strong>strictly positive</strong> because <span class="math inline">\(d_i^2 \geq 0\)</span> and <span class="math inline">\(\lambda &gt; 0\)</span>.<br />
</li>
<li>The last <span class="math inline">\(p - n\)</span> diagonal entries are <strong><span class="math inline">\(\lambda\)</span> (also strictly positive)</strong>.</li>
</ul>
<p>Thus, <strong>all eigenvalues of <span class="math inline">\(\mathbf{X}&#39; \mathbf{X} + \lambda \mathbf{I}\)</span> are strictly positive</strong>, implying that it is <strong>full rank and invertible</strong>.</p>
<p>Since <span class="math inline">\(\mathbf{V}\)</span> is an <strong>orthogonal matrix</strong>, the entire expression</p>
<p><span class="math display">\[
\mathbf{X}&#39; \mathbf{X} + \lambda \mathbf{I} = \mathbf{V} (\mathbf{D}&#39; \mathbf{D} + \lambda \mathbf{I}) \mathbf{V}&#39;
\]</span></p>
<p>is <strong>invertible</strong>, because an orthogonal matrix times an invertible diagonal matrix remains invertible.</p>
<p>So, even when <span class="math inline">\(p &gt; n\)</span>, adding <span class="math inline">\(\lambda \mathbf{I}\)</span> <strong>shifts all eigenvalues away from zero</strong>, ensuring that</p>
<p><span class="math display">\[
\mathbf{X}&#39; \mathbf{X} + \lambda \mathbf{I}
\]</span></p>
<p>is always invertible for <span class="math inline">\(\lambda &gt; 0\)</span>. This guarantees that Ridge Regression always has a unique solution:</p>
<p><span class="math display">\[
\boldsymbol{\beta}_{\text{ridge}} = (\mathbf{X}&#39; \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}&#39; \mathbf{y}.
\]</span></p>
<hr />
</div>
<div id="ridge-regression-as-a-minimization-problem" class="section level4 hasAnchor" number="4.1.2.3">
<h4><span class="header-section-number">4.1.2.3</span> Ridge Regression as a Minimization Problem<a href="linear-regression.html#ridge-regression-as-a-minimization-problem" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Instead of minimizing the standard sum of squared errors, Ridge Regression solves the following regularized problem:</p>
<p><span class="math display">\[
\min_{\boldsymbol{\beta}} \mathcal{L}(\boldsymbol{\beta}) = \min_{\boldsymbol{\beta}} \left\{ \| \mathbf{y} - \mathbf{X} \boldsymbol{\beta} \|_2^2 + \lambda \|\boldsymbol{\beta}\|_2^2 \right\}.
\]</span></p>
<p>where <span class="math inline">\(\lambda &gt; 0\)</span> is a <strong>tuning parameter</strong> that controls the amount of regularization:<br />
- <strong>When <span class="math inline">\(\lambda = 0\)</span></strong>: The problem reduces to ordinary least squares (OLS).<br />
- <strong>When <span class="math inline">\(\lambda \to \infty\)</span></strong>: The penalty dominates, forcing <span class="math inline">\(\boldsymbol{\beta}\)</span> toward <strong>zero</strong>, shrinking coefficients.</p>
<p>Expanding the loss function:</p>
<p><span class="math display">\[
\mathcal{L}(\boldsymbol{\beta}) = (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})&#39; (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) + \lambda \boldsymbol{\beta}&#39; \boldsymbol{\beta}.
\]</span></p>
<p>Taking the derivative with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span> and setting it to zero:</p>
<p><span class="math display">\[
-2 \mathbf{X}&#39; (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) + 2 \lambda \boldsymbol{\beta} = 0.
\]</span></p>
<p>Rearranging:</p>
<p><span class="math display">\[
\mathbf{X}&#39; \mathbf{X} \boldsymbol{\beta} + \lambda \boldsymbol{\beta} = \mathbf{X}&#39; \mathbf{y}.
\]</span></p>
<p>Factoring out <span class="math inline">\(\boldsymbol{\beta}\)</span>:</p>
<p><span class="math display">\[
(\mathbf{X}&#39; \mathbf{X} + \lambda \mathbf{I}) \boldsymbol{\beta} = \mathbf{X}&#39; \mathbf{y}.
\]</span></p>
<p>Since <span class="math inline">\(\mathbf{X}&#39;\mathbf{X}\)</span> may be singular, adding <span class="math inline">\(\lambda \mathbf{I}\)</span> ensures that the matrix <span class="math inline">\((\mathbf{X}&#39; \mathbf{X} + \lambda \mathbf{I})\)</span> is always invertible for any <span class="math inline">\(\lambda &gt; 0\)</span>.</p>
<p>Thus, the Ridge Regression solution is</p>
<p><span class="math display">\[
\boldsymbol{\beta}_{\text{ridge}} = (\mathbf{X}&#39; \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}&#39; \mathbf{y}.
\]</span></p>
<hr />
</div>
<div id="conclusion" class="section level4 hasAnchor" number="4.1.2.4">
<h4><span class="header-section-number">4.1.2.4</span> Conclusion<a href="linear-regression.html#conclusion" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li><strong>Matrix Regularization</strong>: The term <span class="math inline">\(\lambda \mathbf{I}\)</span> ensures that <span class="math inline">\(\mathbf{X}&#39;\mathbf{X} + \lambda \mathbf{I}\)</span> is always invertible because it shifts the eigenvalues of <span class="math inline">\(\mathbf{X}&#39;\mathbf{X}\)</span> away from zero.<br />
</li>
<li><strong>Bias-Variance Tradeoff</strong>: Ridge Regression <strong>reduces variance</strong> at the cost of introducing some <strong>bias</strong>, which can improve prediction accuracy when <span class="math inline">\(\mathbf{X}\)</span> is ill-conditioned or when <span class="math inline">\(p &gt; n\)</span>.<br />
</li>
<li><strong>Shrinkage Effect</strong>: Larger <span class="math inline">\(\lambda\)</span> values shrink the coefficients towards zero, preventing overfitting.<br />
</li>
<li>When <span class="math inline">\(\lambda = 0\)</span>: The problem reduces to ordinary least squares (OLS).<br />
</li>
<li>When <span class="math inline">\(\lambda \to \infty\)</span>: The penalty dominates, forcing <span class="math inline">\(\boldsymbol{\beta}\)</span> toward <strong>zero</strong>, shrinking coefficients.</li>
</ol>
<p>The only thing that is left is selecting the value of <span class="math inline">\(\lambda\)</span></p>
<hr />
</div>
</div>
<div id="lasso-regression" class="section level3 hasAnchor" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> Lasso Regression<a href="linear-regression.html#lasso-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Lasso regression</strong> (Least Absolute Shrinkage and Selection Operator) is a variation of <strong>linear regression</strong> that adds a penalty (like Ridge Regression) to the loss function to promote <strong>sparsity</strong> in the coefficients, effectively setting some of them to zero (unulike Ridge Regression). This makes Lasso a useful technique for <strong>feature selection</strong>, especially when we have many predictors, some of which may be irrelevant or highly correlated.</p>
<hr />
<div id="lasso-regression-as-an-optimization-problem" class="section level4 hasAnchor" number="4.1.3.1">
<h4><span class="header-section-number">4.1.3.1</span> Lasso Regression as an Optimization Problem<a href="linear-regression.html#lasso-regression-as-an-optimization-problem" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The Lasso regression formulation is:</p>
<p><span class="math display">\[
\min_{\boldsymbol{\beta}} \mathcal{L}(\boldsymbol{\beta}) = \min_{\boldsymbol{\beta}} \left\{ \| \mathbf{y} - \mathbf{X} \boldsymbol{\beta} \|_2^2 + \lambda \|\boldsymbol{\beta}\|_1 \right\}.
\]</span></p>
<p>Where:
- <span class="math inline">\(\|\boldsymbol{\beta}\|_1 = \sum_{i=1}^p |\beta_i|\)</span> is the <strong>L1 norm</strong> of the coefficients (sum of absolute values of the coefficients),
- <span class="math inline">\(\lambda \geq 0\)</span> is the <strong>regularization parameter</strong> controlling the strength of the penalty.</p>
<p>The loss function consists of:
1. <strong>Residual Sum of Squares (RSS)</strong>: <span class="math inline">\(\| \mathbf{y} - \mathbf{X} \boldsymbol{\beta} \|_2^2\)</span>, which measures the fit of the model (same as in ordinary least squares regression).
2. <strong>L1 Penalty</strong>: <span class="math inline">\(\lambda \|\boldsymbol{\beta}\|_1\)</span>, which shrinks the coefficients towards zero and encourages sparsity (i.e., some coefficients are exactly zero).</p>
<p>The <strong>objective</strong> is to minimize the <strong>sum of squared residuals</strong> along with a penalty term:</p>
<p><span class="math display">\[
\min_{\boldsymbol{\beta}} \left\{ \| \mathbf{y} - \mathbf{X} \boldsymbol{\beta} \|_2^2 + \lambda \sum_{i=1}^p |\beta_i| \right\}.
\]</span></p>
<p>The key feature of Lasso is that the <strong>L1 penalty</strong> promotes <strong>sparsity</strong> by shrinking some coefficients exactly to zero, which results in a simpler, more interpretable model. The parameter <span class="math inline">\(\lambda\)</span> controls the trade-off between <strong>fit</strong> and <strong>sparsity</strong>:
- <strong>When <span class="math inline">\(\lambda = 0\)</span></strong>: Lasso reduces to ordinary least squares regression (OLS), where no penalty is applied.
- <strong>When <span class="math inline">\(\lambda\)</span> is large</strong>: The penalty dominates, and more coefficients are shrunk to zero.</p>
<p>Unfortunately, unlike Ridge Regression, Lasso has no closed form solution and it is necessary to find the solution numerically.</p>
<hr />
</div>
</div>
<div id="elastic-net" class="section level3 hasAnchor" number="4.1.4">
<h3><span class="header-section-number">4.1.4</span> Elastic Net<a href="linear-regression.html#elastic-net" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Elastic Net is a regularization technique that combines the strengths of <strong>Lasso</strong> and <strong>Ridge</strong> regression. While Lasso uses an L1 penalty and Ridge uses an L2 penalty, Elastic Net applies a <strong>mix</strong> of both penalties, giving a balance between sparsity and regularization strength. Elastic Net is particularly useful when there are <strong>highly correlated features</strong> or when the number of features is larger than the number of observations (<span class="math inline">\(p &gt; n\)</span>).</p>
<hr />
</div>
<div id="elastic-net-as-a-mixed-penalty" class="section level3 hasAnchor" number="4.1.5">
<h3><span class="header-section-number">4.1.5</span> <strong>Elastic Net as a Mixed Penalty</strong><a href="linear-regression.html#elastic-net-as-a-mixed-penalty" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Elastic Net loss function is defined as:</p>
<p><span class="math display">\[
\min_{\boldsymbol{\beta}} \left\{ \| \mathbf{y} - \mathbf{X} \boldsymbol{\beta} \|_2^2 + \lambda_1 \|\boldsymbol{\beta}\|_1 + \frac{\lambda_2}{2} \|\boldsymbol{\beta}\|_2^2 \right\}.
\]</span></p>
<p>Where:
- <span class="math inline">\(\mathbf{y}\)</span> is the <span class="math inline">\(n \times 1\)</span> vector of observed responses,
- <span class="math inline">\(\mathbf{X}\)</span> is the <span class="math inline">\(n \times p\)</span> matrix of predictor variables,
- <span class="math inline">\(\boldsymbol{\beta}\)</span> is the <span class="math inline">\(p \times 1\)</span> vector of regression coefficients,
- <span class="math inline">\(\|\boldsymbol{\beta}\|_1 = \sum_{i=1}^p |\beta_i|\)</span> is the L1 norm (Lasso penalty),
- <span class="math inline">\(\|\boldsymbol{\beta}\|_2^2 = \sum_{i=1}^p \beta_i^2\)</span> is the L2 norm (Ridge penalty),
- <span class="math inline">\(\lambda_1 \geq 0\)</span> is the L1 regularization parameter (controlling the Lasso penalty),
- <span class="math inline">\(\lambda_2 \geq 0\)</span> is the L2 regularization parameter (controlling the Ridge penalty).</p>
<ul>
<li><p><strong>L1 Penalty (Lasso term)</strong>: <span class="math inline">\(\lambda_1 \|\boldsymbol{\beta}\|_1\)</span> encourages sparsity, meaning that it drives some coefficients to exactly zero. This is helpful for feature selection and reduces the complexity of the model.</p></li>
<li><p><strong>L2 Penalty (Ridge term)</strong>: <span class="math inline">\(\frac{\lambda_2}{2} \|\boldsymbol{\beta}\|_2^2\)</span> shrinks the coefficients toward zero without setting them exactly to zero. This helps with multicollinearity, preventing large fluctuations in the estimated coefficients when predictors are highly correlated.</p></li>
</ul>
<hr />
<div id="why-use-elastic-net" class="section level4 hasAnchor" number="4.1.5.1">
<h4><span class="header-section-number">4.1.5.1</span> Why Use Elastic Net?<a href="linear-regression.html#why-use-elastic-net" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p><strong>Correlation between predictors</strong>: When predictors are highly correlated, Lasso tends to select one variable and ignore the others. Elastic Net, by mixing L1 and L2 penalties, can help by including correlated variables in the model but still controlling their coefficients through the L2 penalty.</p></li>
<li><p><strong>Feature selection with many predictors</strong>: In cases where the number of features <span class="math inline">\(p\)</span> is much greater than the number of observations <span class="math inline">\(n\)</span>, Lasso can become unstable. Elastic Net helps stabilize the model by adding a Ridge component, which shrinks the coefficients of less important features without forcing them to zero.</p></li>
</ul>
<p>The Elastic Net can be seen as a <strong>weighted sum</strong> of the Lasso and Ridge penalties, where:</p>
<p><span class="math display">\[
\text{Elastic Net Loss} = \lambda_1 \|\boldsymbol{\beta}\|_1 + \frac{\lambda_2}{2} \|\boldsymbol{\beta}\|_2^2.
\]</span></p>
<p>You can think of <strong><span class="math inline">\(\lambda_1\)</span></strong> as controlling the strength of the <strong>Lasso</strong> penalty (feature selection), and <strong><span class="math inline">\(\lambda_2\)</span></strong> as controlling the strength of the <strong>Ridge</strong> penalty (shrinkage). The Elastic Net is useful when you need both <strong>sparsity</strong> (for feature selection) and <strong>regularization</strong> (to prevent overfitting).</p>
<hr />
</div>
<div id="optimization-problem" class="section level4 hasAnchor" number="4.1.5.2">
<h4><span class="header-section-number">4.1.5.2</span> Optimization Problem<a href="linear-regression.html#optimization-problem" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The Elastic Net optimization problem is:</p>
<p><span class="math display">\[
\min_{\boldsymbol{\beta}} \left\{ \| \mathbf{y} - \mathbf{X} \boldsymbol{\beta} \|_2^2 + \lambda_1 \|\boldsymbol{\beta}\|_1 + \frac{\lambda_2}{2} \|\boldsymbol{\beta}\|_2^2 \right\}.
\]</span></p>
<ul>
<li><p><strong>Objective function</strong>: The goal is to minimize the <strong>sum of squared residuals</strong> (RSS) plus the combined <strong>penalty terms</strong>.</p></li>
<li><p>The optimal values of <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span> are typically chosen via <strong>cross-validation</strong>.</p></li>
</ul>
<hr />
</div>
<div id="connections-to-lasso-and-ridge-regression" class="section level4 hasAnchor" number="4.1.5.3">
<h4><span class="header-section-number">4.1.5.3</span> Connections to Lasso and Ridge Regression<a href="linear-regression.html#connections-to-lasso-and-ridge-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p><strong>When <span class="math inline">\(\lambda_2 = 0\)</span></strong>: Elastic Net becomes Lasso regression, as the Ridge term disappears and only the L1 penalty is applied.</p></li>
<li><p><strong>When <span class="math inline">\(\lambda_1 = 0\)</span></strong>: Elastic Net becomes Ridge regression, as the L1 penalty is removed and only the L2 penalty is applied.</p></li>
<li><p><strong>When both <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span> are non-zero</strong>: Elastic Net is a combination of both regularization methods, providing a balanced approach.</p></li>
</ul>
<hr />
</div>
</div>
<div id="other-options-of-regularization" class="section level3 hasAnchor" number="4.1.6">
<h3><span class="header-section-number">4.1.6</span> Other Options of Regularization<a href="linear-regression.html#other-options-of-regularization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In addition to Elastic Net, Ridge, and Lasso, there are other regularization methods used in machine learning and statistical modeling:</p>
<ol style="list-style-type: decimal">
<li><strong>Group Lasso</strong>:
<ul>
<li><strong>Formula</strong>:<br />
<span class="math display">\[
\min_{\boldsymbol{\beta}} \left\{ \| \mathbf{y} - \mathbf{X} \boldsymbol{\beta} \|_2^2 + \lambda \sum_{g} \|\boldsymbol{\beta}_g\|_2 \right\}.
\]</span></li>
<li><strong>Penalty</strong>: Group Lasso is used when variables are grouped, and the penalty is applied at the group level. It forces entire groups of variables to be either included or excluded from the model.</li>
</ul></li>
<li><strong>Fused Lasso</strong>:
<ul>
<li><strong>Formula</strong>:<br />
<span class="math display">\[
\min_{\boldsymbol{\beta}} \left\{ \| \mathbf{y} - \mathbf{X} \boldsymbol{\beta} \|_2^2 + \lambda_1 \|\boldsymbol{\beta}\|_1 + \lambda_2 \sum_{i} |\beta_i - \beta_{i-1}| \right\}.
\]</span></li>
<li><strong>Penalty</strong>: Fused Lasso adds a penalty to the differences between adjacent coefficients, encouraging <strong>smoothness</strong> in the solution. This is useful in time series or spatial data where adjacent coefficients are expected to be similar.</li>
</ul></li>
<li><strong>Bayesian Regularization</strong>:
<ul>
<li><strong>Formula</strong>: Bayesian regularization methods, like <strong>Bayesian Ridge Regression</strong>, assume a probabilistic model for the coefficients and add a prior distribution (often Gaussian) to the coefficients. The regularization comes from the prior’s influence on the model.</li>
<li><strong>Penalty</strong>: The prior serves as a regularizer, encouraging smaller coefficients with the Gaussian prior.</li>
</ul></li>
</ol>
<hr />
</div>
</div>
<div id="bayesian-linear-regression" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Bayesian Linear Regression<a href="linear-regression.html#bayesian-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Bayesian regression provides a probabilistic framework for regression analysis by incorporating prior knowledge about the parameters. It offers a direct connection to <strong>regularized regression</strong>, by introducing a prior on the regression coefficients.</p>
<div id="basic-bayesian-linear-regression" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Basic Bayesian Linear Regression<a href="linear-regression.html#basic-bayesian-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As before, we consider the standard linear regression model:</p>
<p><span class="math display">\[
\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{e}, \quad \mathbf{e} \sim \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I}).
\]</span></p>
<p>where:<br />
- <span class="math inline">\(\mathbf{y}\)</span> is the <span class="math inline">\(n \times 1\)</span> vector of observed responses,<br />
- <span class="math inline">\(\mathbf{X}\)</span> is the <span class="math inline">\(n \times p\)</span> matrix of predictor variables,<br />
- <span class="math inline">\(\boldsymbol{\beta}\)</span> is the <span class="math inline">\(p \times 1\)</span> vector of regression coefficients,<br />
- <span class="math inline">\(\mathbf{e} \sim \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I})\)</span> is the noise term, assumed to follow a normal distribution with variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>The <strong>likelihood function</strong> follows from the assumption that <span class="math inline">\(\mathbf{y}\)</span> is normally distributed given <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\boldsymbol{\beta}\)</span>:</p>
<p><span class="math display">\[
p(\mathbf{y} | \boldsymbol{\beta}) = \mathcal{N}(\mathbf{X} \boldsymbol{\beta}, \sigma^2 \mathbf{I}).
\]</span></p>
<p>In a Bayesian framework, we assume a <strong>prior distribution</strong> for <span class="math inline">\(\boldsymbol{\beta}\)</span>. There are several (infinite) alternatives to set a prior, however in this case, we are going to work with a very basic model, in fact in a more gneral setting a prior distribution for <span class="math inline">\(\sigma^2\)</span> is usually specified.</p>
<p>We take a <strong>normal prior</strong> with mean zero and covariance matrix <span class="math inline">\(\sigma^2 \mathbf{\Sigma}_\beta\)</span>, where <span class="math inline">\(\mathbf{\Sigma}_\beta\)</span> captures prior beliefs about the relationships between the coefficients:</p>
<p><span class="math display">\[
p(\boldsymbol{\beta}) = \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{\Sigma}_\beta).
\]</span></p>
<p>where <span class="math inline">\(\mathbf{\Sigma}_\beta\)</span> is a positive definite <span class="math inline">\(p \times p\)</span> covariance matrix.</p>
<hr />
<div id="posterior-distribution-of-boldsymbolbeta" class="section level4 hasAnchor" number="4.2.1.1">
<h4><span class="header-section-number">4.2.1.1</span> Posterior Distribution of <span class="math inline">\(\boldsymbol{\beta}\)</span><a href="linear-regression.html#posterior-distribution-of-boldsymbolbeta" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Applying <strong>Bayes’ theorem</strong>, the posterior is proportional to the product of the likelihood and the prior:</p>
<p><span class="math display">\[
p(\boldsymbol{\beta} | \mathbf{y}) \propto p(\mathbf{y} | \boldsymbol{\beta}) p(\boldsymbol{\beta}).
\]</span></p>
<p>Since both the <strong>likelihood</strong> and <strong>prior</strong> are Gaussian, the <strong>posterior</strong> will also be Gaussian. To derive its mean and covariance, we complete the square in the exponent.</p>
<p><span class="math display">\[
p(\mathbf{y} | \boldsymbol{\beta}) \propto \exp \left( -\frac{1}{2\sigma^2} \| \mathbf{y} - \mathbf{X} \boldsymbol{\beta} \|_2^2 \right).
\]</span></p>
<p>Expanding:</p>
<p><span class="math display">\[
\| \mathbf{y} - \mathbf{X} \boldsymbol{\beta} \|_2^2 = (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})&#39; (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}).
\]</span></p>
<p><span class="math display">\[
p(\boldsymbol{\beta}) \propto \exp \left( -\frac{1}{2\sigma^2} \boldsymbol{\beta}&#39; \mathbf{\Sigma}_\beta^{-1} \boldsymbol{\beta} \right).
\]</span></p>
<p>The posterior distribution is proportional to:</p>
<p><span class="math display">\[
\exp \left( -\frac{1}{2\sigma^2} \left[ (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})&#39; (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) + \boldsymbol{\beta}&#39; \mathbf{\Sigma}_\beta^{-1} \boldsymbol{\beta} \right] \right).
\]</span></p>
<p>Expanding the quadratic term:</p>
<p><span class="math display">\[
\mathbf{y}&#39; \mathbf{y} - 2 \mathbf{y}&#39; \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\beta}&#39; \mathbf{X}&#39; \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\beta}&#39; \mathbf{\Sigma}_\beta^{-1} \boldsymbol{\beta}.
\]</span></p>
<p>Rewriting,</p>
<p><span class="math display">\[
- 2 \mathbf{y}&#39; \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\beta}&#39; (\mathbf{X}&#39; \mathbf{X} + \mathbf{\Sigma}_\beta^{-1}) \boldsymbol{\beta}.
\]</span></p>
<p>Completing the square, we identify the posterior mean:</p>
<p><span class="math display">\[
\boldsymbol{\beta} | \mathbf{y} \sim \mathcal{N}(\boldsymbol{\mu}_\beta, \mathbf{\Sigma}_\beta^*),
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\mathbf{\Sigma}_\beta^* = \left( \mathbf{X}&#39; \mathbf{X} + \mathbf{\Sigma}_\beta^{-1} \right)^{-1} \sigma^2,
\]</span></p>
<p><span class="math display">\[
\boldsymbol{\mu}_\beta = \mathbf{\Sigma}_\beta^* \mathbf{X}&#39; \mathbf{y}.
\]</span></p>
<hr />
</div>
<div id="connection-to-ridge-regression" class="section level4 hasAnchor" number="4.2.1.2">
<h4><span class="header-section-number">4.2.1.2</span> Connection to Ridge Regression<a href="linear-regression.html#connection-to-ridge-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>If we assume that the prior covariance is a <strong>scaled identity matrix</strong>, i.e.,</p>
<p><span class="math display">\[
\mathbf{\Sigma}_\beta = \frac{1}{\lambda} \mathbf{I},
\]</span></p>
<p>then its inverse is:</p>
<p><span class="math display">\[
\mathbf{\Sigma}_\beta^{-1} = \lambda \mathbf{I}.
\]</span></p>
<p>Substituting this into the posterior mean formula:</p>
<p><span class="math display">\[
\boldsymbol{\mu}_\beta = \left( \mathbf{X}&#39; \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}&#39; \mathbf{y}.
\]</span></p>
<p>which is <strong>exactly the Ridge estimator</strong>:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}}_{\text{ridge}} = \left( \mathbf{X}&#39; \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}&#39; \mathbf{y}.
\]</span></p>
<p>Thus, we see that Bayesian regression with a normal prior on <span class="math inline">\(\boldsymbol{\beta}\)</span> corresponds to Ridge regression, where the <strong>regularization parameter <span class="math inline">\(\lambda\)</span></strong> is determined by the prior covariance.</p>
</div>
<div id="behavior-of-the-posterior-distribution-as-lambda-varies" class="section level4 hasAnchor" number="4.2.1.3">
<h4><span class="header-section-number">4.2.1.3</span> Behavior of the Posterior Distribution as <span class="math inline">\(\lambda\)</span> Varies<a href="linear-regression.html#behavior-of-the-posterior-distribution-as-lambda-varies" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Since we have shown that the <strong>posterior mean</strong> of <span class="math inline">\(\boldsymbol{\beta}\)</span> is:</p>
<p><span class="math display">\[
\boldsymbol{\mu}_\beta = \left( \mathbf{X}&#39; \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \mathbf{X}&#39; \mathbf{y},
\]</span></p>
<p>and the <strong>posterior covariance</strong> is:</p>
<p><span class="math display">\[
\mathbf{\Sigma}_\beta^* = \sigma^2 \left( \mathbf{X}&#39; \mathbf{X} + \lambda \mathbf{I} \right)^{-1},
\]</span></p>
<p>we can analyze the behavior of the <strong>posterior distribution</strong> under extreme values of <span class="math inline">\(\lambda\)</span>.</p>
<ol style="list-style-type: decimal">
<li>When <span class="math inline">\(\lambda \to 0\)</span> (No Regularization, Pure MLE)</li>
</ol>
<p>As <span class="math inline">\(\lambda \to 0\)</span>, the prior becomes <strong>uninformative</strong>, meaning we are not imposing any shrinkage on the coefficients. In this case:</p>
<p><span class="math display">\[
\left( \mathbf{X}&#39; \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \to (\mathbf{X}&#39; \mathbf{X})^{-1},
\]</span></p>
<p>assuming <span class="math inline">\(\mathbf{X}&#39; \mathbf{X}\)</span> is invertible. Then, the <strong>posterior mean</strong> simplifies to the <strong>ordinary least squares (OLS) estimator</strong>:</p>
<p><span class="math display">\[
\boldsymbol{\mu}_\beta \to (\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{X}&#39; \mathbf{y}.
\]</span></p>
<p>Likewise, the <strong>posterior covariance</strong> reduces to:</p>
<p><span class="math display">\[
\mathbf{\Sigma}_\beta^* \to \sigma^2 (\mathbf{X}&#39; \mathbf{X})^{-1}.
\]</span></p>
<p>This shows that, when <span class="math inline">\(\lambda \to 0\)</span>, <strong>Bayesian regression becomes equivalent to the classical maximum likelihood estimate (MLE) from OLS</strong>, with high variance when <span class="math inline">\(\mathbf{X}&#39; \mathbf{X}\)</span> is ill-conditioned.</p>
<ol start="2" style="list-style-type: decimal">
<li>When <span class="math inline">\(\lambda \to \infty\)</span> (Strong Prior, Heavy Shrinkage)</li>
</ol>
<p>As <span class="math inline">\(\lambda \to \infty\)</span>, the prior dominates and strongly shrinks <span class="math inline">\(\boldsymbol{\beta}\)</span> toward <strong>zero</strong>. In this case:</p>
<p><span class="math display">\[
\left( \mathbf{X}&#39; \mathbf{X} + \lambda \mathbf{I} \right)^{-1} \approx \frac{1}{\lambda} \mathbf{I}, \quad \text{(for large \( \lambda \))}.
\]</span></p>
<p>Thus, the <strong>posterior mean</strong> behaves as:</p>
<p><span class="math display">\[
\boldsymbol{\mu}_\beta \approx \frac{1}{\lambda} \mathbf{X}&#39; \mathbf{y} \to \mathbf{0} \quad \text{as} \quad \lambda \to \infty.
\]</span></p>
<p>Similarly, the <strong>posterior covariance</strong> reduces to:</p>
<p><span class="math display">\[
\mathbf{\Sigma}_\beta^* \approx \frac{\sigma^2}{\lambda} \mathbf{I} \to \mathbf{0}.
\]</span></p>
<p>This means that, for very large <span class="math inline">\(\lambda\)</span>, the posterior distribution becomes <strong>highly concentrated around zero</strong>:</p>
<p><span class="math display">\[
\boldsymbol{\beta} | \mathbf{y} \sim \mathcal{N}(\mathbf{0}, 0).
\]</span></p>
<p><strong>Interpretation:</strong>
- As <span class="math inline">\(\lambda \to \infty\)</span>, the prior <strong>overwhelms the data</strong> and forces all coefficients to shrink to zero.
- The posterior variance also vanishes, meaning the uncertainty about <span class="math inline">\(\boldsymbol{\beta}\)</span> disappears—everything is shrunk toward the prior mean (which is zero in this case).
- This corresponds to <strong>extreme regularization</strong>, effectively setting all coefficients to zero, similar to a very strong Ridge penalty.</p>
<p>Summary of <span class="math inline">\(\lambda\)</span>-Dependence:</p>
<table style="width:100%;">
<colgroup>
<col width="13%" />
<col width="35%" />
<col width="35%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th><span class="math inline">\(\lambda\)</span></th>
<th>Posterior Mean <span class="math inline">\(\boldsymbol{\mu}_\beta\)</span></th>
<th>Posterior Covariance <span class="math inline">\(\mathbf{\Sigma}_\beta^*\)</span></th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\lambda \to 0\)</span></td>
<td>OLS estimate: <span class="math inline">\((\mathbf{X}&#39; \mathbf{X})^{-1} \mathbf{X}&#39; \mathbf{y}\)</span></td>
<td><span class="math inline">\(\sigma^2 (\mathbf{X}&#39; \mathbf{X})^{-1}\)</span></td>
<td>No regularization (pure MLE). Large variance if <span class="math inline">\(\mathbf{X}&#39; \mathbf{X}\)</span> is ill-conditioned.</td>
</tr>
<tr class="even">
<td>Small <span class="math inline">\(\lambda\)</span></td>
<td>Close to OLS</td>
<td>Slightly shrunk covariance</td>
<td>Light regularization. Small shrinkage toward zero.</td>
</tr>
<tr class="odd">
<td>Large <span class="math inline">\(\lambda\)</span></td>
<td>Strongly shrunk toward zero</td>
<td>Shrunk covariance, but still adaptive to data</td>
<td>Ridge-like regularization, balances data and prior.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\lambda \to \infty\)</span></td>
<td><span class="math inline">\(\mathbf{0}\)</span></td>
<td><span class="math inline">\(\mathbf{0}\)</span></td>
<td>Extreme shrinkage; model ignores data and forces coefficients to zero.</td>
</tr>
</tbody>
</table>
<hr />
</div>
<div id="conclusion-1" class="section level4 hasAnchor" number="4.2.1.4">
<h4><span class="header-section-number">4.2.1.4</span> Conclusion<a href="linear-regression.html#conclusion-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The Bayesian regression model presented here is a <strong>basic formulation</strong> that assumes a <strong>Gaussian likelihood</strong> and a <strong>normal prior</strong> on the regression coefficients. This simple setup already reveals deep connections to <strong>Ridge regression</strong>, demonstrating how prior beliefs influence parameter estimation through shrinkage.</p>
<p>Also note that the Bayesian framework allows you to perform Linear Regression even in the case where <span class="math inline">\(p &gt; n\)</span> without the need to change models, as long as a proper prior for <span class="math inline">\( {\boldsymbol \beta} \)</span> is used.</p>
<p>The model developed before, is just one possible <strong>Bayesian approach to regression</strong>. Many alternative priors can be used to <strong>encode different assumptions</strong> about the regression coefficients, leading to distinct forms of regularization:</p>
<ul>
<li><strong>Laplace prior</strong>: Leads to <strong>Bayesian Lasso</strong>, which promotes sparsity by encouraging some coefficients to be exactly zero.<br />
</li>
<li><strong>Spike-and-slab prior</strong>: A mixture of a <strong>point mass at zero</strong> and a <strong>diffuse normal distribution</strong>, allowing for <strong>automatic feature selection</strong>.<br />
</li>
<li><strong>Horseshoe prior</strong>: A heavy-tailed prior that <strong>shrinks small coefficients strongly</strong> while allowing large ones to remain, making it useful for sparse models with some large effects.<br />
</li>
<li><strong>Gaussian Process priors</strong>: Used in <strong>nonparametric Bayesian regression</strong>, allowing for flexible modeling of relationships without assuming a fixed functional form.</li>
</ul>
<p>These richer prior choices allow <strong>Bayesian regression to adapt to a variety of settings</strong>, from high-dimensional problems to nonlinear relationships. Bayesian approaches also provide <strong>full posterior distributions</strong>, enabling uncertainty quantification in predictions—a key advantage over standard frequentist methods.</p>
</div>
</div>
<div id="bayesian-lasso-regression" class="section level3 hasAnchor" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Bayesian Lasso Regression<a href="linear-regression.html#bayesian-lasso-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Given the standard regression model:</p>
<p><span class="math display">\[
\mathbf{y} | \boldsymbol{\beta}, \sigma^2 \sim \mathcal{N}(\mathbf{X} \boldsymbol{\beta}, \sigma^2 \mathbf{I}),
\]</span></p>
<p>we place a <strong>Laplace prior</strong> on each coefficient <span class="math inline">\(\beta_j\)</span>, scaled by <span class="math inline">\(\sigma\)</span>, as follows:</p>
<p><span class="math display">\[
p(\beta_j | \sigma^2) = \frac{\lambda}{2\sigma} \exp \left( - \frac{\lambda}{\sigma} | \beta_j | \right).
\]</span></p>
<p>This prior encourages sparsity, <strong>shrinking small coefficients toward zero</strong> while allowing some large ones.</p>
<p>Unlike the basic Bayesian approach in the last section, the Bayesian Lasso does not
have a closed form posterior distribution. However, it is easy to sample from following a hierarchical prior approach.</p>
<hr />
<div id="hierarchical-representation-of-the-laplace-prior" class="section level4 hasAnchor" number="4.2.2.1">
<h4><span class="header-section-number">4.2.2.1</span> Hierarchical Representation of the Laplace Prior<a href="linear-regression.html#hierarchical-representation-of-the-laplace-prior" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <strong>Laplace prior</strong> can be rewritten as a <strong>hierarchical model</strong> using a <strong>Gaussian scale mixture representation</strong>. Specifically, we introduce auxiliary variance parameters <span class="math inline">\(\tau_j^2\)</span>, where:</p>
<p><span class="math display">\[
\beta_j | \tau_j^2, \sigma^2 \sim \mathcal{N}(0, \sigma^2 \tau_j^2).
\]</span></p>
<p>The prior on <span class="math inline">\(\tau_j^2\)</span> follows an <strong>exponential distribution</strong>:</p>
<p><span class="math display">\[
p(\tau_j^2 | \lambda^2) = \frac{\lambda^2}{2} \exp \left( -\frac{\lambda^2}{2} \tau_j^2 \right).
\]</span></p>
<p>Thus, the Bayesian Lasso can be <strong>interpreted as Bayesian ridge regression with an adaptive prior variance</strong> for each coefficient.</p>
<hr />
</div>
<div id="posterior-distribution-and-map-estimator" class="section level4 hasAnchor" number="4.2.2.2">
<h4><span class="header-section-number">4.2.2.2</span> Posterior Distribution and MAP Estimator<a href="linear-regression.html#posterior-distribution-and-map-estimator" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <strong>posterior distribution</strong> of <span class="math inline">\(\boldsymbol{\beta}\)</span> is given by:</p>
<p><span class="math display">\[
p(\boldsymbol{\beta} | \mathbf{y}, \sigma^2) \propto p(\mathbf{y} | \boldsymbol{\beta}, \sigma^2) p(\boldsymbol{\beta} | \sigma^2).
\]</span></p>
<p>Since:
- The <strong>likelihood</strong> is Gaussian:<br />
<span class="math display">\[
  p(\mathbf{y} | \boldsymbol{\beta}, \sigma^2) \propto \exp \left( - \frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{X} \boldsymbol{\beta} \|_2^2 \right),
  \]</span>
- The <strong>prior</strong> is Laplace:<br />
<span class="math display">\[
  p(\boldsymbol{\beta} | \sigma^2) \propto \exp \left( - \frac{\lambda}{2\sigma} \|\boldsymbol{\beta}\|_1 \right),
  \]</span></p>
<p>then the <strong>posterior mode</strong> (i.e., the Maximum A Posteriori (MAP) estimator) is obtained by solving:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}} = \arg \min_{\boldsymbol{\beta}} \left\{ \|\mathbf{y} - \mathbf{X} \boldsymbol{\beta} \|_2^2 + \sigma \lambda \|\boldsymbol{\beta}\|_1 \right\}.
\]</span></p>
<p>This <strong>exactly recovers the traditional Lasso estimator</strong>, where the regularization term depends on <span class="math inline">\(\sigma \lambda\)</span>.</p>
<p>Thus, the Bayesian Lasso <strong>provides a probabilistic justification</strong> for the Lasso estimator and explains how <strong>shrinkage is controlled by both <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\sigma^2\)</span></strong>.</p>
<hr />
</div>
<div id="behavior-of-the-posterior-as-lambda-and-sigma2-vary" class="section level4 hasAnchor" number="4.2.2.3">
<h4><span class="header-section-number">4.2.2.3</span> Behavior of the Posterior as <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\sigma^2\)</span> Vary<a href="linear-regression.html#behavior-of-the-posterior-as-lambda-and-sigma2-vary" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>As <span class="math inline">\(\lambda \to 0\)</span></strong>: The prior becomes <strong>uninformative</strong>, and the MAP estimate approaches the <strong>MLE (ordinary least squares)</strong>.<br />
</li>
<li><strong>As <span class="math inline">\(\lambda \to \infty\)</span></strong>: The prior dominates the likelihood, and the posterior distribution becomes <strong>highly concentrated at zero</strong> (extreme sparsity).</li>
</ul>
<hr />
</div>
</div>
</div>
<div id="computational-comparisson" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Computational Comparisson<a href="linear-regression.html#computational-comparisson" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We generate a synthetic dataset where only a subset of the predictors are relevant. We compare:</p>
<ul>
<li>Ridge regression.</li>
<li>Lasso regression.</li>
<li>Basic Bayesian Regression.</li>
<li>Bayesian Lasso Regression.</li>
<li>Horseshoe Prior Bayesian Regression.</li>
</ul>
<hr />
<div id="set-up" class="section level3 hasAnchor" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Set-Up<a href="linear-regression.html#set-up" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>First we need some functions and libraries:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="linear-regression.html#cb1-1" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span></code></pre></div>
<ul>
<li>Loads the <strong>glmnet</strong> package, which is used for <strong>Lasso, Ridge, and Elastic Net</strong> regression.<br />
</li>
<li>The <code>glmnet</code> package provides functions like <code>cv.glmnet()</code> and <code>glmnet()</code> to perform <strong>penalized regression</strong> with cross-validation.</li>
</ul>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="linear-regression.html#cb2-1" tabindex="-1"></a><span class="fu">library</span>(monomvn)</span></code></pre></div>
<ul>
<li>Loads the <strong>monomvn</strong> package, which provides Bayesian regression models, including <strong>Bayesian Lasso and Bayesian Ridge</strong>.</li>
</ul>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="linear-regression.html#cb3-1" tabindex="-1"></a><span class="fu">library</span>(mvtnorm)</span></code></pre></div>
<ul>
<li>Loads the <strong>mvtnorm</strong> package, which allows working with the <strong>multivariate normal distribution</strong>.<br />
</li>
<li>Used for <strong>simulating correlated predictors</strong> in the design matrix (<code>X</code>) and for Bayesian sampling.</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="linear-regression.html#cb4-1" tabindex="-1"></a><span class="fu">source</span>(<span class="st">&quot;./horseshoe_sampler.R&quot;</span>)</span>
<span id="cb4-2"><a href="linear-regression.html#cb4-2" tabindex="-1"></a><span class="fu">source</span>(<span class="st">&quot;./fast_sampler.R&quot;</span>)</span>
<span id="cb4-3"><a href="linear-regression.html#cb4-3" tabindex="-1"></a><span class="fu">source</span>(<span class="st">&quot;./fast_horseshoe.R&quot;</span>)</span></code></pre></div>
<ul>
<li>Loads external R scripts (<code>horseshoe_sampler.R</code>, <code>fast_sampler.R</code>, and <code>fast_horseshoe.R</code>), which likely contain <strong>Bayesian sampling functions</strong> for <strong>Horseshoe priors</strong>.<br />
</li>
<li>These scripts implement <strong>Markov Chain Monte Carlo (MCMC) algorithms</strong> or other methods to generate posterior samples.</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="linear-regression.html#cb5-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">222025</span>)</span></code></pre></div>
<ul>
<li><strong>Sets the random seed</strong> to ensure that results are <strong>reproducible</strong>.<br />
</li>
<li>Ensures that simulated data and stochastic processes (e.g., cross-validation, Bayesian sampling) yield the <strong>same results</strong> every time the script is run.</li>
</ul>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="linear-regression.html#cb6-1" tabindex="-1"></a>numTra <span class="ot">&lt;-</span> <span class="dv">300</span>         <span class="co"># Training samples</span></span>
<span id="cb6-2"><a href="linear-regression.html#cb6-2" tabindex="-1"></a>numSam <span class="ot">&lt;-</span> numTra <span class="sc">*</span> <span class="dv">2</span>  <span class="co"># Total samples</span></span>
<span id="cb6-3"><a href="linear-regression.html#cb6-3" tabindex="-1"></a>numVar <span class="ot">&lt;-</span> <span class="dv">500</span>         <span class="co"># Number of predictors</span></span>
<span id="cb6-4"><a href="linear-regression.html#cb6-4" tabindex="-1"></a>sizBlo <span class="ot">&lt;-</span> <span class="dv">10</span>          <span class="co"># Block size for Correlation Matrix</span></span>
<span id="cb6-5"><a href="linear-regression.html#cb6-5" tabindex="-1"></a>numNze <span class="ot">&lt;-</span> <span class="dv">10</span>          <span class="co"># Number of nonzero coefficients</span></span>
<span id="cb6-6"><a href="linear-regression.html#cb6-6" tabindex="-1"></a>sigNoi <span class="ot">&lt;-</span> <span class="dv">1</span>           <span class="co"># Signal to Noise Ratio</span></span></code></pre></div>
<p>We have that:</p>
<ul>
<li><code>numTra</code>: The number of samples available for training.</li>
<li><code>numSam</code>: The total number of samples in the dataset, including both training and testing data. It is set to twice the number of training samples.</li>
<li><code>numVar</code>: The number of predictor variables (features) in the dataset.</li>
<li><code>sizBlo</code>: The size of blocks in the correlation structure of the design matrix. This determines how many variables are within each block.</li>
<li><code>numNze</code>: The number of <strong>nonzero</strong> coefficients in the true regression model. These correspond to the features that actually influence the response variable.</li>
<li><code>sigNoi</code>: The <strong>signal-to-noise ratio</strong>, which controls the relative strength of the true signal (nonzero coefficients) compared to the noise in the observations.</li>
</ul>
<hr />
</div>
<div id="simulation" class="section level3 hasAnchor" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Simulation<a href="linear-regression.html#simulation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="linear-regression.html#cb7-1" tabindex="-1"></a><span class="co"># X, b and y simulation</span></span>
<span id="cb7-2"><a href="linear-regression.html#cb7-2" tabindex="-1"></a>disCor <span class="ot">&lt;-</span> <span class="fl">0.5</span>  <span class="co"># Correlation decay</span></span>
<span id="cb7-3"><a href="linear-regression.html#cb7-3" tabindex="-1"></a>C      <span class="ot">&lt;-</span> <span class="fu">exp</span>(<span class="sc">-</span> disCor <span class="sc">*</span> <span class="fu">abs</span>(<span class="fu">rep</span>(<span class="dv">1</span>, sizBlo) <span class="sc">%*%</span> <span class="fu">t</span>(<span class="dv">1</span><span class="sc">:</span>sizBlo) <span class="sc">-</span> (<span class="dv">1</span><span class="sc">:</span>sizBlo) <span class="sc">%*%</span> <span class="fu">t</span>(<span class="fu">rep</span>(<span class="dv">1</span>, sizBlo))))</span>
<span id="cb7-4"><a href="linear-regression.html#cb7-4" tabindex="-1"></a>X      <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> numSam, <span class="at">ncol =</span> <span class="dv">0</span>)</span>
<span id="cb7-5"><a href="linear-regression.html#cb7-5" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(numVar <span class="sc">/</span> sizBlo)){</span>
<span id="cb7-6"><a href="linear-regression.html#cb7-6" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(X, <span class="fu">rmvnorm</span>(<span class="at">n =</span> numSam, <span class="at">sigma =</span> C))</span>
<span id="cb7-7"><a href="linear-regression.html#cb7-7" tabindex="-1"></a>}</span>
<span id="cb7-8"><a href="linear-regression.html#cb7-8" tabindex="-1"></a></span>
<span id="cb7-9"><a href="linear-regression.html#cb7-9" tabindex="-1"></a><span class="co"># Generates Coefficients and y</span></span>
<span id="cb7-10"><a href="linear-regression.html#cb7-10" tabindex="-1"></a>coeDecRat   <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb7-11"><a href="linear-regression.html#cb7-11" tabindex="-1"></a>b           <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, numVar)</span>
<span id="cb7-12"><a href="linear-regression.html#cb7-12" tabindex="-1"></a>b[<span class="dv">1</span><span class="sc">:</span>numNze] <span class="ot">&lt;-</span> <span class="fu">exp</span>(<span class="sc">-</span> (<span class="dv">1</span><span class="sc">:</span>numNze) <span class="sc">/</span> coeDecRat)</span>
<span id="cb7-13"><a href="linear-regression.html#cb7-13" tabindex="-1"></a>s2          <span class="ot">&lt;-</span> (<span class="fu">mean</span>(b[<span class="dv">1</span><span class="sc">:</span>numNze]) <span class="sc">/</span> sigNoi)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb7-14"><a href="linear-regression.html#cb7-14" tabindex="-1"></a>y           <span class="ot">&lt;-</span> X <span class="sc">%*%</span> b <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="at">n =</span> numSam, <span class="at">sd =</span> <span class="fu">sqrt</span>(s2))</span>
<span id="cb7-15"><a href="linear-regression.html#cb7-15" tabindex="-1"></a></span>
<span id="cb7-16"><a href="linear-regression.html#cb7-16" tabindex="-1"></a><span class="co"># Coefficient Values</span></span>
<span id="cb7-17"><a href="linear-regression.html#cb7-17" tabindex="-1"></a><span class="fu">print</span>(b[<span class="dv">1</span><span class="sc">:</span>(<span class="dv">2</span> <span class="sc">*</span> numNze)])</span></code></pre></div>
<pre><code>##  [1] 0.7788008 0.6065307 0.4723666 0.3678794 0.2865048 0.2231302 0.1737739 0.1353353 0.1053992 0.0820850 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000
## [17] 0.0000000 0.0000000 0.0000000 0.0000000</code></pre>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="linear-regression.html#cb9-1" tabindex="-1"></a><span class="co"># Response Variable Variance</span></span>
<span id="cb9-2"><a href="linear-regression.html#cb9-2" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">var</span>(y))</span></code></pre></div>
<pre><code>##          [,1]
## [1,] 4.276878</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="linear-regression.html#cb11-1" tabindex="-1"></a><span class="co"># Error Standard Deviation</span></span>
<span id="cb11-2"><a href="linear-regression.html#cb11-2" tabindex="-1"></a><span class="fu">print</span>(s2)</span></code></pre></div>
<pre><code>## [1] 0.1044457</code></pre>
<p>This chunk of code <strong>generates the design matrix <span class="math inline">\(\mathbf{X}\)</span>, regression coefficients <span class="math inline">\(\mathbf{b}\)</span>, and response variable <span class="math inline">\(\mathbf{y}\)</span></strong> for a simulated linear regression problem. It incorporates a <strong>correlated feature structure</strong>, meaning some predictors are related to each other.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="linear-regression.html#cb13-1" tabindex="-1"></a>disCor <span class="ot">&lt;-</span> <span class="fl">0.5</span>  <span class="co"># Correlation decay</span></span></code></pre></div>
<ul>
<li><strong><code>disCor</code></strong> controls how quickly correlations <strong>decay</strong> between predictors in the same block.</li>
<li>A <strong>higher value</strong> means stronger correlations between nearby variables, while a <strong>lower value</strong> leads to weaker correlations.</li>
</ul>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="linear-regression.html#cb14-1" tabindex="-1"></a>C <span class="ot">&lt;-</span> <span class="fu">exp</span>(<span class="sc">-</span> disCor <span class="sc">*</span> <span class="fu">abs</span>(<span class="fu">rep</span>(<span class="dv">1</span>, sizBlo) <span class="sc">%*%</span> <span class="fu">t</span>(<span class="dv">1</span><span class="sc">:</span>sizBlo) <span class="sc">-</span> (<span class="dv">1</span><span class="sc">:</span>sizBlo) <span class="sc">%*%</span> <span class="fu">t</span>(<span class="fu">rep</span>(<span class="dv">1</span>, sizBlo))))</span></code></pre></div>
<ul>
<li><strong><code>C</code></strong> is a <strong>block correlation matrix</strong> of size <code>sizBlo × sizBlo</code>.<br />
</li>
<li>The matrix is <strong>Toeplitz-like</strong>, meaning each predictor is <strong>correlated with its neighbors</strong>, and correlation <strong>decays exponentially</strong> as you move further away.<br />
</li>
<li>The function <code>exp(-disCor * distance)</code> ensures that correlations are strongest within blocks and weaken with distance.</li>
</ul>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="linear-regression.html#cb15-1" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> numSam, <span class="at">ncol =</span> <span class="dv">0</span>)</span>
<span id="cb15-2"><a href="linear-regression.html#cb15-2" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(numVar <span class="sc">/</span> sizBlo)){</span>
<span id="cb15-3"><a href="linear-regression.html#cb15-3" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(X, <span class="fu">rmvnorm</span>(<span class="at">n =</span> numSam, <span class="at">sigma =</span> C))</span>
<span id="cb15-4"><a href="linear-regression.html#cb15-4" tabindex="-1"></a>}</span></code></pre></div>
<ul>
<li><strong><code>X</code></strong> is initialized as an empty matrix.<br />
</li>
<li>The <strong>for-loop</strong> constructs <span class="math inline">\(\mathbf{X}\)</span> by generating <strong>blocks of correlated features</strong> using <code>rmvnorm()</code> (from <code>mvtnorm</code> package), which samples from a multivariate normal distribution with covariance matrix <code>C</code>.<br />
</li>
<li>Each block of predictors has size <code>sizBlo</code>, and the total number of blocks is <code>numVar / sizBlo</code>.</li>
</ul>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="linear-regression.html#cb16-1" tabindex="-1"></a>coeDecRat   <span class="ot">&lt;-</span> <span class="dv">4</span>  <span class="co"># Controls how fast coefficients decay</span></span>
<span id="cb16-2"><a href="linear-regression.html#cb16-2" tabindex="-1"></a>b           <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, numVar)  <span class="co"># Initializes all coefficients to zero</span></span>
<span id="cb16-3"><a href="linear-regression.html#cb16-3" tabindex="-1"></a>b[<span class="dv">1</span><span class="sc">:</span>numNze] <span class="ot">&lt;-</span> <span class="fu">exp</span>(<span class="sc">-</span>(<span class="dv">1</span><span class="sc">:</span>numNze) <span class="sc">/</span> coeDecRat)</span></code></pre></div>
<ul>
<li><strong><code>coeDecRat</code></strong> controls the decay rate of the <strong>true regression coefficients</strong>.<br />
</li>
<li>Only the <strong>first <code>numNze</code> coefficients</strong> are <strong>nonzero</strong>, and they follow an <strong>exponential decay pattern</strong>:<br />
<span class="math display">\[
b_j = \exp(-j / \text{coeDecRat}), \quad \text{for } j = 1, \dots, \text{numNze}
\]</span></li>
<li>This means <strong>important variables</strong> have larger effects, and their influence decreases exponentially.</li>
</ul>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="linear-regression.html#cb17-1" tabindex="-1"></a>sigNoi <span class="ot">&lt;-</span> <span class="fu">mean</span>(b[<span class="dv">1</span><span class="sc">:</span>numNze]) <span class="sc">/</span> <span class="dv">2</span></span></code></pre></div>
<ul>
<li><strong><code>sigNoi</code></strong> represents the <strong>signal-to-noise ratio</strong>:
<ul>
<li>Higher <code>sigNoi</code> → stronger signal, less noise.<br />
</li>
<li>Lower <code>sigNoi</code> → weaker signal, more noise.</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="linear-regression.html#cb18-1" tabindex="-1"></a>s2 <span class="ot">&lt;-</span> (<span class="fu">mean</span>(b[<span class="dv">1</span><span class="sc">:</span>numNze]) <span class="sc">/</span> sigNoi)<span class="sc">^</span><span class="dv">2</span></span></code></pre></div>
<ul>
<li><strong><code>s2</code></strong> is the variance of the noise term.<br />
</li>
<li>It ensures that the ratio of <strong>signal to noise</strong> remains controlled.</li>
</ul>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="linear-regression.html#cb19-1" tabindex="-1"></a>y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> b <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="at">n =</span> numSam, <span class="at">sd =</span> <span class="fu">sqrt</span>(s2))</span></code></pre></div>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="linear-regression.html#cb20-1" tabindex="-1"></a><span class="fu">print</span>(b[<span class="dv">1</span><span class="sc">:</span>(<span class="dv">2</span> <span class="sc">*</span> numNze)])  <span class="co"># Print the first few coefficients</span></span></code></pre></div>
<ul>
<li>Displays the first <code>2 * numNze</code> coefficients to confirm they follow the expected decay pattern and zero coefficients afterwards.</li>
</ul>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="linear-regression.html#cb21-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">var</span>(y))  <span class="co"># Print the variance of the response variable</span></span></code></pre></div>
<ul>
<li><strong>Checks the total variance of <span class="math inline">\(y\)</span></strong>, which is affected by both the signal and the noise.</li>
</ul>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="linear-regression.html#cb22-1" tabindex="-1"></a><span class="fu">print</span>(s2)  <span class="co"># Print error variance</span></span></code></pre></div>
<ul>
<li><strong>Prints the variance of the noise</strong> to verify the expected level of randomness in the response variable.</li>
</ul>
<table>
<colgroup>
<col width="44%" />
<col width="55%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Variable</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>disCor</code></td>
<td>Decay parameter controlling correlation between predictors.</td>
</tr>
<tr class="even">
<td><code>C</code></td>
<td>Correlation matrix defining <strong>block-wise correlated predictors</strong>.</td>
</tr>
<tr class="odd">
<td><code>X</code></td>
<td>Design matrix, with <strong>correlated predictor blocks</strong>.</td>
</tr>
<tr class="even">
<td><code>coeDecRat</code></td>
<td>Decay rate for <strong>true coefficients</strong>, controlling sparsity.</td>
</tr>
<tr class="odd">
<td><code>b</code></td>
<td>True regression coefficients, only <code>numNze</code> are <strong>nonzero</strong>.</td>
</tr>
<tr class="even">
<td><code>s2</code></td>
<td>Noise variance, ensuring controlled randomness in <span class="math inline">\(y\)</span>.</td>
</tr>
<tr class="odd">
<td><code>y</code></td>
<td>Response variable, generated from <span class="math inline">\(X\)</span> and <span class="math inline">\(b\)</span> with noise.</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="linear-regression.html#cb23-1" tabindex="-1"></a>traInd <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>numSam, numTra, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb23-2"><a href="linear-regression.html#cb23-2" tabindex="-1"></a>traY <span class="ot">&lt;-</span> y[traInd]</span>
<span id="cb23-3"><a href="linear-regression.html#cb23-3" tabindex="-1"></a>traX <span class="ot">&lt;-</span> X[traInd, ]</span>
<span id="cb23-4"><a href="linear-regression.html#cb23-4" tabindex="-1"></a>tesY <span class="ot">&lt;-</span> y[<span class="sc">-</span>traInd]</span>
<span id="cb23-5"><a href="linear-regression.html#cb23-5" tabindex="-1"></a>tesX <span class="ot">&lt;-</span> X[<span class="sc">-</span>traInd, ]</span></code></pre></div>
<p>This block <strong>splits the dataset</strong> into <strong>training and testing sets</strong>, which is crucial for evaluating model performance.</p>
<p>Step-by-Step Explanation:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="linear-regression.html#cb24-1" tabindex="-1"></a>traInd <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>numSam, numTra, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<ul>
<li><strong><code>sample(1:numSam, numTra, replace = FALSE)</code></strong>
<ul>
<li>Randomly selects <strong><code>numTra</code></strong> indices from <strong><code>1:numSam</code></strong> (the full dataset).<br />
</li>
<li><strong><code>replace = FALSE</code></strong> ensures that no index is selected more than once, maintaining a <strong>random subset</strong> without duplication.<br />
</li>
</ul></li>
<li><strong><code>traInd</code></strong> stores the <strong>indices of the training samples</strong>.</li>
</ul>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="linear-regression.html#cb25-1" tabindex="-1"></a>traY <span class="ot">&lt;-</span> y[traInd]</span>
<span id="cb25-2"><a href="linear-regression.html#cb25-2" tabindex="-1"></a>traX <span class="ot">&lt;-</span> X[traInd, ]</span></code></pre></div>
<ul>
<li><strong><code>traY</code></strong>: Subset of <strong><code>y</code></strong> containing only the selected training indices → <strong>training response values</strong>.<br />
</li>
<li><strong><code>traX</code></strong>: Corresponding rows from <strong>design matrix <code>X</code></strong> → <strong>training predictor values</strong>.</li>
</ul>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="linear-regression.html#cb26-1" tabindex="-1"></a>tesY <span class="ot">&lt;-</span> y[<span class="sc">-</span>traInd]</span>
<span id="cb26-2"><a href="linear-regression.html#cb26-2" tabindex="-1"></a>tesX <span class="ot">&lt;-</span> X[<span class="sc">-</span>traInd, ]</span></code></pre></div>
<ul>
<li><strong><code>tesY</code></strong>: Subset of <code>y</code> <strong>excluding training indices</strong> → <strong>test response values</strong>.<br />
</li>
<li><strong><code>tesX</code></strong>: Corresponding rows from <code>X</code> <strong>excluding training indices</strong> → <strong>test predictor values</strong>.</li>
</ul>
<p>Summary of Variables:</p>
<table>
<thead>
<tr class="header">
<th><strong>Variable</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>traInd</code></td>
<td>Indices randomly chosen for training.</td>
</tr>
<tr class="even">
<td><code>traX</code></td>
<td>Training set predictors (subset of <code>X</code>).</td>
</tr>
<tr class="odd">
<td><code>traY</code></td>
<td>Training set response values (subset of <code>y</code>).</td>
</tr>
<tr class="even">
<td><code>tesX</code></td>
<td>Test set predictors (remaining rows of <code>X</code>).</td>
</tr>
<tr class="odd">
<td><code>tesY</code></td>
<td>Test set response values (remaining rows of <code>y</code>).</td>
</tr>
</tbody>
</table>
<p>Why This Matters?
1. <strong>Prevents overfitting</strong>: The model is trained on <code>traX, traY</code> but evaluated on <code>tesX, tesY</code>, ensuring it generalizes to unseen data.<br />
2. <strong>Mimics real-world scenarios</strong>: In practice, models predict new data points, so testing on unseen data <strong>measures true performance</strong>.<br />
3. <strong>Ensures unbiased evaluation</strong>: A <strong>random split</strong> avoids bias in model evaluation, ensuring the test set represents different feature patterns.</p>
<hr />
</div>
<div id="ols" class="section level3 hasAnchor" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> OLS<a href="linear-regression.html#ols" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Then we first OLS</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="linear-regression.html#cb27-1" tabindex="-1"></a>outOLS                <span class="ot">&lt;-</span> <span class="fu">lm</span>(traY <span class="sc">~</span> traX <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb27-2"><a href="linear-regression.html#cb27-2" tabindex="-1"></a>coeOLS                <span class="ot">&lt;-</span> outOLS<span class="sc">$</span>coefficients</span>
<span id="cb27-3"><a href="linear-regression.html#cb27-3" tabindex="-1"></a>coeOLS[<span class="fu">is.na</span>(coeOLS)] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb27-4"><a href="linear-regression.html#cb27-4" tabindex="-1"></a>mseOLS                <span class="ot">&lt;-</span> <span class="fu">mean</span>((tesX <span class="sc">%*%</span> coeOLS <span class="sc">-</span> tesY)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb27-5"><a href="linear-regression.html#cb27-5" tabindex="-1"></a>pr2OLS                <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> mseOLS <span class="sc">/</span> <span class="fu">var</span>(tesY)</span></code></pre></div>
<p>This block <strong>fits an OLS regression model</strong>, extracts the estimated coefficients, and evaluates the model’s performance on the test set.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="linear-regression.html#cb28-1" tabindex="-1"></a>outOLS <span class="ot">&lt;-</span> <span class="fu">lm</span>(traY <span class="sc">~</span> traX <span class="sc">-</span> <span class="dv">1</span>)</span></code></pre></div>
<ul>
<li><strong><code>lm(traY ~ traX - 1)</code></strong>:
<ul>
<li>Fits a <strong>linear model</strong> where <code>traY</code> is the response variable and <code>traX</code> is the design matrix.<br />
</li>
<li>The <strong><code>-1</code></strong> removes the default intercept, ensuring that all coefficients correspond directly to the predictors in <code>traX</code>.<br />
</li>
</ul></li>
<li><strong><code>outOLS</code></strong> stores the fitted model, including coefficients and residuals.</li>
</ul>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="linear-regression.html#cb29-1" tabindex="-1"></a>coeOLS <span class="ot">&lt;-</span> outOLS<span class="sc">$</span>coefficients</span></code></pre></div>
<ul>
<li><strong>Retrieves the estimated regression coefficients</strong> from the fitted model.</li>
</ul>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="linear-regression.html#cb30-1" tabindex="-1"></a>coeOLS[<span class="fu">is.na</span>(coeOLS)] <span class="ot">&lt;-</span> <span class="dv">0</span></span></code></pre></div>
<ul>
<li><strong>Replaces any <code>NA</code> values with <code>0</code></strong> (this can happen when certain predictors are collinear, leading to undefined coefficients).</li>
</ul>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="linear-regression.html#cb31-1" tabindex="-1"></a>mseOLS <span class="ot">&lt;-</span> <span class="fu">mean</span>((tesX <span class="sc">%*%</span> coeOLS <span class="sc">-</span> tesY)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<ul>
<li>Computes the <strong>Mean Squared Error (MSE)</strong> on the test set:
<span class="math display">\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} ( \hat{y}_i - y_i )^2
\]</span></li>
<li><strong><code>tesX %*% coeOLS</code></strong> calculates predicted values <span class="math inline">\(\hat{y}\)</span> on the test data.</li>
</ul>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="linear-regression.html#cb32-1" tabindex="-1"></a>pr2OLS <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> mseOLS <span class="sc">/</span> <span class="fu">var</span>(tesY)</span></code></pre></div>
<ul>
<li>Computes the <strong>predictive R-squared</strong> <span class="math inline">\(R^2\)</span>:
<span class="math display">\[
R^2 = 1 - \frac{\text{MSE}}{\text{Var}(\text{Test } y)}
\]</span></li>
<li><strong>Interpretation</strong>:
<ul>
<li><strong><span class="math inline">\(R^2 \approx 1\)</span></strong> → Model explains most of the variance in <code>tesY</code> (good performance).</li>
<li><strong><span class="math inline">\(R^2 \approx 0\)</span></strong> → Model performs as poorly as simply predicting the mean.</li>
<li><strong><span class="math inline">\(R^2 &lt; 0\)</span></strong> → Model performs <strong>worse than a constant predictor</strong> (severe overfitting or poor generalization).</li>
</ul></li>
</ul>
<p>Summary of Key Variables:</p>
<table>
<colgroup>
<col width="44%" />
<col width="55%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Variable</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>outOLS</code></td>
<td>OLS model fitted to the training data.</td>
</tr>
<tr class="even">
<td><code>coeOLS</code></td>
<td>Estimated coefficients from the OLS model.</td>
</tr>
<tr class="odd">
<td><code>mseOLS</code></td>
<td>Mean Squared Error on the test set (lower is better).</td>
</tr>
<tr class="even">
<td><code>pr2OLS</code></td>
<td>Predictive <span class="math inline">\(R^2\)</span>, measuring how well the model generalizes to new data.</td>
</tr>
</tbody>
</table>
<p>Why This Matters?</p>
<ul>
<li><strong>OLS assumes no multicollinearity</strong> → Since <code>traX</code> contains <strong>correlated predictors</strong>, OLS may suffer from <strong>unstable coefficient estimates</strong>.</li>
<li><strong>Regularization (e.g., Ridge, Lasso) is often needed</strong> to improve performance when predictors are highly correlated.</li>
<li><strong>Comparison to Ridge/Lasso/Bayesian models</strong> will highlight how regularization techniques handle multicollinearity <strong>better than OLS</strong>.</li>
</ul>
<hr />
</div>
<div id="ridge-regression-1" class="section level3 hasAnchor" number="4.3.4">
<h3><span class="header-section-number">4.3.4</span> Ridge Regression<a href="linear-regression.html#ridge-regression-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="linear-regression.html#cb33-1" tabindex="-1"></a>cvRID  <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(<span class="at">x =</span> traX, <span class="at">y =</span> traY, <span class="at">alpha =</span> <span class="dv">0</span>)</span>
<span id="cb33-2"><a href="linear-regression.html#cb33-2" tabindex="-1"></a>outRID <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(<span class="at">x =</span> traX, <span class="at">y =</span> traY, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> cvRID<span class="sc">$</span>lambda.min)</span>
<span id="cb33-3"><a href="linear-regression.html#cb33-3" tabindex="-1"></a>preRID <span class="ot">&lt;-</span> <span class="fu">predict</span>(outRID, <span class="at">newx =</span> tesX)</span>
<span id="cb33-4"><a href="linear-regression.html#cb33-4" tabindex="-1"></a>mseRID <span class="ot">&lt;-</span> <span class="fu">mean</span>((preRID <span class="sc">-</span> tesY)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb33-5"><a href="linear-regression.html#cb33-5" tabindex="-1"></a>pr2RID <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> mseRID <span class="sc">/</span> <span class="fu">var</span>(tesY)</span></code></pre></div>
<p>This block fits a <strong>Ridge Regression model</strong>, selects an optimal regularization parameter <span class="math inline">\(\lambda\)</span> via cross-validation, and evaluates the model’s performance on the test set.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="linear-regression.html#cb34-1" tabindex="-1"></a>cvRID <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(<span class="at">x =</span> traX, <span class="at">y =</span> traY, <span class="at">alpha =</span> <span class="dv">0</span>)</span></code></pre></div>
<ul>
<li><strong><code>cv.glmnet()</code></strong> performs <strong>cross-validation</strong> to find the best <span class="math inline">\(\lambda\)</span> (regularization strength).<br />
</li>
<li><strong><code>alpha = 0</code></strong> specifies <strong>Ridge Regression</strong> (if <code>alpha = 1</code>, it would perform Lasso instead).<br />
</li>
<li>The function:
<ul>
<li>Splits the training data into <strong>folds</strong>.</li>
<li>Trains Ridge regression models with different <span class="math inline">\(\lambda\)</span> values.</li>
<li>Selects the best <span class="math inline">\(\lambda\)</span> by <strong>minimizing cross-validated MSE</strong>.</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="linear-regression.html#cb35-1" tabindex="-1"></a>outRID <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(<span class="at">x =</span> traX, <span class="at">y =</span> traY, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> cvRID<span class="sc">$</span>lambda.min)</span></code></pre></div>
<ul>
<li><strong>Fits a Ridge Regression model</strong> using the best <span class="math inline">\(\lambda\)</span> found from <code>cv.glmnet()</code>.</li>
<li><strong>Key arguments</strong>:
<ul>
<li><code>x = traX</code>, <code>y = traY</code>: Training data.</li>
<li><code>alpha = 0</code>: Specifies Ridge Regression.</li>
<li><code>lambda = cvRID$lambda.min</code>: Uses the optimal <span class="math inline">\(\lambda\)</span> from cross-validation.</li>
</ul></li>
<li><strong>Effect of Ridge Regularization</strong>:
<ul>
<li>Shrinks <strong>all</strong> coefficients (unlike Lasso, which can set some to zero).</li>
<li>Reduces variance by <strong>stabilizing estimates</strong>, especially in <strong>highly correlated predictor settings</strong>.</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="linear-regression.html#cb36-1" tabindex="-1"></a>preRID <span class="ot">&lt;-</span> <span class="fu">predict</span>(outRID, <span class="at">newx =</span> tesX)</span></code></pre></div>
<ul>
<li>Uses the trained <strong>Ridge model</strong> (<code>outRID</code>) to predict response values on the test set (<code>tesX</code>).</li>
</ul>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="linear-regression.html#cb37-1" tabindex="-1"></a>mseRID <span class="ot">&lt;-</span> <span class="fu">mean</span>((preRID <span class="sc">-</span> tesY)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<ul>
<li>Computes the <strong>Mean Squared Error (MSE)</strong> on the test data:
<span class="math display">\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} ( \hat{y}_i - y_i )^2
\]</span></li>
<li><strong>Lower MSE</strong> indicates better predictive performance.</li>
</ul>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="linear-regression.html#cb38-1" tabindex="-1"></a>pr2RID <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> mseRID <span class="sc">/</span> <span class="fu">var</span>(tesY)</span></code></pre></div>
<ul>
<li>Computes the <strong>predictive <span class="math inline">\(R^2\)</span></strong>:
<span class="math display">\[
R^2 = 1 - \frac{\text{MSE}}{\text{Var}(\text{Test } y)}
\]</span></li>
<li><strong>Interpretation</strong>:
<ul>
<li><strong><span class="math inline">\(R^2 \approx 1\)</span></strong> → Model explains most of the variance (good performance).</li>
<li><strong><span class="math inline">\(R^2 \approx 0\)</span></strong> → Model performs similarly to a constant predictor.</li>
<li><strong><span class="math inline">\(R^2 &lt; 0\)</span></strong> → Model performs worse than predicting the mean (poor generalization).</li>
</ul></li>
</ul>
<p>Summary of Key Variables:</p>
<table>
<thead>
<tr class="header">
<th><strong>Variable</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>cvRID</code></td>
<td>Performs cross-validation to select optimal <span class="math inline">\(\lambda\)</span>.</td>
</tr>
<tr class="even">
<td><code>outRID</code></td>
<td>Trained Ridge Regression model with best <span class="math inline">\(\lambda\)</span>.</td>
</tr>
<tr class="odd">
<td><code>preRID</code></td>
<td>Predicted values for the test set.</td>
</tr>
<tr class="even">
<td><code>mseRID</code></td>
<td>Mean Squared Error on the test set (lower is better).</td>
</tr>
<tr class="odd">
<td><code>pr2RID</code></td>
<td>Predictive <span class="math inline">\(R^2\)</span>, measuring model generalization.</td>
</tr>
</tbody>
</table>
<p>Why Use Ridge Regression?</p>
<ul>
<li><strong>Handles multicollinearity</strong>: When predictors are highly correlated, OLS estimates become unstable. Ridge stabilizes them.</li>
<li><strong>Reduces overfitting</strong>: Adding a penalty term <span class="math inline">\(\lambda \|\boldsymbol{\beta}\|_2^2\)</span> discourages overly large coefficients.</li>
<li><strong>Retains all features</strong>: Unlike Lasso, Ridge does <strong>not</strong> force coefficients to be exactly zero.</li>
</ul>
<hr />
</div>
<div id="lasso" class="section level3 hasAnchor" number="4.3.5">
<h3><span class="header-section-number">4.3.5</span> Lasso<a href="linear-regression.html#lasso" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="linear-regression.html#cb39-1" tabindex="-1"></a>cvLAS  <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(<span class="at">x =</span> traX, <span class="at">y =</span> traY, <span class="at">alpha =</span> <span class="dv">1</span>)</span>
<span id="cb39-2"><a href="linear-regression.html#cb39-2" tabindex="-1"></a><span class="fu">plot</span>(cvLAS)</span></code></pre></div>
<p><img src="_main_files/figure-html/lr-lasso-1.png" width="672" /></p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="linear-regression.html#cb40-1" tabindex="-1"></a><span class="fu">plot</span>(cvLAS<span class="sc">$</span>glmnet.fit)</span></code></pre></div>
<p><img src="_main_files/figure-html/lr-lasso-2.png" width="672" /></p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="linear-regression.html#cb41-1" tabindex="-1"></a>outLAS <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(<span class="at">x =</span> traX, <span class="at">y =</span> traY, <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">lambda =</span> cvLAS<span class="sc">$</span>lambda.min)</span>
<span id="cb41-2"><a href="linear-regression.html#cb41-2" tabindex="-1"></a>preLAS <span class="ot">&lt;-</span> <span class="fu">predict</span>(outLAS, <span class="at">newx =</span> tesX)</span>
<span id="cb41-3"><a href="linear-regression.html#cb41-3" tabindex="-1"></a>mseLAS <span class="ot">&lt;-</span> <span class="fu">mean</span>((preLAS <span class="sc">-</span> tesY)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb41-4"><a href="linear-regression.html#cb41-4" tabindex="-1"></a>pr2LAS <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> mseLAS <span class="sc">/</span> <span class="fu">var</span>(tesY)</span></code></pre></div>
<p>This block fits a <strong>Lasso Regression model</strong>, selects an optimal regularization parameter <span class="math inline">\(\lambda\)</span> via cross-validation, and evaluates the model’s performance on the test set.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="linear-regression.html#cb42-1" tabindex="-1"></a>cvLAS <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(<span class="at">x =</span> traX, <span class="at">y =</span> traY, <span class="at">alpha =</span> <span class="dv">1</span>)</span></code></pre></div>
<ul>
<li><strong><code>cv.glmnet()</code></strong> performs <strong>cross-validation</strong> to find the best <span class="math inline">\(\lambda\)</span> (regularization strength).<br />
</li>
<li><strong><code>alpha = 1</code></strong> specifies <strong>Lasso Regression</strong> (if <code>alpha = 0</code>, it would perform Ridge Regression).<br />
</li>
<li>The function:
<ul>
<li>Splits the training data into <strong>folds</strong>.</li>
<li>Trains Lasso models with different <span class="math inline">\(\lambda\)</span> values.</li>
<li>Selects the best <span class="math inline">\(\lambda\)</span> by <strong>minimizing cross-validated MSE</strong>.</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="linear-regression.html#cb43-1" tabindex="-1"></a><span class="fu">plot</span>(cvLAS)</span>
<span id="cb43-2"><a href="linear-regression.html#cb43-2" tabindex="-1"></a><span class="fu">plot</span>(cvLAS<span class="sc">$</span>glmnet.fit)</span></code></pre></div>
<ul>
<li><strong><code>plot(cvLAS)</code></strong>:
<ul>
<li>Plots the cross-validation error for different values of <span class="math inline">\(\lambda\)</span>.</li>
<li>Shows the selected <span class="math inline">\(\lambda\)</span> (the one that minimizes error).</li>
</ul></li>
<li><strong><code>plot(cvLAS$glmnet.fit)</code></strong>:
<ul>
<li>Plots the <strong>Lasso path</strong>: how coefficients change as <span class="math inline">\(\lambda\)</span> increases.</li>
<li>Higher <span class="math inline">\(\lambda\)</span> values shrink more coefficients to <strong>zero</strong>, performing feature selection.</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="linear-regression.html#cb44-1" tabindex="-1"></a>outLAS <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(<span class="at">x =</span> traX, <span class="at">y =</span> traY, <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">lambda =</span> cvLAS<span class="sc">$</span>lambda.min)</span></code></pre></div>
<ul>
<li><strong>Fits a Lasso Regression model</strong> using the best <span class="math inline">\(\lambda\)</span> found from <code>cv.glmnet()</code>.</li>
<li><strong>Key arguments</strong>:
<ul>
<li><code>x = traX</code>, <code>y = traY</code>: Training data.</li>
<li><code>alpha = 1</code>: Specifies <strong>Lasso Regression</strong>.</li>
<li><code>lambda = cvLAS$lambda.min</code>: Uses the optimal <span class="math inline">\(\lambda\)</span> from cross-validation.</li>
</ul></li>
<li><strong>Effect of Lasso Regularization</strong>:
<ul>
<li>Shrinks some <strong>coefficients to exactly zero</strong>, performing <strong>automatic feature selection</strong>.</li>
<li>Helps when <strong>only a subset of predictors is relevant</strong>.</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="linear-regression.html#cb45-1" tabindex="-1"></a>preLAS <span class="ot">&lt;-</span> <span class="fu">predict</span>(outLAS, <span class="at">newx =</span> tesX)</span></code></pre></div>
<ul>
<li>Uses the trained <strong>Lasso model</strong> (<code>outLAS</code>) to predict response values on the test set (<code>tesX</code>).</li>
</ul>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="linear-regression.html#cb46-1" tabindex="-1"></a>mseLAS <span class="ot">&lt;-</span> <span class="fu">mean</span>((preLAS <span class="sc">-</span> tesY)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<ul>
<li>Computes the <strong>Mean Squared Error (MSE)</strong> on the test data:
<span class="math display">\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} ( \hat{y}_i - y_i )^2
\]</span></li>
<li><strong>Lower MSE</strong> indicates better predictive performance.</li>
</ul>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="linear-regression.html#cb47-1" tabindex="-1"></a>pr2LAS <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> mseLAS <span class="sc">/</span> <span class="fu">var</span>(tesY)</span></code></pre></div>
<ul>
<li>Computes the <strong>predictive <span class="math inline">\(R^2\)</span></strong>:
<span class="math display">\[
R^2 = 1 - \frac{\text{MSE}}{\text{Var}(\text{Test } y)}
\]</span></li>
<li><strong>Interpretation</strong>:
<ul>
<li><strong><span class="math inline">\(R^2 \approx 1\)</span></strong> → Model explains most of the variance (good performance).</li>
<li><strong><span class="math inline">\(R^2 \approx 0\)</span></strong> → Model performs similarly to a constant predictor.</li>
<li><strong><span class="math inline">\(R^2 &lt; 0\)</span></strong> → Model performs worse than predicting the mean (poor generalization).</li>
</ul></li>
</ul>
<hr />
<p>Summary of Key Variables:</p>
<table>
<thead>
<tr class="header">
<th><strong>Variable</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>cvLAS</code></td>
<td>Performs cross-validation to select optimal <span class="math inline">\(\lambda\)</span>.</td>
</tr>
<tr class="even">
<td><code>outLAS</code></td>
<td>Trained Lasso Regression model with best <span class="math inline">\(\lambda\)</span>.</td>
</tr>
<tr class="odd">
<td><code>preLAS</code></td>
<td>Predicted values for the test set.</td>
</tr>
<tr class="even">
<td><code>mseLAS</code></td>
<td>Mean Squared Error on the test set (lower is better).</td>
</tr>
<tr class="odd">
<td><code>pr2LAS</code></td>
<td>Predictive <span class="math inline">\(R^2\)</span>, measuring model generalization.</td>
</tr>
</tbody>
</table>
<p>Why Use Lasso Regression?</p>
<ul>
<li><strong>Feature Selection</strong>: Unlike Ridge, Lasso can shrink some coefficients <strong>exactly to zero</strong>, removing irrelevant predictors.</li>
<li><strong>Handles high-dimensional data</strong>: When <span class="math inline">\(p &gt; n\)</span>, Lasso helps by selecting the most relevant features.</li>
<li><strong>Interpretable models</strong>: Because some coefficients are set to zero, Lasso produces simpler models.</li>
</ul>
<hr />
</div>
<div id="basic-bayesian-regression" class="section level3 hasAnchor" number="4.3.6">
<h3><span class="header-section-number">4.3.6</span> Basic Bayesian Regression<a href="linear-regression.html#basic-bayesian-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="linear-regression.html#cb48-1" tabindex="-1"></a>outBAY <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(<span class="at">n =</span> <span class="dv">2000</span>, <span class="at">mean =</span> outRID<span class="sc">$</span>beta, <span class="at">sigma =</span> s2 <span class="sc">*</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X <span class="sc">+</span> cvRID<span class="sc">$</span>lambda.min <span class="sc">*</span> <span class="fu">diag</span>(numVar)))</span>
<span id="cb48-2"><a href="linear-regression.html#cb48-2" tabindex="-1"></a>coeBAY <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> outBAY, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> median)</span>
<span id="cb48-3"><a href="linear-regression.html#cb48-3" tabindex="-1"></a>preBAY <span class="ot">&lt;-</span> tesX <span class="sc">%*%</span> coeBAY</span>
<span id="cb48-4"><a href="linear-regression.html#cb48-4" tabindex="-1"></a>mseBAY <span class="ot">&lt;-</span> <span class="fu">mean</span>((preBAY <span class="sc">-</span> tesY)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb48-5"><a href="linear-regression.html#cb48-5" tabindex="-1"></a>pr2BAY <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> mseBAY <span class="sc">/</span> <span class="fu">var</span>(tesY)</span></code></pre></div>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="linear-regression.html#cb49-1" tabindex="-1"></a>outBAY <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(<span class="at">n =</span> <span class="dv">2000</span>, </span>
<span id="cb49-2"><a href="linear-regression.html#cb49-2" tabindex="-1"></a>                  <span class="at">mean =</span> outRID<span class="sc">$</span>beta, </span>
<span id="cb49-3"><a href="linear-regression.html#cb49-3" tabindex="-1"></a>                  <span class="at">sigma =</span> s2 <span class="sc">*</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X <span class="sc">+</span> cvRID<span class="sc">$</span>lambda.min <span class="sc">*</span> <span class="fu">diag</span>(numVar)))</span></code></pre></div>
<ul>
<li><strong><code>rmvnorm()</code></strong> generates <strong>2,000 samples</strong> from a <strong>multivariate normal distribution</strong>, approximating the posterior distribution of the regression coefficients <span class="math inline">\(\boldsymbol{\beta}\)</span>.</li>
<li><strong>Posterior Mean</strong>: Uses the Ridge Regression solution (<code>outRID$beta</code>) as the mean of the distribution.</li>
<li><strong>Posterior Covariance</strong>:<br />
<span class="math display">\[
\sigma^2 (\mathbf{X}&#39; \mathbf{X} + \lambda \mathbf{I})^{-1}
\]</span>
<ul>
<li><code>s2 * solve(t(X) %*% X + cvRID$lambda.min * diag(numVar))</code> is the <strong>posterior covariance matrix</strong>, incorporating both the observed data and the prior information.</li>
<li>The <strong><span class="math inline">\(\lambda \mathbf{I}\)</span> term</strong> (from Ridge) ensures <strong>stability</strong> even if <span class="math inline">\(\mathbf{X}&#39; \mathbf{X}\)</span> is singular.</li>
</ul></li>
</ul>
<p>📌 <strong>Key Idea</strong>: Unlike Ridge, which gives a <strong>point estimate</strong> for <span class="math inline">\(\boldsymbol{\beta}\)</span>, Bayesian Ridge treats <span class="math inline">\(\boldsymbol{\beta}\)</span> as a <strong>random variable</strong> and samples from its posterior distribution.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="linear-regression.html#cb50-1" tabindex="-1"></a>coeBAY <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> outBAY, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> median)</span></code></pre></div>
<ul>
<li>For each coefficient <span class="math inline">\(\beta_j\)</span>, we <strong>take the median</strong> across the 2,000 posterior samples.</li>
<li>This is a <strong>Bayesian point estimate</strong>, similar to the Ridge solution but incorporating <strong>posterior uncertainty</strong>.</li>
</ul>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="linear-regression.html#cb51-1" tabindex="-1"></a>preBAY <span class="ot">&lt;-</span> tesX <span class="sc">%*%</span> coeBAY</span></code></pre></div>
<ul>
<li>Uses the <strong>Bayesian posterior median coefficients</strong> to predict <code>tesY</code>.</li>
</ul>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="linear-regression.html#cb52-1" tabindex="-1"></a>mseBAY <span class="ot">&lt;-</span> <span class="fu">mean</span>((preBAY <span class="sc">-</span> tesY)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<ul>
<li>Computes the <strong>Mean Squared Error (MSE)</strong> on the test data:
<span class="math display">\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} ( \hat{y}_i - y_i )^2
\]</span></li>
<li><strong>Lower MSE</strong> indicates better predictive performance.</li>
</ul>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="linear-regression.html#cb53-1" tabindex="-1"></a>pr2BAY <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> mseBAY <span class="sc">/</span> <span class="fu">var</span>(tesY)</span></code></pre></div>
<ul>
<li>Computes the <strong>predictive <span class="math inline">\(R^2\)</span></strong>:
<span class="math display">\[
R^2 = 1 - \frac{\text{MSE}}{\text{Var}(\text{Test } y)}
\]</span></li>
<li><strong>Interpretation</strong>:
<ul>
<li><strong><span class="math inline">\(R^2 \approx 1\)</span></strong> → Model explains most of the variance (good performance).</li>
<li><strong><span class="math inline">\(R^2 \approx 0\)</span></strong> → Model performs similarly to a constant predictor.</li>
<li><strong><span class="math inline">\(R^2 &lt; 0\)</span></strong> → Model performs worse than predicting the mean (poor generalization).</li>
</ul></li>
</ul>
<p>Summary of Key Variables:</p>
<table>
<colgroup>
<col width="44%" />
<col width="55%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Variable</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>outBAY</code></td>
<td>Posterior samples of <span class="math inline">\(\boldsymbol{\beta}\)</span> from a <strong>Bayesian Ridge</strong> model.</td>
</tr>
<tr class="even">
<td><code>coeBAY</code></td>
<td>Posterior <strong>median</strong> coefficients (Bayesian point estimate).</td>
</tr>
<tr class="odd">
<td><code>preBAY</code></td>
<td>Predicted values for the test set.</td>
</tr>
<tr class="even">
<td><code>mseBAY</code></td>
<td>Mean Squared Error on the test set (lower is better).</td>
</tr>
<tr class="odd">
<td><code>pr2BAY</code></td>
<td>Predictive <span class="math inline">\(R^2\)</span>, measuring model generalization.</td>
</tr>
</tbody>
</table>
<p>Why Use Bayesian Ridge Regression?</p>
<ul>
<li><strong>Incorporates uncertainty</strong>: Instead of a single estimate, we get a <strong>posterior distribution</strong> over <span class="math inline">\(\boldsymbol{\beta}\)</span>.</li>
<li><strong>Handles multicollinearity</strong>: Like Ridge, the prior <strong>shrinks</strong> coefficients to prevent instability.</li>
<li><strong>Flexibility</strong>: Bayesian methods allow incorporating <strong>informative priors</strong> when prior knowledge is available.</li>
</ul>
</div>
<div id="bayesian-lasso" class="section level3 hasAnchor" number="4.3.7">
<h3><span class="header-section-number">4.3.7</span> Bayesian Lasso<a href="linear-regression.html#bayesian-lasso" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="linear-regression.html#cb54-1" tabindex="-1"></a>outBLA <span class="ot">&lt;-</span> <span class="fu">blasso</span>(<span class="at">X =</span> traX, <span class="at">y =</span> traY, <span class="at">T =</span> <span class="dv">500</span>)</span></code></pre></div>
<pre><code>## t=100, m=215
## t=200, m=209
## t=300, m=226
## t=400, m=203</code></pre>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="linear-regression.html#cb56-1" tabindex="-1"></a>coeBLA <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> outBLA<span class="sc">$</span>beta, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> median)</span>
<span id="cb56-2"><a href="linear-regression.html#cb56-2" tabindex="-1"></a>preBLA <span class="ot">&lt;-</span> tesX <span class="sc">%*%</span> coeBLA</span>
<span id="cb56-3"><a href="linear-regression.html#cb56-3" tabindex="-1"></a>mseBLA <span class="ot">&lt;-</span> <span class="fu">mean</span>((preBLA <span class="sc">-</span> tesY)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb56-4"><a href="linear-regression.html#cb56-4" tabindex="-1"></a>pr2BLA <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> mseBLA <span class="sc">/</span> <span class="fu">var</span>(tesY)</span></code></pre></div>
<p>This block implements a <strong>Bayesian version of Lasso Regression</strong>, where instead of solving a convex optimization problem (like traditional Lasso), it <strong>samples from the posterior distribution</strong> of the regression coefficients using a Bayesian framework.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="linear-regression.html#cb57-1" tabindex="-1"></a>outBLA <span class="ot">&lt;-</span> <span class="fu">blasso</span>(<span class="at">X =</span> traX, <span class="at">y =</span> traY, <span class="at">T =</span> <span class="dv">50</span>)</span></code></pre></div>
<ul>
<li><strong><code>blasso()</code></strong> performs <strong>Bayesian Lasso regression</strong>, which introduces a <strong>Laplace (double-exponential) prior</strong> on <span class="math inline">\(\boldsymbol{\beta}\)</span>.</li>
<li><strong>Key argument</strong>:
<ul>
<li><code>T = 50</code>: Runs <strong>50 iterations</strong> of a <strong>Markov Chain Monte Carlo (MCMC) sampler</strong> to draw posterior samples of <span class="math inline">\(\boldsymbol{\beta}\)</span>.</li>
</ul></li>
</ul>
<p>📌 <strong>Bayesian Lasso vs. Traditional Lasso</strong><br />
- <strong>Traditional Lasso</strong>: Estimates <span class="math inline">\(\boldsymbol{\beta}\)</span> by <strong>solving an optimization problem</strong> with an <span class="math inline">\(\ell_1\)</span>-penalty.
- <strong>Bayesian Lasso</strong>: Places a <strong>Laplace prior</strong> on <span class="math inline">\(\boldsymbol{\beta}\)</span>, then <strong>samples</strong> from the posterior.
- Unlike Traditional Lasso, the Bayesian Lasso doesn’t perform exact variable selection.</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="linear-regression.html#cb58-1" tabindex="-1"></a>coeBLA <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> outBLA<span class="sc">$</span>beta, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> median)</span></code></pre></div>
<ul>
<li><strong>Takes the median</strong> of the sampled coefficients across the 50 iterations.</li>
<li>This provides a <strong>point estimate</strong> similar to the Lasso solution but incorporates <strong>posterior uncertainty</strong>.</li>
</ul>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="linear-regression.html#cb59-1" tabindex="-1"></a>preBLA <span class="ot">&lt;-</span> tesX <span class="sc">%*%</span> coeBLA</span></code></pre></div>
<ul>
<li>Uses the <strong>posterior median coefficients</strong> to predict response values on the test set.</li>
</ul>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="linear-regression.html#cb60-1" tabindex="-1"></a>mseBLA <span class="ot">&lt;-</span> <span class="fu">mean</span>((preBLA <span class="sc">-</span> tesY)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<ul>
<li>Computes the <strong>Mean Squared Error (MSE)</strong> on the test data:
<span class="math display">\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} ( \hat{y}_i - y_i )^2
\]</span></li>
<li><strong>Lower MSE</strong> indicates better predictive performance.</li>
</ul>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="linear-regression.html#cb61-1" tabindex="-1"></a>pr2BLA <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> mseBLA <span class="sc">/</span> <span class="fu">var</span>(tesY)</span></code></pre></div>
<ul>
<li>Computes the <strong>predictive <span class="math inline">\(R^2\)</span></strong>:
<span class="math display">\[
R^2 = 1 - \frac{\text{MSE}}{\text{Var}(\text{Test } y)}
\]</span></li>
<li><strong>Interpretation</strong>:
<ul>
<li><strong><span class="math inline">\(R^2 \approx 1\)</span></strong> → Model explains most of the variance (good performance).</li>
<li><strong><span class="math inline">\(R^2 \approx 0\)</span></strong> → Model performs similarly to a constant predictor.</li>
<li><strong><span class="math inline">\(R^2 &lt; 0\)</span></strong> → Model performs worse than predicting the mean (poor generalization).</li>
</ul></li>
</ul>
<p>Summary of Key Variables:</p>
<table>
<colgroup>
<col width="44%" />
<col width="55%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Variable</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>outBLA</code></td>
<td>Posterior samples of <span class="math inline">\(\boldsymbol{\beta}\)</span> from the <strong>Bayesian Lasso</strong> model.</td>
</tr>
<tr class="even">
<td><code>coeBLA</code></td>
<td>Posterior <strong>median</strong> coefficients (Bayesian point estimate).</td>
</tr>
<tr class="odd">
<td><code>preBLA</code></td>
<td>Predicted values for the test set.</td>
</tr>
<tr class="even">
<td><code>mseBLA</code></td>
<td>Mean Squared Error on the test set (lower is better).</td>
</tr>
<tr class="odd">
<td><code>pr2BLA</code></td>
<td>Predictive <span class="math inline">\(R^2\)</span>, measuring model generalization.</td>
</tr>
</tbody>
</table>
<p>Why Use Bayesian Lasso Regression?</p>
<ul>
<li><strong>Incorporates Uncertainty</strong>: Unlike standard Lasso, it provides <strong>posterior distributions</strong> instead of just point estimates.</li>
<li><strong>More Robust to Noise</strong>: Bayesian priors smooth out <strong>overfitting issues</strong> that can occur in traditional Lasso.</li>
</ul>
<hr />
</div>
<div id="bayesian-horseshoe-prior" class="section level3 hasAnchor" number="4.3.8">
<h3><span class="header-section-number">4.3.8</span> Bayesian Horseshoe Prior<a href="linear-regression.html#bayesian-horseshoe-prior" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="linear-regression.html#cb62-1" tabindex="-1"></a>outHOR <span class="ot">&lt;-</span> <span class="fu">horseshoe_sampler</span>(<span class="at">X =</span> X, <span class="at">y =</span> y, <span class="at">S =</span> <span class="dv">500</span>)</span></code></pre></div>
<pre><code>##   |                                                                                |                                                                        |   0%  |                                                                                |                                                                        |   1%  |                                                                                |=                                                                       |   1%  |                                                                                |=                                                                       |   2%  |                                                                                |==                                                                      |   2%  |                                                                                |==                                                                      |   3%  |                                                                                |===                                                                     |   4%  |                                                                                |===                                                                     |   5%  |                                                                                |====                                                                    |   5%  |                                                                                |====                                                                    |   6%  |                                                                                |=====                                                                   |   6%  |                                                                                |=====                                                                   |   7%  |                                                                                |=====                                                                   |   8%  |                                                                                |======                                                                  |   8%  |                                                                                |======                                                                  |   9%  |                                                                                |=======                                                                 |   9%  |                                                                                |=======                                                                 |  10%  |                                                                                |========                                                                |  11%  |                                                                                |========                                                                |  12%  |                                                                                |=========                                                               |  12%  |                                                                                |=========                                                               |  13%  |                                                                                |==========                                                              |  13%  |                                                                                |==========                                                              |  14%  |                                                                                |===========                                                             |  15%  |                                                                                |===========                                                             |  16%  |                                                                                |============                                                            |  16%  |                                                                                |============                                                            |  17%  |                                                                                |=============                                                           |  17%  |                                                                                |=============                                                           |  18%  |                                                                                |=============                                                           |  19%  |                                                                                |==============                                                          |  19%  |                                                                                |==============                                                          |  20%  |                                                                                |===============                                                         |  20%  |                                                                                |===============                                                         |  21%  |                                                                                |================                                                        |  22%  |                                                                                |================                                                        |  23%  |                                                                                |=================                                                       |  23%  |                                                                                |=================                                                       |  24%  |                                                                                |==================                                                      |  24%  |                                                                                |==================                                                      |  25%  |                                                                                |==================                                                      |  26%  |                                                                                |===================                                                     |  26%  |                                                                                |===================                                                     |  27%  |                                                                                |====================                                                    |  27%  |                                                                                |====================                                                    |  28%  |                                                                                |=====================                                                   |  29%  |                                                                                |=====================                                                   |  30%  |                                                                                |======================                                                  |  30%  |                                                                                |======================                                                  |  31%  |                                                                                |=======================                                                 |  31%  |                                                                                |=======================                                                 |  32%  |                                                                                |=======================                                                 |  33%  |                                                                                |========================                                                |  33%  |                                                                                |========================                                                |  34%  |                                                                                |=========================                                               |  34%  |                                                                                |=========================                                               |  35%  |                                                                                |==========================                                              |  36%  |                                                                                |==========================                                              |  37%  |                                                                                |===========================                                             |  37%  |                                                                                |===========================                                             |  38%  |                                                                                |============================                                            |  38%  |                                                                                |============================                                            |  39%  |                                                                                |=============================                                           |  40%  |                                                                                |=============================                                           |  41%  |                                                                                |==============================                                          |  41%  |                                                                                |==============================                                          |  42%  |                                                                                |===============================                                         |  42%  |                                                                                |===============================                                         |  43%  |                                                                                |===============================                                         |  44%  |                                                                                |================================                                        |  44%  |                                                                                |================================                                        |  45%  |                                                                                |=================================                                       |  45%  |                                                                                |=================================                                       |  46%  |                                                                                |==================================                                      |  47%  |                                                                                |==================================                                      |  48%  |                                                                                |===================================                                     |  48%  |                                                                                |===================================                                     |  49%  |                                                                                |====================================                                    |  49%  |                                                                                |====================================                                    |  50%  |                                                                                |====================================                                    |  51%  |                                                                                |=====================================                                   |  51%  |                                                                                |=====================================                                   |  52%  |                                                                                |======================================                                  |  52%  |                                                                                |======================================                                  |  53%  |                                                                                |=======================================                                 |  54%  |                                                                                |=======================================                                 |  55%  |                                                                                |========================================                                |  55%  |                                                                                |========================================                                |  56%  |                                                                                |=========================================                               |  56%  |                                                                                |=========================================                               |  57%  |                                                                                |=========================================                               |  58%  |                                                                                |==========================================                              |  58%  |                                                                                |==========================================                              |  59%  |                                                                                |===========================================                             |  59%  |                                                                                |===========================================                             |  60%  |                                                                                |============================================                            |  61%  |                                                                                |============================================                            |  62%  |                                                                                |=============================================                           |  62%  |                                                                                |=============================================                           |  63%  |                                                                                |==============================================                          |  63%  |                                                                                |==============================================                          |  64%  |                                                                                |===============================================                         |  65%  |                                                                                |===============================================                         |  66%  |                                                                                |================================================                        |  66%  |                                                                                |================================================                        |  67%  |                                                                                |=================================================                       |  67%  |                                                                                |=================================================                       |  68%  |                                                                                |=================================================                       |  69%  |                                                                                |==================================================                      |  69%  |                                                                                |==================================================                      |  70%  |                                                                                |===================================================                     |  70%  |                                                                                |===================================================                     |  71%  |                                                                                |====================================================                    |  72%  |                                                                                |====================================================                    |  73%  |                                                                                |=====================================================                   |  73%  |                                                                                |=====================================================                   |  74%  |                                                                                |======================================================                  |  74%  |                                                                                |======================================================                  |  75%  |                                                                                |======================================================                  |  76%  |                                                                                |=======================================================                 |  76%  |                                                                                |=======================================================                 |  77%  |                                                                                |========================================================                |  77%  |                                                                                |========================================================                |  78%  |                                                                                |=========================================================               |  79%  |                                                                                |=========================================================               |  80%  |                                                                                |==========================================================              |  80%  |                                                                                |==========================================================              |  81%  |                                                                                |===========================================================             |  81%  |                                                                                |===========================================================             |  82%  |                                                                                |===========================================================             |  83%  |                                                                                |============================================================            |  83%  |                                                                                |============================================================            |  84%  |                                                                                |=============================================================           |  84%  |                                                                                |=============================================================           |  85%  |                                                                                |==============================================================          |  86%  |                                                                                |==============================================================          |  87%  |                                                                                |===============================================================         |  87%  |                                                                                |===============================================================         |  88%  |                                                                                |================================================================        |  88%  |                                                                                |================================================================        |  89%  |                                                                                |=================================================================       |  90%  |                                                                                |=================================================================       |  91%  |                                                                                |==================================================================      |  91%  |                                                                                |==================================================================      |  92%  |                                                                                |===================================================================     |  92%  |                                                                                |===================================================================     |  93%  |                                                                                |===================================================================     |  94%  |                                                                                |====================================================================    |  94%  |                                                                                |====================================================================    |  95%  |                                                                                |=====================================================================   |  95%  |                                                                                |=====================================================================   |  96%  |                                                                                |======================================================================  |  97%  |                                                                                |======================================================================  |  98%  |                                                                                |======================================================================= |  98%  |                                                                                |======================================================================= |  99%  |                                                                                |========================================================================|  99%  |                                                                                |========================================================================| 100%</code></pre>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="linear-regression.html#cb64-1" tabindex="-1"></a>coeHOR <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> outHOR<span class="sc">$</span>B, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> median)</span>
<span id="cb64-2"><a href="linear-regression.html#cb64-2" tabindex="-1"></a>preHOR <span class="ot">&lt;-</span> tesX <span class="sc">%*%</span> coeHOR</span>
<span id="cb64-3"><a href="linear-regression.html#cb64-3" tabindex="-1"></a>mseHOR <span class="ot">&lt;-</span> <span class="fu">mean</span>((preHOR <span class="sc">-</span> tesY)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb64-4"><a href="linear-regression.html#cb64-4" tabindex="-1"></a>pr2HOR <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> mseHOR <span class="sc">/</span> <span class="fu">var</span>(tesY)</span></code></pre></div>
<p>This block implements <strong>Bayesian regression using the Horseshoe prior</strong>, which is particularly useful for <strong>sparse models</strong> where only a small subset of predictors are truly relevant. The Horseshoe prior is known for <strong>strong shrinkage</strong> of irrelevant coefficients while allowing <strong>large signals to remain unshrunk</strong>. In this case custom code is used to perform faster sampling when <span class="math inline">\(p &gt; n\)</span>.</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="linear-regression.html#cb65-1" tabindex="-1"></a>outHOR <span class="ot">&lt;-</span> <span class="fu">horseshoe_sampler</span>(<span class="at">X =</span> X, <span class="at">y =</span> y, <span class="at">S =</span> <span class="dv">500</span>)</span></code></pre></div>
<ul>
<li><strong><code>horseshoe_sampler()</code></strong> performs <strong>Bayesian regression with a Horseshoe prior</strong>, which is an <strong>adaptive shrinkage prior</strong>.</li>
<li><strong>Key argument</strong>:
<ul>
<li><code>S = 500</code>: Runs <strong>500 MCMC iterations</strong> to generate posterior samples of the coefficients <span class="math inline">\(\boldsymbol{\beta}\)</span>.</li>
</ul></li>
</ul>
<p>📌 <strong>Horseshoe Prior vs. Other Bayesian Methods</strong><br />
- <strong>Bayesian Ridge</strong>: Uses a <strong>Gaussian prior</strong>, shrinking all coefficients uniformly.
- <strong>Bayesian Lasso</strong>: Uses a <strong>Laplace prior</strong>, putting more weight at zero in the prior.
- <strong>Horseshoe Prior</strong>: Uses a <strong>hierarchical prior</strong> that <strong>strongly shrinks small coefficients</strong> but allows large coefficients to stay large.</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="linear-regression.html#cb66-1" tabindex="-1"></a>coeHOR <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> outHOR<span class="sc">$</span>B, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> median)</span></code></pre></div>
<ul>
<li><strong>Takes the median</strong> of the posterior samples for each coefficient.</li>
<li>This provides a <strong>point estimate</strong> similar to Bayesian Lasso but with <strong>adaptive shrinkage</strong>.</li>
</ul>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="linear-regression.html#cb67-1" tabindex="-1"></a>preHOR <span class="ot">&lt;-</span> tesX <span class="sc">%*%</span> coeHOR</span></code></pre></div>
<ul>
<li>Uses the <strong>posterior median coefficients</strong> to predict response values on the test set.</li>
</ul>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="linear-regression.html#cb68-1" tabindex="-1"></a>mseHOR <span class="ot">&lt;-</span> <span class="fu">mean</span>((preHOR <span class="sc">-</span> tesY)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<ul>
<li>Computes the <strong>Mean Squared Error (MSE)</strong> on the test data:
<span class="math display">\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} ( \hat{y}_i - y_i )^2
\]</span></li>
<li><strong>Lower MSE</strong> indicates better predictive performance.</li>
</ul>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="linear-regression.html#cb69-1" tabindex="-1"></a>pr2HOR <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> mseHOR <span class="sc">/</span> <span class="fu">var</span>(tesY)</span></code></pre></div>
<ul>
<li>Computes the <strong>predictive <span class="math inline">\(R^2\)</span></strong>:
<span class="math display">\[
R^2 = 1 - \frac{\text{MSE}}{\text{Var}(\text{Test } y)}
\]</span></li>
<li><strong>Interpretation</strong>:
<ul>
<li><strong><span class="math inline">\(R^2 \approx 1\)</span></strong> → Model explains most of the variance (good performance).</li>
<li><strong><span class="math inline">\(R^2 \approx 0\)</span></strong> → Model performs similarly to a constant predictor.</li>
<li><strong><span class="math inline">\(R^2 &lt; 0\)</span></strong> → Model performs worse than predicting the mean (poor generalization).</li>
</ul></li>
</ul>
<p>Summary of Key Variables:</p>
<table>
<colgroup>
<col width="44%" />
<col width="55%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Variable</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>outHOR</code></td>
<td>Posterior samples of <span class="math inline">\(\boldsymbol{\beta}\)</span> from the <strong>Horseshoe prior</strong> model.</td>
</tr>
<tr class="even">
<td><code>coeHOR</code></td>
<td>Posterior <strong>median</strong> coefficients (Bayesian point estimate).</td>
</tr>
<tr class="odd">
<td><code>preHOR</code></td>
<td>Predicted values for the test set.</td>
</tr>
<tr class="even">
<td><code>mseHOR</code></td>
<td>Mean Squared Error on the test set (lower is better).</td>
</tr>
<tr class="odd">
<td><code>pr2HOR</code></td>
<td>Predictive <span class="math inline">\(R^2\)</span>, measuring model generalization.</td>
</tr>
</tbody>
</table>
<p>Why Use the Horseshoe Prior?</p>
<ul>
<li><strong>Handles Sparse Models Well</strong>: Encourages <strong>strong shrinkage</strong> for irrelevant coefficients while keeping relevant ones intact.</li>
<li><strong>Better than Lasso for Small Signals</strong>: Lasso tends to shrink <strong>all</strong> coefficients, while Horseshoe allows some to stay large.</li>
<li><strong>Works in High-Dimensional Settings</strong>: Performs well when <span class="math inline">\(p &gt; n\)</span>, where traditional methods like OLS and Ridge struggle.</li>
</ul>
</div>
<div id="results-comparisson" class="section level3 hasAnchor" number="4.3.9">
<h3><span class="header-section-number">4.3.9</span> Results Comparisson<a href="linear-regression.html#results-comparisson" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="estimation-comparisson" class="section level4 hasAnchor" number="4.3.9.1">
<h4><span class="header-section-number">4.3.9.1</span> Estimation Comparisson<a href="linear-regression.html#estimation-comparisson" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="linear-regression.html#cb70-1" tabindex="-1"></a><span class="co"># Plots Both</span></span>
<span id="cb70-2"><a href="linear-regression.html#cb70-2" tabindex="-1"></a>coeInd <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb70-3"><a href="linear-regression.html#cb70-3" tabindex="-1"></a>denBAY <span class="ot">&lt;-</span> <span class="fu">density</span>(<span class="at">x =</span> outBAY[, coeInd])</span>
<span id="cb70-4"><a href="linear-regression.html#cb70-4" tabindex="-1"></a>denBLA <span class="ot">&lt;-</span> <span class="fu">density</span>(<span class="at">x =</span> outBLA<span class="sc">$</span>beta[<span class="dv">101</span><span class="sc">:</span><span class="dv">500</span>, coeInd])</span>
<span id="cb70-5"><a href="linear-regression.html#cb70-5" tabindex="-1"></a>denHOR <span class="ot">&lt;-</span> <span class="fu">density</span>(<span class="at">x =</span> outHOR<span class="sc">$</span>B[<span class="dv">101</span><span class="sc">:</span><span class="dv">500</span>, coeInd])</span>
<span id="cb70-6"><a href="linear-regression.html#cb70-6" tabindex="-1"></a>xmin   <span class="ot">&lt;-</span> <span class="fu">min</span>(b[coeInd],</span>
<span id="cb70-7"><a href="linear-regression.html#cb70-7" tabindex="-1"></a>              outRID<span class="sc">$</span>beta[coeInd],</span>
<span id="cb70-8"><a href="linear-regression.html#cb70-8" tabindex="-1"></a>              outLAS<span class="sc">$</span>beta[coeInd],</span>
<span id="cb70-9"><a href="linear-regression.html#cb70-9" tabindex="-1"></a>              outBAY[, coeInd],</span>
<span id="cb70-10"><a href="linear-regression.html#cb70-10" tabindex="-1"></a>              outBLA<span class="sc">$</span>beta[<span class="dv">101</span><span class="sc">:</span><span class="dv">500</span>, coeInd],</span>
<span id="cb70-11"><a href="linear-regression.html#cb70-11" tabindex="-1"></a>              outHOR<span class="sc">$</span>B[<span class="dv">101</span><span class="sc">:</span><span class="dv">500</span>, coeInd])</span>
<span id="cb70-12"><a href="linear-regression.html#cb70-12" tabindex="-1"></a>xmax   <span class="ot">&lt;-</span> <span class="fu">max</span>(b[coeInd],</span>
<span id="cb70-13"><a href="linear-regression.html#cb70-13" tabindex="-1"></a>              outRID<span class="sc">$</span>beta[coeInd],</span>
<span id="cb70-14"><a href="linear-regression.html#cb70-14" tabindex="-1"></a>              outLAS<span class="sc">$</span>beta[coeInd],</span>
<span id="cb70-15"><a href="linear-regression.html#cb70-15" tabindex="-1"></a>              outBAY[, coeInd],</span>
<span id="cb70-16"><a href="linear-regression.html#cb70-16" tabindex="-1"></a>              outBLA<span class="sc">$</span>beta[<span class="dv">101</span><span class="sc">:</span><span class="dv">500</span>, coeInd],</span>
<span id="cb70-17"><a href="linear-regression.html#cb70-17" tabindex="-1"></a>              outHOR<span class="sc">$</span>B[<span class="dv">101</span><span class="sc">:</span><span class="dv">500</span>, coeInd])</span>
<span id="cb70-18"><a href="linear-regression.html#cb70-18" tabindex="-1"></a>ymax   <span class="ot">&lt;-</span> <span class="fu">max</span>(denBAY<span class="sc">$</span>y, denBLA<span class="sc">$</span>y, denHOR<span class="sc">$</span>y)</span>
<span id="cb70-19"><a href="linear-regression.html#cb70-19" tabindex="-1"></a><span class="fu">plot</span>(denBAY,</span>
<span id="cb70-20"><a href="linear-regression.html#cb70-20" tabindex="-1"></a>     <span class="at">col  =</span> <span class="fu">rgb</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>),</span>
<span id="cb70-21"><a href="linear-regression.html#cb70-21" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax),</span>
<span id="cb70-22"><a href="linear-regression.html#cb70-22" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(   <span class="dv">0</span>, ymax),</span>
<span id="cb70-23"><a href="linear-regression.html#cb70-23" tabindex="-1"></a>     <span class="at">main =</span> <span class="fu">paste0</span>(<span class="st">&quot;Coefficient &quot;</span>, coeInd),</span>
<span id="cb70-24"><a href="linear-regression.html#cb70-24" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="fu">bquote</span>(beta[.(coeInd)]))</span>
<span id="cb70-25"><a href="linear-regression.html#cb70-25" tabindex="-1"></a><span class="fu">polygon</span>(denBAY,</span>
<span id="cb70-26"><a href="linear-regression.html#cb70-26" tabindex="-1"></a>        <span class="at">col    =</span> <span class="fu">rgb</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.5</span>),</span>
<span id="cb70-27"><a href="linear-regression.html#cb70-27" tabindex="-1"></a>        <span class="at">border =</span> <span class="cn">NA</span>)</span>
<span id="cb70-28"><a href="linear-regression.html#cb70-28" tabindex="-1"></a><span class="fu">par</span>(<span class="at">new=</span><span class="cn">TRUE</span>)</span>
<span id="cb70-29"><a href="linear-regression.html#cb70-29" tabindex="-1"></a><span class="fu">plot</span>(denBLA,</span>
<span id="cb70-30"><a href="linear-regression.html#cb70-30" tabindex="-1"></a>     <span class="at">col  =</span> <span class="fu">rgb</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>),</span>
<span id="cb70-31"><a href="linear-regression.html#cb70-31" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax),</span>
<span id="cb70-32"><a href="linear-regression.html#cb70-32" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(   <span class="dv">0</span>, ymax),</span>
<span id="cb70-33"><a href="linear-regression.html#cb70-33" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb70-34"><a href="linear-regression.html#cb70-34" tabindex="-1"></a>     <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>,</span>
<span id="cb70-35"><a href="linear-regression.html#cb70-35" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb70-36"><a href="linear-regression.html#cb70-36" tabindex="-1"></a><span class="fu">polygon</span>(denBLA,</span>
<span id="cb70-37"><a href="linear-regression.html#cb70-37" tabindex="-1"></a>        <span class="at">col =</span> <span class="fu">rgb</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.5</span>),</span>
<span id="cb70-38"><a href="linear-regression.html#cb70-38" tabindex="-1"></a>        <span class="at">border =</span> <span class="cn">NA</span>)</span>
<span id="cb70-39"><a href="linear-regression.html#cb70-39" tabindex="-1"></a><span class="fu">par</span>(<span class="at">new=</span><span class="cn">TRUE</span>)</span>
<span id="cb70-40"><a href="linear-regression.html#cb70-40" tabindex="-1"></a><span class="fu">plot</span>(denHOR,</span>
<span id="cb70-41"><a href="linear-regression.html#cb70-41" tabindex="-1"></a>     <span class="at">col  =</span> <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb70-42"><a href="linear-regression.html#cb70-42" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax),</span>
<span id="cb70-43"><a href="linear-regression.html#cb70-43" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(   <span class="dv">0</span>, ymax),</span>
<span id="cb70-44"><a href="linear-regression.html#cb70-44" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb70-45"><a href="linear-regression.html#cb70-45" tabindex="-1"></a>     <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>,</span>
<span id="cb70-46"><a href="linear-regression.html#cb70-46" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb70-47"><a href="linear-regression.html#cb70-47" tabindex="-1"></a><span class="fu">polygon</span>(denHOR,</span>
<span id="cb70-48"><a href="linear-regression.html#cb70-48" tabindex="-1"></a>        <span class="at">col =</span> <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.5</span>),</span>
<span id="cb70-49"><a href="linear-regression.html#cb70-49" tabindex="-1"></a>        <span class="at">border =</span> <span class="cn">NA</span>)</span>
<span id="cb70-50"><a href="linear-regression.html#cb70-50" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> b[coeInd],           <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>))</span>
<span id="cb70-51"><a href="linear-regression.html#cb70-51" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> outRID<span class="sc">$</span>beta[coeInd], <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb70-52"><a href="linear-regression.html#cb70-52" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> outLAS<span class="sc">$</span>beta[coeInd], <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/lr-plots-estimates-non-zero-coefficient-1.png" width="672" /></p>
<p>This code <strong>visualizes the posterior distributions</strong> of a <strong>single regression coefficient</strong> <span class="math inline">\(\beta_{\text{coeInd}}\)</span> estimated using <strong>Bayesian Ridge, Bayesian Lasso, and Bayesian Horseshoe priors</strong>. Additionally, it compares these estimates to those from <strong>Ridge and Lasso</strong> regression.</p>
<p>Why This Matters?</p>
<ul>
<li><strong>Ridge (blue line)</strong> shrinks coefficients but <strong>never sets them exactly to zero</strong>.</li>
<li><strong>Lasso (green line)</strong> performs feature selection by setting some coefficients <strong>exactly to zero</strong>.</li>
<li><strong>Bayesian Ridge (red curve)</strong> resembles Ridge but <strong>accounts for uncertainty</strong>.</li>
<li><strong>Bayesian Lasso (purple curve)</strong> is similar to Lasso but <strong>includes a distribution over coefficients</strong>.</li>
<li><strong>Horseshoe (cyan curve)</strong> applies <strong>strong shrinkage</strong> to small coefficients while allowing <strong>important ones to remain large</strong>.</li>
</ul>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="linear-regression.html#cb71-1" tabindex="-1"></a>coeInd <span class="ot">&lt;-</span> <span class="dv">1</span></span></code></pre></div>
<ul>
<li><strong><code>coeInd = 1</code></strong> selects the <strong>first coefficient</strong> (<span class="math inline">\(\beta_1\)</span>) for visualization.</li>
<li>The user can change this to plot other coefficients.</li>
</ul>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="linear-regression.html#cb72-1" tabindex="-1"></a>denBAY <span class="ot">&lt;-</span> <span class="fu">density</span>(<span class="at">x =</span> outBAY[, coeInd])</span>
<span id="cb72-2"><a href="linear-regression.html#cb72-2" tabindex="-1"></a>denBLA <span class="ot">&lt;-</span> <span class="fu">density</span>(<span class="at">x =</span> outBLA<span class="sc">$</span>beta[<span class="dv">101</span><span class="sc">:</span><span class="dv">500</span>, coeInd])</span>
<span id="cb72-3"><a href="linear-regression.html#cb72-3" tabindex="-1"></a>denHOR <span class="ot">&lt;-</span> <span class="fu">density</span>(<span class="at">x =</span> outHOR<span class="sc">$</span>B[<span class="dv">101</span><span class="sc">:</span><span class="dv">500</span>, coeInd])</span></code></pre></div>
<ul>
<li><strong><code>density()</code></strong> estimates the probability density function (PDF) of the sampled coefficients.</li>
<li><strong>Bayesian Models:</strong>
<ul>
<li><strong><code>outBAY[, coeInd]</code></strong>: Posterior samples from <strong>Bayesian Ridge Regression</strong>.</li>
<li><strong><code>outBLA$beta[101:500, coeInd]</code></strong>: Posterior samples from <strong>Bayesian Lasso</strong> (excluding first 100 as burn-in).</li>
<li><strong><code>outHOR$B[101:500, coeInd]</code></strong>: Posterior samples from <strong>Bayesian Horseshoe</strong> (excluding first 100 as burn-in).</li>
</ul></li>
</ul>
<p>📌 <strong>Why exclude the first 100 samples?</strong>
- In <strong>MCMC sampling</strong>, the first few iterations (burn-in period) may not be from the true posterior distribution.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="linear-regression.html#cb73-1" tabindex="-1"></a>xmin <span class="ot">&lt;-</span> <span class="fu">min</span>(b[coeInd],</span>
<span id="cb73-2"><a href="linear-regression.html#cb73-2" tabindex="-1"></a>            outRID<span class="sc">$</span>beta[coeInd],</span>
<span id="cb73-3"><a href="linear-regression.html#cb73-3" tabindex="-1"></a>            outLAS<span class="sc">$</span>beta[coeInd],</span>
<span id="cb73-4"><a href="linear-regression.html#cb73-4" tabindex="-1"></a>            outBAY[, coeInd],</span>
<span id="cb73-5"><a href="linear-regression.html#cb73-5" tabindex="-1"></a>            outBLA<span class="sc">$</span>beta[<span class="dv">101</span><span class="sc">:</span><span class="dv">500</span>, coeInd],</span>
<span id="cb73-6"><a href="linear-regression.html#cb73-6" tabindex="-1"></a>            outHOR<span class="sc">$</span>B[<span class="dv">101</span><span class="sc">:</span><span class="dv">500</span>, coeInd])</span>
<span id="cb73-7"><a href="linear-regression.html#cb73-7" tabindex="-1"></a></span>
<span id="cb73-8"><a href="linear-regression.html#cb73-8" tabindex="-1"></a>xmax <span class="ot">&lt;-</span> <span class="fu">max</span>(b[coeInd],</span>
<span id="cb73-9"><a href="linear-regression.html#cb73-9" tabindex="-1"></a>            outRID<span class="sc">$</span>beta[coeInd],</span>
<span id="cb73-10"><a href="linear-regression.html#cb73-10" tabindex="-1"></a>            outLAS<span class="sc">$</span>beta[coeInd],</span>
<span id="cb73-11"><a href="linear-regression.html#cb73-11" tabindex="-1"></a>            outBAY[, coeInd],</span>
<span id="cb73-12"><a href="linear-regression.html#cb73-12" tabindex="-1"></a>            outBLA<span class="sc">$</span>beta[<span class="dv">101</span><span class="sc">:</span><span class="dv">500</span>, coeInd],</span>
<span id="cb73-13"><a href="linear-regression.html#cb73-13" tabindex="-1"></a>            outHOR<span class="sc">$</span>B[<span class="dv">101</span><span class="sc">:</span><span class="dv">500</span>, coeInd])</span>
<span id="cb73-14"><a href="linear-regression.html#cb73-14" tabindex="-1"></a></span>
<span id="cb73-15"><a href="linear-regression.html#cb73-15" tabindex="-1"></a>ymax <span class="ot">&lt;-</span> <span class="fu">max</span>(denBAY<span class="sc">$</span>y, denBLA<span class="sc">$</span>y, denHOR<span class="sc">$</span>y)</span></code></pre></div>
<ul>
<li><strong><code>xmin</code> &amp; <code>xmax</code></strong>: Find the minimum and maximum values of the estimated coefficients to set the x-axis range.</li>
<li><strong><code>ymax</code></strong>: Finds the maximum density value across all models to set the y-axis range.</li>
</ul>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="linear-regression.html#cb74-1" tabindex="-1"></a><span class="fu">plot</span>(denBAY,</span>
<span id="cb74-2"><a href="linear-regression.html#cb74-2" tabindex="-1"></a>     <span class="at">col  =</span> <span class="fu">rgb</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>),</span>
<span id="cb74-3"><a href="linear-regression.html#cb74-3" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax),</span>
<span id="cb74-4"><a href="linear-regression.html#cb74-4" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, ymax),</span>
<span id="cb74-5"><a href="linear-regression.html#cb74-5" tabindex="-1"></a>     <span class="at">main =</span> <span class="fu">paste0</span>(<span class="st">&quot;Coefficient &quot;</span>, coeInd),</span>
<span id="cb74-6"><a href="linear-regression.html#cb74-6" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="fu">bquote</span>(beta[.(coeInd)]))</span>
<span id="cb74-7"><a href="linear-regression.html#cb74-7" tabindex="-1"></a><span class="fu">polygon</span>(denBAY,</span>
<span id="cb74-8"><a href="linear-regression.html#cb74-8" tabindex="-1"></a>        <span class="at">col    =</span> <span class="fu">rgb</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.5</span>),</span>
<span id="cb74-9"><a href="linear-regression.html#cb74-9" tabindex="-1"></a>        <span class="at">border =</span> <span class="cn">NA</span>)</span></code></pre></div>
<ul>
<li><strong>Plots the density of Bayesian Ridge estimates (in red)</strong>.</li>
<li><strong><code>polygon()</code></strong> fills the area under the curve for better visualization.</li>
</ul>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="linear-regression.html#cb75-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">new=</span><span class="cn">TRUE</span>)</span>
<span id="cb75-2"><a href="linear-regression.html#cb75-2" tabindex="-1"></a><span class="fu">plot</span>(denBLA,</span>
<span id="cb75-3"><a href="linear-regression.html#cb75-3" tabindex="-1"></a>     <span class="at">col  =</span> <span class="fu">rgb</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>),</span>
<span id="cb75-4"><a href="linear-regression.html#cb75-4" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax),</span>
<span id="cb75-5"><a href="linear-regression.html#cb75-5" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, ymax),</span>
<span id="cb75-6"><a href="linear-regression.html#cb75-6" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb75-7"><a href="linear-regression.html#cb75-7" tabindex="-1"></a>     <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>,</span>
<span id="cb75-8"><a href="linear-regression.html#cb75-8" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb75-9"><a href="linear-regression.html#cb75-9" tabindex="-1"></a><span class="fu">polygon</span>(denBLA,</span>
<span id="cb75-10"><a href="linear-regression.html#cb75-10" tabindex="-1"></a>        <span class="at">col =</span> <span class="fu">rgb</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.5</span>),</span>
<span id="cb75-11"><a href="linear-regression.html#cb75-11" tabindex="-1"></a>        <span class="at">border =</span> <span class="cn">NA</span>)</span></code></pre></div>
<ul>
<li><strong>Plots Bayesian Lasso estimates (in purple)</strong>.</li>
<li><strong><code>par(new=TRUE)</code></strong> allows overlaying this plot on the previous one.</li>
</ul>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="linear-regression.html#cb76-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">new=</span><span class="cn">TRUE</span>)</span>
<span id="cb76-2"><a href="linear-regression.html#cb76-2" tabindex="-1"></a><span class="fu">plot</span>(denHOR,</span>
<span id="cb76-3"><a href="linear-regression.html#cb76-3" tabindex="-1"></a>     <span class="at">col  =</span> <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb76-4"><a href="linear-regression.html#cb76-4" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(xmin, xmax),</span>
<span id="cb76-5"><a href="linear-regression.html#cb76-5" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, ymax),</span>
<span id="cb76-6"><a href="linear-regression.html#cb76-6" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb76-7"><a href="linear-regression.html#cb76-7" tabindex="-1"></a>     <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>,</span>
<span id="cb76-8"><a href="linear-regression.html#cb76-8" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb76-9"><a href="linear-regression.html#cb76-9" tabindex="-1"></a><span class="fu">polygon</span>(denHOR,</span>
<span id="cb76-10"><a href="linear-regression.html#cb76-10" tabindex="-1"></a>        <span class="at">col =</span> <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.5</span>),</span>
<span id="cb76-11"><a href="linear-regression.html#cb76-11" tabindex="-1"></a>        <span class="at">border =</span> <span class="cn">NA</span>)</span></code></pre></div>
<ul>
<li><strong>Plots Bayesian Horseshoe estimates (in cyan/light blue)</strong>.</li>
<li>The densities <strong>overlap</strong>, allowing a direct comparison of uncertainty across models.</li>
</ul>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="linear-regression.html#cb77-1" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> b[coeInd],           <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>)) <span class="co"># True coefficient (black)</span></span>
<span id="cb77-2"><a href="linear-regression.html#cb77-2" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> outRID<span class="sc">$</span>beta[coeInd], <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>)) <span class="co"># Ridge estimate (blue)</span></span>
<span id="cb77-3"><a href="linear-regression.html#cb77-3" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> outLAS<span class="sc">$</span>beta[coeInd], <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>)) <span class="co"># Lasso estimate (green)</span></span></code></pre></div>
<ul>
<li><strong>Black line</strong> → True coefficient value.</li>
<li><strong>Blue line</strong> → Ridge estimate.</li>
<li><strong>Green line</strong> → Lasso estimate.</li>
</ul>
<p>📌 <strong>Why are Ridge and Lasso shown separately?</strong><br />
- Unlike Bayesian methods, Ridge and Lasso provide <strong>point estimates</strong> instead of full posterior distributions.</p>
</div>
<div id="performance-metrics" class="section level4 hasAnchor" number="4.3.9.2">
<h4><span class="header-section-number">4.3.9.2</span> Performance Metrics<a href="linear-regression.html#performance-metrics" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="linear-regression.html#cb78-1" tabindex="-1"></a><span class="co"># Create a table with rounded values</span></span>
<span id="cb78-2"><a href="linear-regression.html#cb78-2" tabindex="-1"></a>coef_table <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="fu">cbind</span>(</span>
<span id="cb78-3"><a href="linear-regression.html#cb78-3" tabindex="-1"></a>  <span class="at">True_Beta =</span> b, </span>
<span id="cb78-4"><a href="linear-regression.html#cb78-4" tabindex="-1"></a>  <span class="at">OLS =</span> outOLS<span class="sc">$</span>coefficients, </span>
<span id="cb78-5"><a href="linear-regression.html#cb78-5" tabindex="-1"></a>  <span class="at">Ridge =</span> <span class="fu">as.numeric</span>(outRID<span class="sc">$</span>beta), </span>
<span id="cb78-6"><a href="linear-regression.html#cb78-6" tabindex="-1"></a>  <span class="at">Lasso =</span> <span class="fu">as.numeric</span>(outLAS<span class="sc">$</span>beta), </span>
<span id="cb78-7"><a href="linear-regression.html#cb78-7" tabindex="-1"></a>  <span class="at">B_Ridge =</span> coeBAY, </span>
<span id="cb78-8"><a href="linear-regression.html#cb78-8" tabindex="-1"></a>  <span class="at">B_Lasso =</span> coeBLA, </span>
<span id="cb78-9"><a href="linear-regression.html#cb78-9" tabindex="-1"></a>  <span class="at">Horseshoe =</span> coeHOR</span>
<span id="cb78-10"><a href="linear-regression.html#cb78-10" tabindex="-1"></a>)[<span class="dv">1</span><span class="sc">:</span><span class="dv">15</span>, ], <span class="dv">4</span>)</span>
<span id="cb78-11"><a href="linear-regression.html#cb78-11" tabindex="-1"></a></span>
<span id="cb78-12"><a href="linear-regression.html#cb78-12" tabindex="-1"></a><span class="fu">rownames</span>(coef_table) <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;Coefficient &quot;</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">15</span>)</span>
<span id="cb78-13"><a href="linear-regression.html#cb78-13" tabindex="-1"></a></span>
<span id="cb78-14"><a href="linear-regression.html#cb78-14" tabindex="-1"></a><span class="co"># Print as a formatted table</span></span>
<span id="cb78-15"><a href="linear-regression.html#cb78-15" tabindex="-1"></a><span class="fu">kable</span>(<span class="fu">as.matrix</span>(coef_table), <span class="at">caption =</span> <span class="st">&quot;Comparison of Coefficient Estimates Across Methods&quot;</span>)</span></code></pre></div>
<table>
<caption><span id="tab:lr-coefficient-comparisson">Table 4.1: </span>Comparison of Coefficient Estimates Across Methods</caption>
<colgroup>
<col width="20%" />
<col width="13%" />
<col width="10%" />
<col width="10%" />
<col width="9%" />
<col width="10%" />
<col width="10%" />
<col width="13%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">True_Beta</th>
<th align="right">OLS</th>
<th align="right">Ridge</th>
<th align="right">Lasso</th>
<th align="right">B_Ridge</th>
<th align="right">B_Lasso</th>
<th align="right">Horseshoe</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Coefficient 1</td>
<td align="right">0.7788</td>
<td align="right">1.0284</td>
<td align="right">0.1298</td>
<td align="right">0.7248</td>
<td align="right">0.1291</td>
<td align="right">0.7485</td>
<td align="right">0.7657</td>
</tr>
<tr class="even">
<td align="left">Coefficient 2</td>
<td align="right">0.6065</td>
<td align="right">0.8395</td>
<td align="right">0.1416</td>
<td align="right">0.6148</td>
<td align="right">0.1407</td>
<td align="right">0.6366</td>
<td align="right">0.6066</td>
</tr>
<tr class="odd">
<td align="left">Coefficient 3</td>
<td align="right">0.4724</td>
<td align="right">1.2123</td>
<td align="right">0.1398</td>
<td align="right">0.4795</td>
<td align="right">0.1414</td>
<td align="right">0.4802</td>
<td align="right">0.4812</td>
</tr>
<tr class="even">
<td align="left">Coefficient 4</td>
<td align="right">0.3679</td>
<td align="right">-0.0911</td>
<td align="right">0.1192</td>
<td align="right">0.3627</td>
<td align="right">0.1187</td>
<td align="right">0.3661</td>
<td align="right">0.3635</td>
</tr>
<tr class="odd">
<td align="left">Coefficient 5</td>
<td align="right">0.2865</td>
<td align="right">-0.5125</td>
<td align="right">0.0986</td>
<td align="right">0.2963</td>
<td align="right">0.0993</td>
<td align="right">0.2965</td>
<td align="right">0.2957</td>
</tr>
<tr class="even">
<td align="left">Coefficient 6</td>
<td align="right">0.2231</td>
<td align="right">1.1521</td>
<td align="right">0.0778</td>
<td align="right">0.2083</td>
<td align="right">0.0782</td>
<td align="right">0.2129</td>
<td align="right">0.2056</td>
</tr>
<tr class="odd">
<td align="left">Coefficient 7</td>
<td align="right">0.1738</td>
<td align="right">-0.8806</td>
<td align="right">0.0567</td>
<td align="right">0.1360</td>
<td align="right">0.0564</td>
<td align="right">0.1474</td>
<td align="right">0.1661</td>
</tr>
<tr class="even">
<td align="left">Coefficient 8</td>
<td align="right">0.1353</td>
<td align="right">-0.0308</td>
<td align="right">0.0382</td>
<td align="right">0.1173</td>
<td align="right">0.0385</td>
<td align="right">0.1154</td>
<td align="right">0.1175</td>
</tr>
<tr class="odd">
<td align="left">Coefficient 9</td>
<td align="right">0.1054</td>
<td align="right">0.7145</td>
<td align="right">0.0288</td>
<td align="right">0.0585</td>
<td align="right">0.0285</td>
<td align="right">0.0616</td>
<td align="right">0.0970</td>
</tr>
<tr class="even">
<td align="left">Coefficient 10</td>
<td align="right">0.0821</td>
<td align="right">0.3688</td>
<td align="right">0.0261</td>
<td align="right">0.0858</td>
<td align="right">0.0280</td>
<td align="right">0.1142</td>
<td align="right">0.0829</td>
</tr>
<tr class="odd">
<td align="left">Coefficient 11</td>
<td align="right">0.0000</td>
<td align="right">-0.1746</td>
<td align="right">-0.0186</td>
<td align="right">0.0000</td>
<td align="right">-0.0176</td>
<td align="right">0.0000</td>
<td align="right">-0.0001</td>
</tr>
<tr class="even">
<td align="left">Coefficient 12</td>
<td align="right">0.0000</td>
<td align="right">0.5834</td>
<td align="right">-0.0202</td>
<td align="right">0.0000</td>
<td align="right">-0.0208</td>
<td align="right">0.0000</td>
<td align="right">-0.0001</td>
</tr>
<tr class="odd">
<td align="left">Coefficient 13</td>
<td align="right">0.0000</td>
<td align="right">-0.4630</td>
<td align="right">-0.0106</td>
<td align="right">0.0000</td>
<td align="right">-0.0114</td>
<td align="right">0.0057</td>
<td align="right">0.0000</td>
</tr>
<tr class="even">
<td align="left">Coefficient 14</td>
<td align="right">0.0000</td>
<td align="right">-0.3090</td>
<td align="right">-0.0019</td>
<td align="right">0.0000</td>
<td align="right">-0.0022</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
</tr>
<tr class="odd">
<td align="left">Coefficient 15</td>
<td align="right">0.0000</td>
<td align="right">0.0410</td>
<td align="right">-0.0051</td>
<td align="right">0.0000</td>
<td align="right">-0.0045</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
</tr>
</tbody>
</table>
<p><strong>Observations:</strong>
- <strong>OLS</strong> coefficients tend to be <strong>large and unstable</strong> due to multicollinearity.
- <strong>Ridge</strong> shrinks all coefficients but <strong>keeps them nonzero</strong>.
- <strong>Lasso</strong> performs <strong>feature selection</strong>, setting some coefficients <strong>exactly to zero</strong>.
- <strong>Bayesian methods</strong> provide <strong>uncertainty-aware estimates</strong> and <strong>adaptive shrinkage</strong>.
- <strong>Horseshoe</strong> applies <strong>strong shrinkage to irrelevant coefficients</strong> while keeping relevant ones <strong>unshrunk</strong>.</p>
<p>Would you like to include a <strong>heatmap visualization</strong> of these coefficient estimates? 🚀</p>
<hr />
</div>
<div id="predictive-performance" class="section level4 hasAnchor" number="4.3.9.3">
<h4><span class="header-section-number">4.3.9.3</span> Predictive Performance<a href="linear-regression.html#predictive-performance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="linear-regression.html#cb79-1" tabindex="-1"></a><span class="co"># Create a performance table</span></span>
<span id="cb79-2"><a href="linear-regression.html#cb79-2" tabindex="-1"></a>performance_table <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb79-3"><a href="linear-regression.html#cb79-3" tabindex="-1"></a>  <span class="at">Method =</span> <span class="fu">c</span>(<span class="st">&quot;OLS&quot;</span>, <span class="st">&quot;Ridge&quot;</span>, <span class="st">&quot;Lasso&quot;</span>, <span class="st">&quot;Bayesian Ridge&quot;</span>, <span class="st">&quot;Bayesian Lasso&quot;</span>, <span class="st">&quot;Horseshoe&quot;</span>),</span>
<span id="cb79-4"><a href="linear-regression.html#cb79-4" tabindex="-1"></a>  <span class="at">MSE =</span> <span class="fu">round</span>(<span class="fu">c</span>(mseOLS, mseRID, mseLAS, mseBAY, mseBLA, mseHOR), <span class="dv">4</span>),</span>
<span id="cb79-5"><a href="linear-regression.html#cb79-5" tabindex="-1"></a>  <span class="at">Predictive_R2 =</span> <span class="fu">round</span>(<span class="fu">c</span>(pr2OLS, pr2RID, pr2LAS, pr2BAY, pr2BLA, pr2HOR), <span class="dv">4</span>)</span>
<span id="cb79-6"><a href="linear-regression.html#cb79-6" tabindex="-1"></a>)</span>
<span id="cb79-7"><a href="linear-regression.html#cb79-7" tabindex="-1"></a></span>
<span id="cb79-8"><a href="linear-regression.html#cb79-8" tabindex="-1"></a><span class="co"># Print as a formatted table</span></span>
<span id="cb79-9"><a href="linear-regression.html#cb79-9" tabindex="-1"></a><span class="fu">kable</span>(performance_table, <span class="at">caption =</span> <span class="st">&quot;Model Comparison: MSE and Predictive R²&quot;</span>)</span></code></pre></div>
<table>
<caption><span id="tab:lr-performance-mse-pr2">Table 4.2: </span>Model Comparison: MSE and Predictive R²</caption>
<thead>
<tr class="header">
<th align="left">Method</th>
<th align="right">MSE</th>
<th align="right">Predictive_R2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">OLS</td>
<td align="right">35.7967</td>
<td align="right">-8.0661</td>
</tr>
<tr class="even">
<td align="left">Ridge</td>
<td align="right">2.3529</td>
<td align="right">0.4041</td>
</tr>
<tr class="odd">
<td align="left">Lasso</td>
<td align="right">0.1065</td>
<td align="right">0.9730</td>
</tr>
<tr class="even">
<td align="left">Bayesian Ridge</td>
<td align="right">2.3150</td>
<td align="right">0.4137</td>
</tr>
<tr class="odd">
<td align="left">Bayesian Lasso</td>
<td align="right">0.1162</td>
<td align="right">0.9706</td>
</tr>
<tr class="even">
<td align="left">Horseshoe</td>
<td align="right">0.0939</td>
<td align="right">0.9762</td>
</tr>
</tbody>
</table>
<p><strong>Interpretation</strong></p>
<ul>
<li><strong>Lower MSE</strong> → Better predictive performance.</li>
<li><strong>Higher <span class="math inline">\(R^2\)</span></strong> → Model explains more variance in the test set.</li>
<li><strong>OLS</strong> likely performs the worst due to overfitting and instability.</li>
<li><strong>Ridge</strong> improves stability but retains all predictors.</li>
<li><strong>Lasso</strong> performs feature selection, possibly improving interpretability.</li>
<li><strong>Bayesian methods</strong> incorporate uncertainty and adaptive shrinkage.</li>
<li><strong>Horseshoe</strong> is expected to perform well in sparse settings.</li>
</ul>
</div>
<div id="variable-selection-post-processing" class="section level4 hasAnchor" number="4.3.9.4">
<h4><span class="header-section-number">4.3.9.4</span> Variable Selection Post-Processing<a href="linear-regression.html#variable-selection-post-processing" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="linear-regression.html#cb80-1" tabindex="-1"></a><span class="co"># No post-processing necessary</span></span>
<span id="cb80-2"><a href="linear-regression.html#cb80-2" tabindex="-1"></a>estBLA <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> outBLA<span class="sc">$</span>beta, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> median)</span>
<span id="cb80-3"><a href="linear-regression.html#cb80-3" tabindex="-1"></a>estLAS <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(outLAS<span class="sc">$</span>beta)</span>
<span id="cb80-4"><a href="linear-regression.html#cb80-4" tabindex="-1"></a><span class="co"># Post-processing for Ridge regression </span></span>
<span id="cb80-5"><a href="linear-regression.html#cb80-5" tabindex="-1"></a>estRID <span class="ot">&lt;-</span> outRID<span class="sc">$</span>beta</span>
<span id="cb80-6"><a href="linear-regression.html#cb80-6" tabindex="-1"></a>kmeRID <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(<span class="at">x =</span> <span class="fu">as.numeric</span>(<span class="fu">abs</span>(outRID<span class="sc">$</span>beta)), <span class="at">centers =</span> <span class="dv">2</span>)</span>
<span id="cb80-7"><a href="linear-regression.html#cb80-7" tabindex="-1"></a><span class="cf">if</span>(kmeRID<span class="sc">$</span>centers[<span class="dv">1</span>] <span class="sc">&gt;</span> kmeRID<span class="sc">$</span>centers[<span class="dv">2</span>]){</span>
<span id="cb80-8"><a href="linear-regression.html#cb80-8" tabindex="-1"></a>  estRID[kmeRID<span class="sc">$</span>cluster <span class="sc">==</span> <span class="dv">2</span>] <span class="ot">&lt;-</span>  <span class="dv">0</span></span>
<span id="cb80-9"><a href="linear-regression.html#cb80-9" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb80-10"><a href="linear-regression.html#cb80-10" tabindex="-1"></a>  estRID[kmeRID<span class="sc">$</span>cluster <span class="sc">==</span> <span class="dv">1</span>] <span class="ot">&lt;-</span>  <span class="dv">0</span></span>
<span id="cb80-11"><a href="linear-regression.html#cb80-11" tabindex="-1"></a>}</span>
<span id="cb80-12"><a href="linear-regression.html#cb80-12" tabindex="-1"></a>estRID <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(estRID)</span>
<span id="cb80-13"><a href="linear-regression.html#cb80-13" tabindex="-1"></a><span class="co"># Post-processing for Basic Bayesian regression</span></span>
<span id="cb80-14"><a href="linear-regression.html#cb80-14" tabindex="-1"></a>kmeBAY <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(<span class="at">x =</span> <span class="fu">as.numeric</span>(<span class="fu">abs</span>(outBAY)), <span class="at">centers =</span> <span class="dv">2</span>)</span>
<span id="cb80-15"><a href="linear-regression.html#cb80-15" tabindex="-1"></a><span class="cf">if</span>(kmeBAY<span class="sc">$</span>centers[<span class="dv">1</span>] <span class="sc">&gt;</span> kmeBAY<span class="sc">$</span>centers[<span class="dv">2</span>]){</span>
<span id="cb80-16"><a href="linear-regression.html#cb80-16" tabindex="-1"></a>  outBAY[kmeBAY<span class="sc">$</span>cluster <span class="sc">==</span> <span class="dv">2</span>] <span class="ot">&lt;-</span>  <span class="dv">0</span></span>
<span id="cb80-17"><a href="linear-regression.html#cb80-17" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb80-18"><a href="linear-regression.html#cb80-18" tabindex="-1"></a>  outBAY[kmeBAY<span class="sc">$</span>cluster <span class="sc">==</span> <span class="dv">1</span>] <span class="ot">&lt;-</span>  <span class="dv">0</span></span>
<span id="cb80-19"><a href="linear-regression.html#cb80-19" tabindex="-1"></a>}</span>
<span id="cb80-20"><a href="linear-regression.html#cb80-20" tabindex="-1"></a>estBAY <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> outBAY, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> median)</span>
<span id="cb80-21"><a href="linear-regression.html#cb80-21" tabindex="-1"></a><span class="co"># Post-processing for Horseshoe Regression</span></span>
<span id="cb80-22"><a href="linear-regression.html#cb80-22" tabindex="-1"></a>coeHOR <span class="ot">&lt;-</span> outHOR<span class="sc">$</span>B</span>
<span id="cb80-23"><a href="linear-regression.html#cb80-23" tabindex="-1"></a>kmeHOR <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(<span class="at">x =</span> <span class="fu">as.numeric</span>(<span class="fu">abs</span>(coeHOR)), <span class="at">centers =</span> <span class="dv">2</span>)</span>
<span id="cb80-24"><a href="linear-regression.html#cb80-24" tabindex="-1"></a><span class="cf">if</span>(kmeHOR<span class="sc">$</span>centers[<span class="dv">1</span>] <span class="sc">&gt;</span> kmeHOR<span class="sc">$</span>centers[<span class="dv">2</span>]){</span>
<span id="cb80-25"><a href="linear-regression.html#cb80-25" tabindex="-1"></a>  coeHOR[kmeHOR<span class="sc">$</span>cluster <span class="sc">==</span> <span class="dv">2</span>] <span class="ot">&lt;-</span>  <span class="dv">0</span></span>
<span id="cb80-26"><a href="linear-regression.html#cb80-26" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb80-27"><a href="linear-regression.html#cb80-27" tabindex="-1"></a>  coeHOR[kmeHOR<span class="sc">$</span>cluster <span class="sc">==</span> <span class="dv">1</span>] <span class="ot">&lt;-</span>  <span class="dv">0</span></span>
<span id="cb80-28"><a href="linear-regression.html#cb80-28" tabindex="-1"></a>}</span>
<span id="cb80-29"><a href="linear-regression.html#cb80-29" tabindex="-1"></a>estHOR <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> coeHOR, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> median)</span>
<span id="cb80-30"><a href="linear-regression.html#cb80-30" tabindex="-1"></a></span>
<span id="cb80-31"><a href="linear-regression.html#cb80-31" tabindex="-1"></a><span class="co"># Create a table with rounded values</span></span>
<span id="cb80-32"><a href="linear-regression.html#cb80-32" tabindex="-1"></a>coef_table <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="fu">cbind</span>(</span>
<span id="cb80-33"><a href="linear-regression.html#cb80-33" tabindex="-1"></a>  <span class="at">True_Beta =</span> b, </span>
<span id="cb80-34"><a href="linear-regression.html#cb80-34" tabindex="-1"></a>  <span class="at">Ridge     =</span> estRID, </span>
<span id="cb80-35"><a href="linear-regression.html#cb80-35" tabindex="-1"></a>  <span class="at">Lasso     =</span> estLAS, </span>
<span id="cb80-36"><a href="linear-regression.html#cb80-36" tabindex="-1"></a>  <span class="at">B_Ridge   =</span> estBAY, </span>
<span id="cb80-37"><a href="linear-regression.html#cb80-37" tabindex="-1"></a>  <span class="at">B_Lasso   =</span> estBLA, </span>
<span id="cb80-38"><a href="linear-regression.html#cb80-38" tabindex="-1"></a>  <span class="at">Horseshoe =</span> estHOR</span>
<span id="cb80-39"><a href="linear-regression.html#cb80-39" tabindex="-1"></a>)[<span class="dv">1</span><span class="sc">:</span><span class="dv">15</span>, ], <span class="dv">4</span>)</span>
<span id="cb80-40"><a href="linear-regression.html#cb80-40" tabindex="-1"></a></span>
<span id="cb80-41"><a href="linear-regression.html#cb80-41" tabindex="-1"></a><span class="fu">rownames</span>(coef_table) <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;Coefficient &quot;</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">15</span>)</span>
<span id="cb80-42"><a href="linear-regression.html#cb80-42" tabindex="-1"></a></span>
<span id="cb80-43"><a href="linear-regression.html#cb80-43" tabindex="-1"></a><span class="co"># Print as a formatted table</span></span>
<span id="cb80-44"><a href="linear-regression.html#cb80-44" tabindex="-1"></a><span class="fu">kable</span>(<span class="fu">as.matrix</span>(coef_table), <span class="at">caption =</span> <span class="st">&quot;Comparison of Post-processed Coefficient Across Methods&quot;</span>)</span></code></pre></div>
<table>
<caption><span id="tab:lr-post-processed-coefficient-comparisson">Table 4.3: </span>Comparison of Post-processed Coefficient Across Methods</caption>
<colgroup>
<col width="23%" />
<col width="15%" />
<col width="10%" />
<col width="10%" />
<col width="12%" />
<col width="12%" />
<col width="15%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">True_Beta</th>
<th align="right">Ridge</th>
<th align="right">Lasso</th>
<th align="right">B_Ridge</th>
<th align="right">B_Lasso</th>
<th align="right">Horseshoe</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Coefficient 1</td>
<td align="right">0.7788</td>
<td align="right">0.1298</td>
<td align="right">0.7248</td>
<td align="right">0.1291</td>
<td align="right">0.7485</td>
<td align="right">0.7657</td>
</tr>
<tr class="even">
<td align="left">Coefficient 2</td>
<td align="right">0.6065</td>
<td align="right">0.1416</td>
<td align="right">0.6148</td>
<td align="right">0.1407</td>
<td align="right">0.6366</td>
<td align="right">0.6066</td>
</tr>
<tr class="odd">
<td align="left">Coefficient 3</td>
<td align="right">0.4724</td>
<td align="right">0.1398</td>
<td align="right">0.4795</td>
<td align="right">0.1414</td>
<td align="right">0.4802</td>
<td align="right">0.4812</td>
</tr>
<tr class="even">
<td align="left">Coefficient 4</td>
<td align="right">0.3679</td>
<td align="right">0.1192</td>
<td align="right">0.3627</td>
<td align="right">0.1187</td>
<td align="right">0.3661</td>
<td align="right">0.3635</td>
</tr>
<tr class="odd">
<td align="left">Coefficient 5</td>
<td align="right">0.2865</td>
<td align="right">0.0986</td>
<td align="right">0.2963</td>
<td align="right">0.0993</td>
<td align="right">0.2965</td>
<td align="right">0.2957</td>
</tr>
<tr class="even">
<td align="left">Coefficient 6</td>
<td align="right">0.2231</td>
<td align="right">0.0778</td>
<td align="right">0.2083</td>
<td align="right">0.0782</td>
<td align="right">0.2129</td>
<td align="right">0.0000</td>
</tr>
<tr class="odd">
<td align="left">Coefficient 7</td>
<td align="right">0.1738</td>
<td align="right">0.0000</td>
<td align="right">0.1360</td>
<td align="right">0.0564</td>
<td align="right">0.1474</td>
<td align="right">0.0000</td>
</tr>
<tr class="even">
<td align="left">Coefficient 8</td>
<td align="right">0.1353</td>
<td align="right">0.0000</td>
<td align="right">0.1173</td>
<td align="right">0.0385</td>
<td align="right">0.1154</td>
<td align="right">0.0000</td>
</tr>
<tr class="odd">
<td align="left">Coefficient 9</td>
<td align="right">0.1054</td>
<td align="right">0.0000</td>
<td align="right">0.0585</td>
<td align="right">0.0000</td>
<td align="right">0.0616</td>
<td align="right">0.0000</td>
</tr>
<tr class="even">
<td align="left">Coefficient 10</td>
<td align="right">0.0821</td>
<td align="right">0.0000</td>
<td align="right">0.0858</td>
<td align="right">0.0000</td>
<td align="right">0.1142</td>
<td align="right">0.0000</td>
</tr>
<tr class="odd">
<td align="left">Coefficient 11</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
</tr>
<tr class="even">
<td align="left">Coefficient 12</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
</tr>
<tr class="odd">
<td align="left">Coefficient 13</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0057</td>
<td align="right">0.0000</td>
</tr>
<tr class="even">
<td align="left">Coefficient 14</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
</tr>
<tr class="odd">
<td align="left">Coefficient 15</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
<td align="right">0.0000</td>
</tr>
</tbody>
</table>
<p>This block refines and compares the <strong>coefficient estimates</strong> across <strong>different regression methods</strong>. Some methods, like Bayesian Ridge and Horseshoe, <strong>require post-processing</strong> to identify which coefficients should be set to zero. The final output is a table comparing the <strong>first 15 coefficients</strong> across methods.</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="linear-regression.html#cb81-1" tabindex="-1"></a>estBLA <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> outBLA<span class="sc">$</span>beta, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> median)</span>
<span id="cb81-2"><a href="linear-regression.html#cb81-2" tabindex="-1"></a>estLAS <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(outLAS<span class="sc">$</span>beta)</span></code></pre></div>
<ul>
<li><strong><code>estBLA</code></strong>: Extracts <strong>posterior median</strong> estimates from <strong>Bayesian Lasso</strong>.<br />
</li>
<li><strong><code>estLAS</code></strong>: Converts <strong>Lasso coefficients</strong> to a numeric vector.<br />
</li>
<li><strong>No post-processing is required</strong> for these methods since Lasso and Bayesian Lasso naturally <strong>set some coefficients to zero</strong>.</li>
</ul>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="linear-regression.html#cb82-1" tabindex="-1"></a>estRID <span class="ot">&lt;-</span> outRID<span class="sc">$</span>beta</span>
<span id="cb82-2"><a href="linear-regression.html#cb82-2" tabindex="-1"></a>kmeRID <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(<span class="at">x =</span> <span class="fu">as.numeric</span>(<span class="fu">abs</span>(outRID<span class="sc">$</span>beta)), <span class="at">centers =</span> <span class="dv">2</span>)</span>
<span id="cb82-3"><a href="linear-regression.html#cb82-3" tabindex="-1"></a><span class="cf">if</span>(kmeRID<span class="sc">$</span>centers[<span class="dv">1</span>] <span class="sc">&gt;</span> kmeRID<span class="sc">$</span>centers[<span class="dv">2</span>]){</span>
<span id="cb82-4"><a href="linear-regression.html#cb82-4" tabindex="-1"></a>  estRID[kmeRID<span class="sc">$</span>cluster <span class="sc">==</span> <span class="dv">2</span>] <span class="ot">&lt;-</span>  <span class="dv">0</span></span>
<span id="cb82-5"><a href="linear-regression.html#cb82-5" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb82-6"><a href="linear-regression.html#cb82-6" tabindex="-1"></a>  estRID[kmeRID<span class="sc">$</span>cluster <span class="sc">==</span> <span class="dv">1</span>] <span class="ot">&lt;-</span>  <span class="dv">0</span></span>
<span id="cb82-7"><a href="linear-regression.html#cb82-7" tabindex="-1"></a>}</span>
<span id="cb82-8"><a href="linear-regression.html#cb82-8" tabindex="-1"></a>estRID <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(estRID)</span></code></pre></div>
<ul>
<li><strong>Ridge Regression retains all coefficients</strong> (i.e., does <strong>not</strong> set any to zero).<br />
</li>
<li><strong>Post-processing approach</strong>:
<ul>
<li>Uses <strong>K-means clustering</strong> to <strong>separate large and small coefficients</strong> into two groups (<code>centers = 2</code>).</li>
<li>The group with the <strong>smaller average magnitude</strong> is assumed to contain <strong>insignificant coefficients</strong>, so they are <strong>set to zero</strong>.</li>
</ul></li>
</ul>
<p>📌 <strong>Why is this needed?</strong><br />
- Ridge does not perform <strong>feature selection</strong>, so this helps <strong>identify zero coefficients</strong>.</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="linear-regression.html#cb83-1" tabindex="-1"></a>kmeBAY <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(<span class="at">x =</span> <span class="fu">as.numeric</span>(<span class="fu">abs</span>(outBAY)), <span class="at">centers =</span> <span class="dv">2</span>)</span>
<span id="cb83-2"><a href="linear-regression.html#cb83-2" tabindex="-1"></a><span class="cf">if</span>(kmeBAY<span class="sc">$</span>centers[<span class="dv">1</span>] <span class="sc">&gt;</span> kmeBAY<span class="sc">$</span>centers[<span class="dv">2</span>]){</span>
<span id="cb83-3"><a href="linear-regression.html#cb83-3" tabindex="-1"></a>  outBAY[kmeBAY<span class="sc">$</span>cluster <span class="sc">==</span> <span class="dv">2</span>] <span class="ot">&lt;-</span>  <span class="dv">0</span></span>
<span id="cb83-4"><a href="linear-regression.html#cb83-4" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb83-5"><a href="linear-regression.html#cb83-5" tabindex="-1"></a>  outBAY[kmeBAY<span class="sc">$</span>cluster <span class="sc">==</span> <span class="dv">1</span>] <span class="ot">&lt;-</span>  <span class="dv">0</span></span>
<span id="cb83-6"><a href="linear-regression.html#cb83-6" tabindex="-1"></a>}</span>
<span id="cb83-7"><a href="linear-regression.html#cb83-7" tabindex="-1"></a>estBAY <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> outBAY, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> median)</span></code></pre></div>
<ul>
<li>Bayesian Ridge also <strong>does not perform feature selection</strong>, so we apply <strong>K-means post-processing</strong>.<br />
</li>
<li>The <strong>median of the posterior samples</strong> is used as the final estimate.</li>
</ul>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="linear-regression.html#cb84-1" tabindex="-1"></a>coeHOR <span class="ot">&lt;-</span> outHOR<span class="sc">$</span>B</span>
<span id="cb84-2"><a href="linear-regression.html#cb84-2" tabindex="-1"></a>kmeHOR <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(<span class="at">x =</span> <span class="fu">as.numeric</span>(<span class="fu">abs</span>(coeHOR)), <span class="at">centers =</span> <span class="dv">2</span>)</span>
<span id="cb84-3"><a href="linear-regression.html#cb84-3" tabindex="-1"></a><span class="cf">if</span>(kmeHOR<span class="sc">$</span>centers[<span class="dv">1</span>] <span class="sc">&gt;</span> kmeHOR<span class="sc">$</span>centers[<span class="dv">2</span>]){</span>
<span id="cb84-4"><a href="linear-regression.html#cb84-4" tabindex="-1"></a>  coeHOR[kmeHOR<span class="sc">$</span>cluster <span class="sc">==</span> <span class="dv">2</span>] <span class="ot">&lt;-</span>  <span class="dv">0</span></span>
<span id="cb84-5"><a href="linear-regression.html#cb84-5" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb84-6"><a href="linear-regression.html#cb84-6" tabindex="-1"></a>  coeHOR[kmeHOR<span class="sc">$</span>cluster <span class="sc">==</span> <span class="dv">1</span>] <span class="ot">&lt;-</span>  <span class="dv">0</span></span>
<span id="cb84-7"><a href="linear-regression.html#cb84-7" tabindex="-1"></a>}</span>
<span id="cb84-8"><a href="linear-regression.html#cb84-8" tabindex="-1"></a>estHOR <span class="ot">&lt;-</span> <span class="fu">apply</span>(<span class="at">X =</span> coeHOR, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> median)</span></code></pre></div>
<ul>
<li>The <strong>Horseshoe prior naturally encourages sparsity</strong>, but we <strong>reinforce it</strong> using the <strong>K-means clustering method</strong>.</li>
<li>The <strong>posterior median</strong> is used as the final estimate.</li>
</ul>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="linear-regression.html#cb85-1" tabindex="-1"></a><span class="co"># Create a table with rounded values</span></span>
<span id="cb85-2"><a href="linear-regression.html#cb85-2" tabindex="-1"></a>coef_table <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="fu">cbind</span>(</span>
<span id="cb85-3"><a href="linear-regression.html#cb85-3" tabindex="-1"></a>  <span class="at">True_Beta =</span> b, </span>
<span id="cb85-4"><a href="linear-regression.html#cb85-4" tabindex="-1"></a>  <span class="at">Ridge     =</span> estRID, </span>
<span id="cb85-5"><a href="linear-regression.html#cb85-5" tabindex="-1"></a>  <span class="at">Lasso     =</span> estLAS, </span>
<span id="cb85-6"><a href="linear-regression.html#cb85-6" tabindex="-1"></a>  <span class="at">B_Ridge   =</span> estBAY, </span>
<span id="cb85-7"><a href="linear-regression.html#cb85-7" tabindex="-1"></a>  <span class="at">B_Lasso   =</span> estBLA, </span>
<span id="cb85-8"><a href="linear-regression.html#cb85-8" tabindex="-1"></a>  <span class="at">Horseshoe =</span> estHOR</span>
<span id="cb85-9"><a href="linear-regression.html#cb85-9" tabindex="-1"></a>)[<span class="dv">1</span><span class="sc">:</span><span class="dv">15</span>, ], <span class="dv">4</span>)</span>
<span id="cb85-10"><a href="linear-regression.html#cb85-10" tabindex="-1"></a></span>
<span id="cb85-11"><a href="linear-regression.html#cb85-11" tabindex="-1"></a><span class="fu">rownames</span>(coef_table) <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;Coefficient &quot;</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">15</span>)</span>
<span id="cb85-12"><a href="linear-regression.html#cb85-12" tabindex="-1"></a></span>
<span id="cb85-13"><a href="linear-regression.html#cb85-13" tabindex="-1"></a><span class="co"># Print as a formatted table</span></span>
<span id="cb85-14"><a href="linear-regression.html#cb85-14" tabindex="-1"></a><span class="fu">kable</span>(<span class="fu">as.matrix</span>(coef_table), <span class="at">caption =</span> <span class="st">&quot;Comparison of Post-processed Coefficient Across Methods&quot;</span>)</span></code></pre></div>
<ul>
<li><strong>Combines all coefficient estimates</strong> (True <span class="math inline">\(\beta\)</span>, Ridge, Lasso, Bayesian Ridge, Bayesian Lasso, Horseshoe).</li>
<li><strong>Rounds values to 4 decimal places</strong> for readability.</li>
<li><strong>Formats the table using <code>kable()</code></strong> for a clean output.</li>
</ul>
<p><strong>Key Insights</strong>
- <strong>OLS and Ridge Regression</strong> retain all coefficients → Ridge needs post-processing to remove small values.
- <strong>Lasso and Bayesian Lasso</strong> naturally set some coefficients <strong>exactly to zero</strong>.
- <strong>Bayesian Ridge needs post-processing</strong> to enforce sparsity.
- <strong>Horseshoe Prior is adaptive</strong>, but post-processing enhances sparsity.</p>
<hr />
</div>
</div>
</div>
<div id="efficient-computation" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Efficient Computation<a href="linear-regression.html#efficient-computation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When dealing with <strong>high-dimensional settings</strong> where <span class="math inline">\(p &gt; n\)</span>, direct computation of <strong>matrix inverses and decompositions</strong> becomes computationally expensive. Specifically:</p>
<ul>
<li><strong>Matrix inversion</strong> for an <span class="math inline">\(n \times p\)</span> matrix is typically <span class="math inline">\(O(p^3)\)</span>, which is prohibitive when <span class="math inline">\(p\)</span> is large.</li>
<li><strong>Cholesky decomposition</strong>, commonly used for solving linear systems, is <span class="math inline">\(O(p^3)\)</span> as well.</li>
</ul>
<p>To make computations more <strong>efficient</strong>, we can express operations in terms of <strong><span class="math inline">\(n\)</span> rather than <span class="math inline">\(p\)</span></strong> whenever possible. A key technique in <strong>Machine Learning and Statistics</strong> is the <strong>Woodbury Matrix Identity</strong>, which allows us to rewrite certain matrix inversions in a computationally cheaper form.</p>
<hr />
<div id="efficient-computation-of-ridge-regression-using-the-woodbury-identity" class="section level3 hasAnchor" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> Efficient Computation of Ridge Regression using the Woodbury Identity<a href="linear-regression.html#efficient-computation-of-ridge-regression-using-the-woodbury-identity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Ridge regression estimates the coefficients <span class="math inline">\(\boldsymbol{\beta}\)</span> as:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}}_{\text{ridge}} = (\mathbf{X}&#39; \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}&#39; \mathbf{y}
\]</span></p>
<p>where:<br />
- <span class="math inline">\(\mathbf{X}\)</span> is an <span class="math inline">\(n \times p\)</span> design matrix,<br />
- <span class="math inline">\(\lambda \mathbf{I}\)</span> is a regularization term (where <span class="math inline">\(\lambda &gt; 0\)</span> ensures invertibility),<br />
- <span class="math inline">\(\mathbf{y}\)</span> is an <span class="math inline">\(n \times 1\)</span> response vector.</p>
<p>Computing <span class="math inline">\((\mathbf{X}&#39; \mathbf{X} + \lambda \mathbf{I})^{-1}\)</span> <strong>directly</strong> is expensive when <span class="math inline">\(p\)</span> is large. Instead, we use the <strong>Woodbury Identity</strong>, which states:</p>
<p><span class="math display">\[
(\mathbf{A} + \mathbf{U} \mathbf{C} \mathbf{V})^{-1} = \mathbf{A}^{-1} - \mathbf{A}^{-1} \mathbf{U} (\mathbf{C}^{-1} + \mathbf{V} \mathbf{A}^{-1} \mathbf{U})^{-1} \mathbf{V} \mathbf{A}^{-1}
\]</span></p>
<p>To apply this to <strong>Ridge Regression</strong>, let’s define:</p>
<ul>
<li><span class="math inline">\(\mathbf{A} = \lambda \mathbf{I}_p\)</span> (a <span class="math inline">\(p \times p\)</span> diagonal matrix),<br />
</li>
<li><span class="math inline">\(\mathbf{U} = \mathbf{X}&#39;\)</span> (a <span class="math inline">\(p \times n\)</span> matrix),<br />
</li>
<li><span class="math inline">\(\mathbf{C} = \mathbf{I}_n\)</span> (an <span class="math inline">\(n \times n\)</span> identity matrix),<br />
</li>
<li><span class="math inline">\(\mathbf{V} = \mathbf{X}\)</span> (an <span class="math inline">\(n \times p\)</span> matrix).</li>
</ul>
<p>Applying the <strong>Woodbury Identity</strong>:</p>
<p><span class="math display">\[
(\mathbf{X}&#39; \mathbf{X} + \lambda \mathbf{I})^{-1} = \lambda^{-1} \mathbf{I} - \lambda^{-1} \mathbf{X}&#39; (\mathbf{I} + \mathbf{X} \lambda^{-1} \mathbf{X}&#39;)^{-1} \mathbf{X} \lambda^{-1}
\]</span></p>
<p>Substituting this into the Ridge Regression equation:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}}_{\text{ridge}} = \left( \lambda^{-1} \mathbf{I} - \lambda^{-1} \mathbf{X}&#39; (\mathbf{I} + \mathbf{X} \lambda^{-1} \mathbf{X}&#39;)^{-1} \mathbf{X} \lambda^{-1} \right) \mathbf{X}&#39; \mathbf{y}
\]</span></p>
<p>Rearranging:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}}_{\text{ridge}} = \lambda^{-1} \mathbf{X}&#39; \mathbf{y} - \lambda^{-1} \mathbf{X}&#39; (\mathbf{I} + \mathbf{X} \lambda^{-1} \mathbf{X}&#39;)^{-1} \mathbf{X} \lambda^{-1} \mathbf{X}&#39; \mathbf{y}
\]</span></p>
<p>Defining <strong><span class="math inline">\(n \times n\)</span> matrix inversion instead of <span class="math inline">\(p \times p\)</span></strong>:</p>
<p><span class="math display">\[
\mathbf{M} = (\mathbf{I} + \mathbf{X} \lambda^{-1} \mathbf{X}&#39;)^{-1}
\]</span></p>
<p><span class="math display">\[
\hat{\boldsymbol{\beta}}_{\text{ridge}} = \lambda^{-1} \mathbf{X}&#39; \mathbf{y} - \lambda^{-1} \mathbf{X}&#39; \mathbf{M} \mathbf{X} \lambda^{-1} \mathbf{X}&#39; \mathbf{y}
\]</span></p>
<p>Computational Benefits:</p>
<ul>
<li><strong>Direct inversion of <span class="math inline">\(\mathbf{X}&#39; \mathbf{X} + \lambda  {\boldsymbol I} \)</span> (size <span class="math inline">\(p \times p\)</span>) is <span class="math inline">\(O(p^3)\)</span></strong>.</li>
<li><strong>Computing <span class="math inline">\(\mathbf{M}\)</span> involves inverting an <span class="math inline">\(n \times n\)</span> matrix</strong>, which is <strong>only <span class="math inline">\(O(n^3)\)</span> (much faster when <span class="math inline">\(p \gg n\)</span>)</strong>.</li>
<li>Since <span class="math inline">\(n \ll p\)</span> in high-dimensional settings, this trick significantly reduces computational cost.</li>
</ul>
<p>Conclusion:</p>
<ul>
<li><strong>Using the Woodbury Identity</strong>, we reformulate the Ridge Regression inversion in terms of <span class="math inline">\(n\)</span> instead of <span class="math inline">\(p\)</span>.</li>
<li>This approach is particularly useful in <strong>high-dimensional settings</strong> where <span class="math inline">\(p &gt; n\)</span>.</li>
<li>Many <strong>Bayesian methods</strong>, such as Gaussian Process Regression and Variational Inference, also rely on the <strong>Woodbury identity</strong> for computational efficiency.</li>
</ul>
<hr />
</div>
<div id="efficient-bayesian-sampling-for-gaussian-scale-mixture-priors" class="section level3 hasAnchor" number="4.4.2">
<h3><span class="header-section-number">4.4.2</span> Efficient Bayesian Sampling for Gaussian Scale-Mixture Priors<a href="linear-regression.html#efficient-bayesian-sampling-for-gaussian-scale-mixture-priors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><em>(Based on Bhattacharya et al. (2016), “Fast Sampling with Gaussian Scale-Mixture Priors in High-Dimensional Regression”)</em></p>
<hr />
<div id="motivation" class="section level4 hasAnchor" number="4.4.2.1">
<h4><span class="header-section-number">4.4.2.1</span> Motivation<a href="linear-regression.html#motivation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Gaussian scale-mixture priors, such as the <strong>Horseshoe prior</strong> and <strong>Bayesian Lasso</strong>, are widely used for high-dimensional regression due to their <strong>adaptive shrinkage properties</strong>. These priors impose <strong>global-local shrinkage</strong> on regression coefficients, ensuring that <strong>irrelevant predictors are strongly shrunk</strong>, while allowing <strong>large signals to remain unshrunk</strong>.</p>
<p>However, standard <strong>Markov Chain Monte Carlo (MCMC) sampling</strong> for these priors can be computationally expensive when <span class="math inline">\(p \gg n\)</span>, particularly due to the <strong>matrix inversion</strong> required in Gibbs sampling steps. Existing approaches rely on <strong>Cholesky decomposition</strong>, which has a <strong>computational complexity of <span class="math inline">\(O(p^3)\)</span></strong>, making it prohibitive for large-scale problems.</p>
<p>Bhattacharya et al. (2016) propose an <strong>exact sampling algorithm</strong> that avoids direct matrix inversion by leveraging:
1. <strong>Latent variable augmentation</strong> to restructure the sampling problem.
2. <strong>Sherman–Morrison–Woodbury identity</strong> to reduce matrix inversion complexity from <span class="math inline">\(O(p^3)\)</span> to <span class="math inline">\(O(n^3)\)</span>.
3. <strong>Block Gibbs sampling</strong>, allowing efficient updates of the regression coefficients.</p>
<p>This method <strong>scales linearly with <span class="math inline">\(p\)</span></strong>, making Bayesian inference feasible in high-dimensional settings.</p>
<hr />
</div>
<div id="model-setup-gaussian-scale-mixture-priors-in-bayesian-regression" class="section level4 hasAnchor" number="4.4.2.2">
<h4><span class="header-section-number">4.4.2.2</span> Model Setup: Gaussian Scale-Mixture Priors in Bayesian Regression<a href="linear-regression.html#model-setup-gaussian-scale-mixture-priors-in-bayesian-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We consider the standard <strong>Bayesian linear regression model</strong>:</p>
<p><span class="math display">\[
\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{e}, \quad \mathbf{e} \sim \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I})
\]</span></p>
<p>where:
- <span class="math inline">\(\mathbf{X}\)</span> is the <span class="math inline">\(n \times p\)</span> design matrix.
- <span class="math inline">\(\boldsymbol{\beta}\)</span> is the <span class="math inline">\(p\)</span>-dimensional vector of regression coefficients.
- <span class="math inline">\(\mathbf{y}\)</span> is the <span class="math inline">\(n\)</span>-dimensional response vector.
- <span class="math inline">\(\mathbf{e}\)</span> represents independent Gaussian noise.</p>
<p>A <strong>Gaussian scale-mixture prior</strong> is imposed on <span class="math inline">\(\boldsymbol{\beta}\)</span>:</p>
<p><span class="math display">\[
\beta_j | \lambda_j, \tau, \sigma^2 \sim \mathcal{N}(0, \lambda_j^2 \tau^2 \sigma^2)
\]</span></p>
<p>where:
- <span class="math inline">\(\lambda_j\)</span> are <strong>local shrinkage parameters</strong>, controlling the sparsity of each coefficient.
- <span class="math inline">\(\tau\)</span> is a <strong>global shrinkage parameter</strong>, controlling overall sparsity.
- Different choices of <span class="math inline">\(p(\lambda_j)\)</span> lead to different priors (e.g., Horseshoe, Bayesian Lasso).</p>
<p>The <strong>full conditional posterior of <span class="math inline">\(\boldsymbol{\beta}\)</span></strong> is given by:</p>
<p><span class="math display">\[
\boldsymbol{\beta} | \mathbf{y}, \lambda, \tau, \sigma^2 \sim \mathcal{N} \left( \mathbf{A}^{-1} \mathbf{X}&#39; \mathbf{y}, \sigma^2 \mathbf{A}^{-1} \right)
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\mathbf{A} = \mathbf{X}&#39; \mathbf{X} + \Lambda^{-1}, \quad \Lambda = \tau^2 \text{diag}(\lambda_1^2, ..., \lambda_p^2).
\]</span></p>
<p>Computing <span class="math inline">\(\mathbf{A}^{-1}\)</span> directly is <strong>prohibitively expensive</strong> for large <span class="math inline">\(p\)</span>, motivating the need for an <strong>efficient sampling algorithm</strong>.</p>
<p>To sample from:</p>
<p><span class="math display">\[
\boldsymbol{\beta} \sim \mathcal{N} \left( \mathbf{A}^{-1} \mathbf{X}&#39; \mathbf{y}, \sigma^2 \mathbf{A}^{-1} \right)
\]</span></p>
<p>define:</p>
<p><span class="math display">\[
\mathbf{Q} = (\mathbf{X}&#39; \mathbf{X} + \Lambda^{-1})^{-1}, \quad \mathbf{b} = \mathbf{X}&#39; \mathbf{y}.
\]</span></p>
<p>Instead of inverting <span class="math inline">\(\mathbf{Q}\)</span>, introduce an <strong>auxiliary variable</strong>:</p>
<p><span class="math display">\[
\mathbf{v} = \mathbf{X} \boldsymbol{\beta} + \mathbf{z}, \quad \mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})
\]</span></p>
<p>which allows a <strong>Woodbury-based reformulation</strong> of the problem.</p>
<hr />
</div>
<div id="fast-gibbs-sampling-algorithm" class="section level4 hasAnchor" number="4.4.2.3">
<h4><span class="header-section-number">4.4.2.3</span> Fast Gibbs Sampling Algorithm<a href="linear-regression.html#fast-gibbs-sampling-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Step 1: Sample Auxiliary Variables</p>
<ol style="list-style-type: decimal">
<li>Sample <span class="math inline">\(\mathbf{u} \sim \mathcal{N}(\mathbf{0}, \Lambda)\)</span>.</li>
<li>Sample <span class="math inline">\(\boldsymbol{\delta} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}_n)\)</span>.</li>
</ol>
<p>Step 2: Compute a Latent Variable</p>
<ol start="3" style="list-style-type: decimal">
<li><p>Compute:</p>
<p><span class="math display">\[
\mathbf{v} = \mathbf{X} \mathbf{u} + \boldsymbol{\delta}
\]</span></p></li>
</ol>
<p>Step 3: Solve a Linear System</p>
<ol start="4" style="list-style-type: decimal">
<li><p>Solve for <span class="math inline">\(\mathbf{w}\)</span> in:</p>
<p><span class="math display">\[
(\mathbf{X} \Lambda \mathbf{X}&#39; + \mathbf{I}) \mathbf{w} = (\mathbf{y} - \mathbf{v})
\]</span></p>
<p>This avoids inverting <span class="math inline">\(\mathbf{X}&#39; \mathbf{X} + \Lambda^{-1}\)</span>.</p></li>
</ol>
<p>Step 4: Compute the Updated Sample for <span class="math inline">\(\boldsymbol{\beta}\)</span></p>
<ol start="5" style="list-style-type: decimal">
<li><p>Compute:</p>
<p><span class="math display">\[
\boldsymbol{\beta} = \mathbf{u} + \Lambda \mathbf{X}&#39; \mathbf{w}
\]</span></p></li>
</ol>
<p>📌 <strong>Key Insight:</strong> Instead of inverting a <span class="math inline">\(p \times p\)</span> matrix, the algorithm <strong>solves a linear system of size <span class="math inline">\(n \times n\)</span>, reducing computational complexity from <span class="math inline">\(O(p^3)\)</span> to <span class="math inline">\(O(n^3)\)</span></strong>.</p>
<hr />
</div>
<div id="computational-efficiency" class="section level4 hasAnchor" number="4.4.2.4">
<h4><span class="header-section-number">4.4.2.4</span> Computational Efficiency<a href="linear-regression.html#computational-efficiency" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>Standard MCMC:</strong> Requires inverting <span class="math inline">\(p \times p\)</span> matrices (<span class="math inline">\(O(p^3)\)</span>).</li>
<li><strong>Proposed method:</strong> Uses matrix multiplications and solves a system of size <span class="math inline">\(n \times n\)</span> (<span class="math inline">\(O(n^3)\)</span>).</li>
<li><strong>When <span class="math inline">\(p \gg n\)</span>, this leads to significant computational savings</strong>.</li>
</ul>
<p>For example, the algorithm <strong>achieves a 250× speedup when <span class="math inline">\(p = 5000\)</span></strong> compared to standard methods.</p>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="principal-component-analysis.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
