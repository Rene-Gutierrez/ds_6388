<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>10 Gaussian Mixture Models | DS 6388 Spring 2025</title>
  <meta name="description" content="10 Gaussian Mixture Models | DS 6388 Spring 2025" />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="10 Gaussian Mixture Models | DS 6388 Spring 2025" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="10 Gaussian Mixture Models | DS 6388 Spring 2025" />
  
  
  

<meta name="author" content="Rene Gutierrez University of Texas at El Paso" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="expectation-maximization.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> DS 6388: Multivariate Statistical Methods for High-dimensional Data</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#calendar"><i class="fa fa-check"></i><b>1.1</b> Calendar</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#importatnt-dates"><i class="fa fa-check"></i><b>1.1.1</b> Importatnt Dates</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#class-schedule"><i class="fa fa-check"></i><b>1.1.2</b> Class Schedule</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>2</b> Prerequisites</a>
<ul>
<li class="chapter" data-level="2.1" data-path="prerequisites.html"><a href="prerequisites.html#linear-algebra"><i class="fa fa-check"></i><b>2.1</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="prerequisites.html"><a href="prerequisites.html#linear-independence"><i class="fa fa-check"></i><b>2.1.1</b> Linear Independence</a></li>
<li class="chapter" data-level="2.1.2" data-path="prerequisites.html"><a href="prerequisites.html#column-space-of-a-matrix"><i class="fa fa-check"></i><b>2.1.2</b> Column Space of a Matrix</a></li>
<li class="chapter" data-level="2.1.3" data-path="prerequisites.html"><a href="prerequisites.html#rank-of-a-matrix"><i class="fa fa-check"></i><b>2.1.3</b> Rank of a Matrix</a></li>
<li class="chapter" data-level="2.1.4" data-path="prerequisites.html"><a href="prerequisites.html#full-rank-matrix"><i class="fa fa-check"></i><b>2.1.4</b> Full Rank Matrix</a></li>
<li class="chapter" data-level="2.1.5" data-path="prerequisites.html"><a href="prerequisites.html#inverse-matrix"><i class="fa fa-check"></i><b>2.1.5</b> Inverse Matrix</a></li>
<li class="chapter" data-level="2.1.6" data-path="prerequisites.html"><a href="prerequisites.html#positive-definite-matrix"><i class="fa fa-check"></i><b>2.1.6</b> Positive Definite Matrix</a></li>
<li class="chapter" data-level="2.1.7" data-path="prerequisites.html"><a href="prerequisites.html#singular-value-decomposition"><i class="fa fa-check"></i><b>2.1.7</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="2.1.8" data-path="prerequisites.html"><a href="prerequisites.html#eigendecomposition"><i class="fa fa-check"></i><b>2.1.8</b> Eigendecomposition</a></li>
<li class="chapter" data-level="2.1.9" data-path="prerequisites.html"><a href="prerequisites.html#idempotent-matrix"><i class="fa fa-check"></i><b>2.1.9</b> Idempotent Matrix</a></li>
<li class="chapter" data-level="2.1.10" data-path="prerequisites.html"><a href="prerequisites.html#determinant-of-a-matrix"><i class="fa fa-check"></i><b>2.1.10</b> Determinant of a Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="prerequisites.html"><a href="prerequisites.html#calculus"><i class="fa fa-check"></i><b>2.2</b> Calculus</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="prerequisites.html"><a href="prerequisites.html#gradient"><i class="fa fa-check"></i><b>2.2.1</b> Gradient</a></li>
<li class="chapter" data-level="2.2.2" data-path="prerequisites.html"><a href="prerequisites.html#hessian-matrix"><i class="fa fa-check"></i><b>2.2.2</b> Hessian Matrix</a></li>
<li class="chapter" data-level="2.2.3" data-path="prerequisites.html"><a href="prerequisites.html#applications-1"><i class="fa fa-check"></i><b>2.2.3</b> Applications:</a></li>
<li class="chapter" data-level="2.2.4" data-path="prerequisites.html"><a href="prerequisites.html#matrix-calculus"><i class="fa fa-check"></i><b>2.2.4</b> Matrix Calculus</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="prerequisites.html"><a href="prerequisites.html#probability"><i class="fa fa-check"></i><b>2.3</b> Probability</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="prerequisites.html"><a href="prerequisites.html#expected-value"><i class="fa fa-check"></i><b>2.3.1</b> Expected Value</a></li>
<li class="chapter" data-level="2.3.2" data-path="prerequisites.html"><a href="prerequisites.html#variance"><i class="fa fa-check"></i><b>2.3.2</b> Variance</a></li>
<li class="chapter" data-level="2.3.3" data-path="prerequisites.html"><a href="prerequisites.html#cross-covariance-matrix"><i class="fa fa-check"></i><b>2.3.3</b> Cross-Covariance Matrix</a></li>
<li class="chapter" data-level="2.3.4" data-path="prerequisites.html"><a href="prerequisites.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>2.3.4</b> Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="2.3.5" data-path="prerequisites.html"><a href="prerequisites.html#chi2-distribution"><i class="fa fa-check"></i><b>2.3.5</b> <span class="math inline">\(\chi^2\)</span> Distribution</a></li>
<li class="chapter" data-level="2.3.6" data-path="prerequisites.html"><a href="prerequisites.html#t-distribution"><i class="fa fa-check"></i><b>2.3.6</b> <span class="math inline">\(t\)</span> Distribution</a></li>
<li class="chapter" data-level="2.3.7" data-path="prerequisites.html"><a href="prerequisites.html#f-distribution"><i class="fa fa-check"></i><b>2.3.7</b> <span class="math inline">\(F\)</span> Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="prerequisites.html"><a href="prerequisites.html#statistics"><i class="fa fa-check"></i><b>2.4</b> Statistics</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="prerequisites.html"><a href="prerequisites.html#bias-of-an-estimator"><i class="fa fa-check"></i><b>2.4.1</b> Bias of an Estimator</a></li>
<li class="chapter" data-level="2.4.2" data-path="prerequisites.html"><a href="prerequisites.html#unbiased-estimator"><i class="fa fa-check"></i><b>2.4.2</b> Unbiased Estimator</a></li>
<li class="chapter" data-level="2.4.3" data-path="prerequisites.html"><a href="prerequisites.html#mean-square-error-of-an-estimator"><i class="fa fa-check"></i><b>2.4.3</b> Mean Square Error of an Estimator</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>3</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introduction.html"><a href="introduction.html#key-challenges"><i class="fa fa-check"></i><b>3.1</b> Key Challenges</a></li>
<li class="chapter" data-level="3.2" data-path="introduction.html"><a href="introduction.html#core-concepts"><i class="fa fa-check"></i><b>3.2</b> Core Concepts</a></li>
<li class="chapter" data-level="3.3" data-path="introduction.html"><a href="introduction.html#applications-4"><i class="fa fa-check"></i><b>3.3</b> Applications</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>4</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="linear-regression.html"><a href="linear-regression.html#machine-learning"><i class="fa fa-check"></i><b>4.1</b> Machine Learning</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="linear-regression.html"><a href="linear-regression.html#ordinary-least-squares"><i class="fa fa-check"></i><b>4.1.1</b> Ordinary Least Squares</a></li>
<li class="chapter" data-level="4.1.2" data-path="linear-regression.html"><a href="linear-regression.html#ridge-regression"><i class="fa fa-check"></i><b>4.1.2</b> Ridge Regression</a></li>
<li class="chapter" data-level="4.1.3" data-path="linear-regression.html"><a href="linear-regression.html#lasso-regression"><i class="fa fa-check"></i><b>4.1.3</b> Lasso Regression</a></li>
<li class="chapter" data-level="4.1.4" data-path="linear-regression.html"><a href="linear-regression.html#elastic-net"><i class="fa fa-check"></i><b>4.1.4</b> Elastic Net</a></li>
<li class="chapter" data-level="4.1.5" data-path="linear-regression.html"><a href="linear-regression.html#elastic-net-as-a-mixed-penalty"><i class="fa fa-check"></i><b>4.1.5</b> <strong>Elastic Net as a Mixed Penalty</strong></a></li>
<li class="chapter" data-level="4.1.6" data-path="linear-regression.html"><a href="linear-regression.html#other-options-of-regularization"><i class="fa fa-check"></i><b>4.1.6</b> Other Options of Regularization</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-linear-regression"><i class="fa fa-check"></i><b>4.2</b> Bayesian Linear Regression</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="linear-regression.html"><a href="linear-regression.html#basic-bayesian-linear-regression"><i class="fa fa-check"></i><b>4.2.1</b> Basic Bayesian Linear Regression</a></li>
<li class="chapter" data-level="4.2.2" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-lasso-regression"><i class="fa fa-check"></i><b>4.2.2</b> Bayesian Lasso Regression</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="linear-regression.html"><a href="linear-regression.html#computational-comparisson"><i class="fa fa-check"></i><b>4.3</b> Computational Comparisson</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="linear-regression.html"><a href="linear-regression.html#set-up"><i class="fa fa-check"></i><b>4.3.1</b> Set-Up</a></li>
<li class="chapter" data-level="4.3.2" data-path="linear-regression.html"><a href="linear-regression.html#simulation"><i class="fa fa-check"></i><b>4.3.2</b> Simulation</a></li>
<li class="chapter" data-level="4.3.3" data-path="linear-regression.html"><a href="linear-regression.html#ols"><i class="fa fa-check"></i><b>4.3.3</b> OLS</a></li>
<li class="chapter" data-level="4.3.4" data-path="linear-regression.html"><a href="linear-regression.html#ridge-regression-1"><i class="fa fa-check"></i><b>4.3.4</b> Ridge Regression</a></li>
<li class="chapter" data-level="4.3.5" data-path="linear-regression.html"><a href="linear-regression.html#lasso"><i class="fa fa-check"></i><b>4.3.5</b> Lasso</a></li>
<li class="chapter" data-level="4.3.6" data-path="linear-regression.html"><a href="linear-regression.html#basic-bayesian-regression"><i class="fa fa-check"></i><b>4.3.6</b> Basic Bayesian Regression</a></li>
<li class="chapter" data-level="4.3.7" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-lasso"><i class="fa fa-check"></i><b>4.3.7</b> Bayesian Lasso</a></li>
<li class="chapter" data-level="4.3.8" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-horseshoe-prior"><i class="fa fa-check"></i><b>4.3.8</b> Bayesian Horseshoe Prior</a></li>
<li class="chapter" data-level="4.3.9" data-path="linear-regression.html"><a href="linear-regression.html#results-comparisson"><i class="fa fa-check"></i><b>4.3.9</b> Results Comparisson</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="linear-regression.html"><a href="linear-regression.html#efficient-computation"><i class="fa fa-check"></i><b>4.4</b> Efficient Computation</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="linear-regression.html"><a href="linear-regression.html#efficient-computation-of-ridge-regression-using-the-woodbury-identity"><i class="fa fa-check"></i><b>4.4.1</b> Efficient Computation of Ridge Regression using the Woodbury Identity</a></li>
<li class="chapter" data-level="4.4.2" data-path="linear-regression.html"><a href="linear-regression.html#efficient-bayesian-sampling-for-gaussian-scale-mixture-priors"><i class="fa fa-check"></i><b>4.4.2</b> Efficient Bayesian Sampling for Gaussian Scale-Mixture Priors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>5</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="5.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#pca-as-a-low-rank-approximation-of-mathbfx"><i class="fa fa-check"></i><b>5.1</b> PCA as a Low-Rank Approximation of <span class="math inline">\(\mathbf{X}\)</span></a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#why-low-rank-approximation-matters"><i class="fa fa-check"></i><b>5.1.1</b> Why Low-Rank Approximation Matters</a></li>
<li class="chapter" data-level="5.1.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#the-singular-value-solution-is-the-best-low-rank-approximation-eckartyoungmirsky-theorem"><i class="fa fa-check"></i><b>5.1.2</b> The Singular Value Solution is the Best Low-Rank Approximation (Eckart–Young–Mirsky Theorem)</a></li>
<li class="chapter" data-level="5.1.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#eckartyoungmirsky-theorem-for-the-spectral-norm"><i class="fa fa-check"></i><b>5.1.3</b> Eckart–Young–Mirsky Theorem for the Spectral Norm</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#variance-maximization-in-pca"><i class="fa fa-check"></i><b>5.2</b> Variance Maximization in PCA</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#first-principal-cmponent"><i class="fa fa-check"></i><b>5.2.1</b> First Principal Cmponent</a></li>
<li class="chapter" data-level="5.2.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#second-principal-component"><i class="fa fa-check"></i><b>5.2.2</b> Second Principal Component</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#parafac-decomposition"><i class="fa fa-check"></i><b>5.3</b> PARAFAC decomposition</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#what-is-a-tensor"><i class="fa fa-check"></i><b>5.3.1</b> What is a Tensor?</a></li>
<li class="chapter" data-level="5.3.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#motivation-why-generalize-pca-to-tensors"><i class="fa fa-check"></i><b>5.3.2</b> Motivation: Why Generalize PCA to Tensors?</a></li>
<li class="chapter" data-level="5.3.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#parafac-decomposition-definition"><i class="fa fa-check"></i><b>5.3.3</b> PARAFAC Decomposition Definition</a></li>
<li class="chapter" data-level="5.3.4" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#how-is-parafac-related-to-pca"><i class="fa fa-check"></i><b>5.3.4</b> How is PARAFAC Related to PCA?</a></li>
<li class="chapter" data-level="5.3.5" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#key-properties-of-parafac-decomposition"><i class="fa fa-check"></i><b>5.3.5</b> Key Properties of PARAFAC Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#high-dimensional-pca"><i class="fa fa-check"></i><b>5.4</b> High-Dimensional PCA</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#high-dimensional-pca-p-gg-n"><i class="fa fa-check"></i><b>5.4.1</b> High-Dimensional PCA (<span class="math inline">\(p \gg n\)</span>)</a></li>
<li class="chapter" data-level="5.4.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#randomized-pca"><i class="fa fa-check"></i><b>5.4.2</b> Randomized PCA</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#principal-components-regression"><i class="fa fa-check"></i><b>5.5</b> Principal Components Regression</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#what-is-principal-components-regression"><i class="fa fa-check"></i><b>5.5.1</b> What is Principal Components Regression?</a></li>
<li class="chapter" data-level="5.5.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#why-use-pcr"><i class="fa fa-check"></i><b>5.5.2</b> Why Use PCR?</a></li>
<li class="chapter" data-level="5.5.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#mathematical-formulation"><i class="fa fa-check"></i><b>5.5.3</b> Mathematical Formulation</a></li>
<li class="chapter" data-level="5.5.4" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#choosing-the-number-of-components"><i class="fa fa-check"></i><b>5.5.4</b> Choosing the Number of Components</a></li>
<li class="chapter" data-level="5.5.5" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#advantages-of-pcr"><i class="fa fa-check"></i><b>5.5.5</b> Advantages of PCR</a></li>
<li class="chapter" data-level="5.5.6" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#disadvantages"><i class="fa fa-check"></i><b>5.5.6</b> Disadvantages</a></li>
<li class="chapter" data-level="5.5.7" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#randomized-pca-and-pcr"><i class="fa fa-check"></i><b>5.5.7</b> Randomized PCA and PCR</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="factor-analysis.html"><a href="factor-analysis.html"><i class="fa fa-check"></i><b>6</b> Factor Analysis</a>
<ul>
<li class="chapter" data-level="6.1" data-path="factor-analysis.html"><a href="factor-analysis.html#introduction-2"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="factor-analysis.html"><a href="factor-analysis.html#how-factor-analysis-works"><i class="fa fa-check"></i><b>6.1.1</b> How Factor Analysis Works</a></li>
<li class="chapter" data-level="6.1.2" data-path="factor-analysis.html"><a href="factor-analysis.html#why-is-factor-analysis-useful"><i class="fa fa-check"></i><b>6.1.2</b> Why is Factor Analysis Useful?</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="factor-analysis.html"><a href="factor-analysis.html#factor-analysis-model"><i class="fa fa-check"></i><b>6.2</b> Factor Analysis Model</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="factor-analysis.html"><a href="factor-analysis.html#model-specification-1"><i class="fa fa-check"></i><b>6.2.1</b> Model Specification</a></li>
<li class="chapter" data-level="6.2.2" data-path="factor-analysis.html"><a href="factor-analysis.html#assumptions"><i class="fa fa-check"></i><b>6.2.2</b> Assumptions</a></li>
<li class="chapter" data-level="6.2.3" data-path="factor-analysis.html"><a href="factor-analysis.html#variance-covariance-matrix"><i class="fa fa-check"></i><b>6.2.3</b> Variance-Covariance Matrix</a></li>
<li class="chapter" data-level="6.2.4" data-path="factor-analysis.html"><a href="factor-analysis.html#implications"><i class="fa fa-check"></i><b>6.2.4</b> Implications</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="factor-analysis.html"><a href="factor-analysis.html#estimation-in-factor-analysis"><i class="fa fa-check"></i><b>6.3</b> Estimation in Factor Analysis</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="factor-analysis.html"><a href="factor-analysis.html#principal-components-method"><i class="fa fa-check"></i><b>6.3.1</b> Principal Components Method</a></li>
<li class="chapter" data-level="6.3.2" data-path="factor-analysis.html"><a href="factor-analysis.html#principal-factor-method"><i class="fa fa-check"></i><b>6.3.2</b> Principal Factor Method</a></li>
<li class="chapter" data-level="6.3.3" data-path="factor-analysis.html"><a href="factor-analysis.html#maximum-likelihood-estimation-mle"><i class="fa fa-check"></i><b>6.3.3</b> Maximum Likelihood Estimation (MLE)</a></li>
<li class="chapter" data-level="6.3.4" data-path="factor-analysis.html"><a href="factor-analysis.html#bayesian-estimation-hierarchical-priors"><i class="fa fa-check"></i><b>6.3.4</b> Bayesian Estimation (Hierarchical Priors)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="factor-analysis.html"><a href="factor-analysis.html#estimating-factor-scores-in-factor-analysis"><i class="fa fa-check"></i><b>6.4</b> Estimating Factor Scores in Factor Analysis</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="factor-analysis.html"><a href="factor-analysis.html#ordinary-least-squares-ols-method"><i class="fa fa-check"></i><b>6.4.1</b> Ordinary Least Squares (OLS) Method</a></li>
<li class="chapter" data-level="6.4.2" data-path="factor-analysis.html"><a href="factor-analysis.html#weighted-least-squares-wls-method-bartletts-method"><i class="fa fa-check"></i><b>6.4.2</b> Weighted Least Squares (WLS) Method (Bartlett’s Method)</a></li>
<li class="chapter" data-level="6.4.3" data-path="factor-analysis.html"><a href="factor-analysis.html#regression-method-thompsons-estimator"><i class="fa fa-check"></i><b>6.4.3</b> Regression Method (Thompson’s Estimator)</a></li>
<li class="chapter" data-level="6.4.4" data-path="factor-analysis.html"><a href="factor-analysis.html#summary-of-factor-score-estimators"><i class="fa fa-check"></i><b>6.4.4</b> Summary of Factor Score Estimators</a></li>
<li class="chapter" data-level="6.4.5" data-path="factor-analysis.html"><a href="factor-analysis.html#key-takeaways"><i class="fa fa-check"></i><b>6.4.5</b> Key Takeaways</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="factor-analysis.html"><a href="factor-analysis.html#factor-analysis-example"><i class="fa fa-check"></i><b>6.5</b> Factor Analysis Example</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="factor-analysis.html"><a href="factor-analysis.html#load-required-libraries"><i class="fa fa-check"></i><b>6.5.1</b> Load Required Libraries</a></li>
<li class="chapter" data-level="6.5.2" data-path="factor-analysis.html"><a href="factor-analysis.html#generate-sample-data"><i class="fa fa-check"></i><b>6.5.2</b> Generate Sample Data</a></li>
<li class="chapter" data-level="6.5.3" data-path="factor-analysis.html"><a href="factor-analysis.html#estimation-of-factor-loadings-and-unique-variances"><i class="fa fa-check"></i><b>6.5.3</b> Estimation of Factor Loadings and Unique Variances</a></li>
<li class="chapter" data-level="6.5.4" data-path="factor-analysis.html"><a href="factor-analysis.html#estimate-factor-scores"><i class="fa fa-check"></i><b>6.5.4</b> Estimate Factor Scores</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html"><i class="fa fa-check"></i><b>7</b> Canonical Correlation Analysis (CCA)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#motivation-1"><i class="fa fa-check"></i><b>7.1</b> Motivation</a></li>
<li class="chapter" data-level="7.2" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#mathematical-formulation-1"><i class="fa fa-check"></i><b>7.2</b> Mathematical Formulation</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#key-properties-6"><i class="fa fa-check"></i><b>7.2.1</b> Key Properties</a></li>
<li class="chapter" data-level="7.2.2" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#implementation-in-r"><i class="fa fa-check"></i><b>7.2.2</b> Implementation in R</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#canonical-directions-estimation"><i class="fa fa-check"></i><b>7.3</b> Canonical Directions Estimation</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#problem-definition-with-random-variables"><i class="fa fa-check"></i><b>7.3.1</b> Problem Definition with Random Variables</a></li>
<li class="chapter" data-level="7.3.2" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#canonical-correlation-maximization-problem"><i class="fa fa-check"></i><b>7.3.2</b> Canonical Correlation Maximization Problem</a></li>
<li class="chapter" data-level="7.3.3" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#solving-for-the-canonical-directions-using-lagrange-multipliers"><i class="fa fa-check"></i><b>7.3.3</b> Solving for the Canonical Directions Using Lagrange Multipliers</a></li>
<li class="chapter" data-level="7.3.4" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#showing-that-the-matrices-have-the-same-eigenvalues"><i class="fa fa-check"></i><b>7.3.4</b> Showing That the Matrices Have the Same Eigenvalues</a></li>
<li class="chapter" data-level="7.3.5" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#showing-that-the-largest-eigenvalue-maximizes-the-objective-function"><i class="fa fa-check"></i><b>7.3.5</b> Showing That the Largest Eigenvalue Maximizes the Objective Function</a></li>
<li class="chapter" data-level="7.3.6" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#conclusion-3"><i class="fa fa-check"></i><b>7.3.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#hd-cca"><i class="fa fa-check"></i><b>7.4</b> HD CCA</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#regularized-canonical-correlation-analysis-ridge-cca"><i class="fa fa-check"></i><b>7.4.1</b> Regularized Canonical Correlation Analysis (Ridge CCA)</a></li>
<li class="chapter" data-level="7.4.2" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#sparse-canonical-correlation-analysis-sparse-cca"><i class="fa fa-check"></i><b>7.4.2</b> Sparse Canonical Correlation Analysis (Sparse CCA)**</a></li>
<li class="chapter" data-level="7.4.3" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#low-rank-approximation-cca-randomized-svd-approach"><i class="fa fa-check"></i><b>7.4.3</b> Low-Rank Approximation CCA (Randomized SVD Approach)</a></li>
<li class="chapter" data-level="7.4.4" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#factor-model-based-cca"><i class="fa fa-check"></i><b>7.4.4</b> Factor Model-Based CCA</a></li>
<li class="chapter" data-level="7.4.5" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#comparison-of-methods-for-high-dimensional-cca"><i class="fa fa-check"></i><b>7.4.5</b> Comparison of Methods for High-Dimensional CCA</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#conclusion-which-method-to-use"><i class="fa fa-check"></i><b>7.5</b> <strong>Conclusion: Which Method to Use?</strong></a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="k-means-clustering.html"><a href="k-means-clustering.html"><i class="fa fa-check"></i><b>8</b> k-Means Clustering</a>
<ul>
<li class="chapter" data-level="8.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#introduction-3"><i class="fa fa-check"></i><b>8.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#intuition-behind-k-means"><i class="fa fa-check"></i><b>8.1.1</b> Intuition Behind k-Means</a></li>
<li class="chapter" data-level="8.1.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#applications-of-k-means"><i class="fa fa-check"></i><b>8.1.2</b> Applications of k-Means</a></li>
<li class="chapter" data-level="8.1.3" data-path="k-means-clustering.html"><a href="k-means-clustering.html#choosing-the-number-of-clusters-k"><i class="fa fa-check"></i><b>8.1.3</b> Choosing the Number of Clusters <span class="math inline">\(k\)</span></a></li>
<li class="chapter" data-level="8.1.4" data-path="k-means-clustering.html"><a href="k-means-clustering.html#r-implementation-of-k-means"><i class="fa fa-check"></i><b>8.1.4</b> R Implementation of k-Means</a></li>
<li class="chapter" data-level="8.1.5" data-path="k-means-clustering.html"><a href="k-means-clustering.html#strengths-and-weaknesses-of-k-means"><i class="fa fa-check"></i><b>8.1.5</b> Strengths and Weaknesses of k-Means</a></li>
<li class="chapter" data-level="8.1.6" data-path="k-means-clustering.html"><a href="k-means-clustering.html#when-to-use-k-means"><i class="fa fa-check"></i><b>8.1.6</b> When to Use k-Means</a></li>
<li class="chapter" data-level="8.1.7" data-path="k-means-clustering.html"><a href="k-means-clustering.html#conclusion-k-means"><i class="fa fa-check"></i><b>8.1.7</b> Conclusion k-means</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#problem-statement"><i class="fa fa-check"></i><b>8.2</b> Problem Statement</a></li>
<li class="chapter" data-level="8.3" data-path="k-means-clustering.html"><a href="k-means-clustering.html#naive-solution"><i class="fa fa-check"></i><b>8.3</b> Naive Solution</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#why-evaluating-all-possible-partitions-works"><i class="fa fa-check"></i><b>8.3.1</b> Why Evaluating All Possible Partitions Works?</a></li>
<li class="chapter" data-level="8.3.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#why-does-this-work-for-all-possible-partitions"><i class="fa fa-check"></i><b>8.3.2</b> Why Does This Work for All Possible Partitions?</a></li>
<li class="chapter" data-level="8.3.3" data-path="k-means-clustering.html"><a href="k-means-clustering.html#efficiency-of-the-naive-approach"><i class="fa fa-check"></i><b>8.3.3</b> Efficiency of the Naive Approach</a></li>
<li class="chapter" data-level="8.3.4" data-path="k-means-clustering.html"><a href="k-means-clustering.html#key-intuition"><i class="fa fa-check"></i><b>8.3.4</b> Key Intuition</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="k-means-clustering.html"><a href="k-means-clustering.html#the-k-means-algorithm-lloyds-algorithm"><i class="fa fa-check"></i><b>8.4</b> The k-Means Algorithm (Lloyd’s algorithm)</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#why-does-k-means-works"><i class="fa fa-check"></i><b>8.4.1</b> Why does k-Means works?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="expectation-maximization.html"><a href="expectation-maximization.html"><i class="fa fa-check"></i><b>9</b> Expectation Maximization</a>
<ul>
<li class="chapter" data-level="9.1" data-path="expectation-maximization.html"><a href="expectation-maximization.html#introduction-4"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="expectation-maximization.html"><a href="expectation-maximization.html#model-framework"><i class="fa fa-check"></i><b>9.2</b> Model Framework</a></li>
<li class="chapter" data-level="9.3" data-path="expectation-maximization.html"><a href="expectation-maximization.html#algorithm"><i class="fa fa-check"></i><b>9.3</b> Algorithm</a></li>
<li class="chapter" data-level="9.4" data-path="expectation-maximization.html"><a href="expectation-maximization.html#proof-of-em-convergence"><i class="fa fa-check"></i><b>9.4</b> Proof of EM Convergence</a></li>
<li class="chapter" data-level="9.5" data-path="expectation-maximization.html"><a href="expectation-maximization.html#conclusion-4"><i class="fa fa-check"></i><b>9.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="gaussian-mixture-models.html"><a href="gaussian-mixture-models.html"><i class="fa fa-check"></i><b>10</b> Gaussian Mixture Models</a>
<ul>
<li class="chapter" data-level="10.1" data-path="gaussian-mixture-models.html"><a href="gaussian-mixture-models.html#introduction-5"><i class="fa fa-check"></i><b>10.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="gaussian-mixture-models.html"><a href="gaussian-mixture-models.html#model-definition"><i class="fa fa-check"></i><b>10.1.1</b> Model Definition</a></li>
<li class="chapter" data-level="10.1.2" data-path="gaussian-mixture-models.html"><a href="gaussian-mixture-models.html#latent-variable-interpretation"><i class="fa fa-check"></i><b>10.1.2</b> Latent Variable Interpretation</a></li>
<li class="chapter" data-level="10.1.3" data-path="gaussian-mixture-models.html"><a href="gaussian-mixture-models.html#parameters-to-estimate"><i class="fa fa-check"></i><b>10.1.3</b> Parameters to Estimate</a></li>
<li class="chapter" data-level="10.1.4" data-path="gaussian-mixture-models.html"><a href="gaussian-mixture-models.html#summary-3"><i class="fa fa-check"></i><b>10.1.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="gaussian-mixture-models.html"><a href="gaussian-mixture-models.html#em-on-gmm"><i class="fa fa-check"></i><b>10.2</b> EM on GMM</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="gaussian-mixture-models.html"><a href="gaussian-mixture-models.html#original-model"><i class="fa fa-check"></i><b>10.2.1</b> Original model</a></li>
<li class="chapter" data-level="10.2.2" data-path="gaussian-mixture-models.html"><a href="gaussian-mixture-models.html#new-model"><i class="fa fa-check"></i><b>10.2.2</b> New model</a></li>
<li class="chapter" data-level="10.2.3" data-path="gaussian-mixture-models.html"><a href="gaussian-mixture-models.html#model-equivalency"><i class="fa fa-check"></i><b>10.2.3</b> Model Equivalency</a></li>
<li class="chapter" data-level="10.2.4" data-path="gaussian-mixture-models.html"><a href="gaussian-mixture-models.html#expectation-computation"><i class="fa fa-check"></i><b>10.2.4</b> Expectation Computation</a></li>
<li class="chapter" data-level="10.2.5" data-path="gaussian-mixture-models.html"><a href="gaussian-mixture-models.html#maximization-step"><i class="fa fa-check"></i><b>10.2.5</b> Maximization Step</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">DS 6388 Spring 2025</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="gaussian-mixture-models" class="section level1 hasAnchor" number="10">
<h1><span class="header-section-number">10</span> Gaussian Mixture Models<a href="gaussian-mixture-models.html#gaussian-mixture-models" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introduction-5" class="section level2 hasAnchor" number="10.1">
<h2><span class="header-section-number">10.1</span> Introduction<a href="gaussian-mixture-models.html#introduction-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Gaussian Mixture Models (GMMs) are <strong>probabilistic models</strong> used to represent data as a <strong>mixture of several Gaussian distributions</strong>. Each component of the mixture corresponds to a cluster or subpopulation within the overall data distribution.</p>
<p>GMMs are widely used in <strong>clustering</strong>, <strong>density estimation</strong>, and as building blocks in more complex generative models.</p>
<hr />
<div id="model-definition" class="section level3 hasAnchor" number="10.1.1">
<h3><span class="header-section-number">10.1.1</span> Model Definition<a href="gaussian-mixture-models.html#model-definition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose <span class="math inline">\(\mathbf{x}_1, \dots, \mathbf{x}_n \in \mathbb{R}^p\)</span> are observed data points. A GMM models the distribution of each <span class="math inline">\(\mathbf{x}_i\)</span> as arising from one of <span class="math inline">\(K\)</span> components:</p>
<p><span class="math display">\[
    p(\mathbf{x}) = \sum_{k=1}^{K} \pi_k \cdot \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
    \]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\pi_k \in (0, 1)\)</span> are the <strong>mixing proportions</strong>, with <span class="math inline">\(\sum_{k=1}^{K} \pi_k = 1\)</span></p></li>
<li><p><span class="math inline">\(\boldsymbol{\mu}_k \in \mathbb{R}^p\)</span> is the <strong>mean</strong> of the <span class="math inline">\(k\)</span>-th Gaussian</p></li>
<li><p><span class="math inline">\(\boldsymbol{\Sigma}_k \in \mathbb{R}^{p \times p}\)</span> is the <strong>covariance matrix</strong> of the <span class="math inline">\(k\)</span>-th Gaussian</p></li>
<li><p><span class="math inline">\(\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma})\)</span> denotes the <strong>multivariate Gaussian density</strong></p>
<hr /></li>
</ul>
</div>
<div id="latent-variable-interpretation" class="section level3 hasAnchor" number="10.1.2">
<h3><span class="header-section-number">10.1.2</span> Latent Variable Interpretation<a href="gaussian-mixture-models.html#latent-variable-interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>GMMs introduce an <strong>unobserved categorical variable</strong> <span class="math inline">\(z_i \in \{1, \dots, K\}\)</span> indicating which component generated <span class="math inline">\(\mathbf{x}_i\)</span>. The full generative process is:</p>
<ol style="list-style-type: decimal">
<li><p>Sample component:<br />
<span class="math display">\[
    z_i \sim \text{Categorical}(\pi_1, \dots, \pi_K)
    \]</span></p></li>
<li><p>Given <span class="math inline">\(z_i = k\)</span>, sample data:<br />
<span class="math display">\[
\mathbf{x}_i \sim \mathcal{N}(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
\]</span></p></li>
</ol>
<p>The observed data likelihood becomes:</p>
<p><span class="math display">\[
    p(\mathbf{x}_i) = \sum_{k=1}^{K} \pi_k \cdot \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
    \]</span></p>
<hr />
</div>
<div id="parameters-to-estimate" class="section level3 hasAnchor" number="10.1.3">
<h3><span class="header-section-number">10.1.3</span> Parameters to Estimate<a href="gaussian-mixture-models.html#parameters-to-estimate" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The unknown parameters are:</p>
<ul>
<li>Mixing weights: <span class="math inline">\(\pi_1, \dots, \pi_K\)</span></li>
<li>Component means: <span class="math inline">\(\boldsymbol{\mu}_1, \dots, \boldsymbol{\mu}_K\)</span></li>
<li>Covariances: <span class="math inline">\(\boldsymbol{\Sigma}_1, \dots, \boldsymbol{\Sigma}_K\)</span></li>
</ul>
<p>These are typically estimated via the <strong>Expectation-Maximization (EM)</strong> algorithm.</p>
<hr />
</div>
<div id="summary-3" class="section level3 hasAnchor" number="10.1.4">
<h3><span class="header-section-number">10.1.4</span> Summary<a href="gaussian-mixture-models.html#summary-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>GMMs generalize <strong>k-means</strong> by allowing soft assignments and modeling cluster shape.</li>
<li>Each data point has a <strong>probabilistic association</strong> to each cluster.</li>
<li>Inference is typically performed via <strong>EM</strong>, which alternates between computing responsibilities and updating parameters.</li>
</ul>
</div>
</div>
<div id="em-on-gmm" class="section level2 hasAnchor" number="10.2">
<h2><span class="header-section-number">10.2</span> EM on GMM<a href="gaussian-mixture-models.html#em-on-gmm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="original-model" class="section level3 hasAnchor" number="10.2.1">
<h3><span class="header-section-number">10.2.1</span> Original model<a href="gaussian-mixture-models.html#original-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[
p(\mathbf{x}_i \mid \boldsymbol{\theta}) = \sum_{k=1}^{K} \pi_k \, \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_k, \mathbf{\Sigma}_k)
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\theta} = \{ \boldsymbol{\mu}_k, \mathbf{\Sigma}_k, \pi_k \}_{k=1}^K\)</span></p>
<p>Then the likelihood is:</p>
<p><span class="math display">\[
p(\mathbf{X} \mid \boldsymbol{\theta}) = \prod_{i=1}^{n} \left( \sum_{k=1}^{K} \pi_k \, \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_k, \mathbf{\Sigma}_k) \right)
\]</span></p>
<hr />
</div>
<div id="new-model" class="section level3 hasAnchor" number="10.2.2">
<h3><span class="header-section-number">10.2.2</span> New model<a href="gaussian-mixture-models.html#new-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math display">\[
\mathbf{x}_i \mid z_i = k \sim \mathcal{N}(\boldsymbol{\mu}_k, \mathbf{\Sigma}_k)
\]</span></p>
<p><span class="math display">\[
z_i \mid \boldsymbol{\theta} \sim \text{Categorical}(\pi_1, \dots, \pi_K)
\]</span></p>
<p>Both, independent across observations.</p>
<hr />
</div>
<div id="model-equivalency" class="section level3 hasAnchor" number="10.2.3">
<h3><span class="header-section-number">10.2.3</span> Model Equivalency<a href="gaussian-mixture-models.html#model-equivalency" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now let us check that the marginal distribution of the new model is the same as the original model.</p>
<p>Note that:</p>
<p><span class="math display">\[\begin{align*}
p(\mathbf{x}_i, z_i \mid \boldsymbol{\theta})
  &amp;= p(\mathbf{x}_i, \mid z_i \boldsymbol{\theta}) \\
  &amp;= \prod_{k=1}^{K} \left[ \pi_k  \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_k, \mathbf{\Sigma}_k) \right]^{\mathbb{I}[z_i = k]} \\
\end{align*}\]</span></p>
<p>Then:</p>
<p><span class="math display">\[
p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta}) = \prod_{i=1}^{n} \prod_{k=1}^{K} \left[ \pi_k \, \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_k, \mathbf{\Sigma}_k) \right]^{\mathbb{I}[z_i = k]}
\]</span></p>
<p>Then:</p>
<p><span class="math display">\[\begin{align*}
p(\mathbf{X} \mid \boldsymbol{\theta})
  &amp;= \sum_{ {\boldsymbol Z} } p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta}) \\
  &amp;= \sum_{z_1} \ldots \sum_{z_n} \prod_{i=1}^{n} \prod_{k=1}^{K} \left[ \pi_k \, \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_k, \mathbf{\Sigma}_k) \right]^{\mathbb{I}[z_i = k]} \\
  &amp;= \sum_{z_1} \ldots \sum_{z_{n-1}} \prod_{i=1}^{n-1} \prod_{k=1}^{K} \left[ \pi_k \, \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_k, \mathbf{\Sigma}_k) \right]^{\mathbb{I}[z_i = k]} \sum_{z_{n}} \prod_{k=1}^{K} \left[ \pi_k \, \mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_k, \mathbf{\Sigma}_k) \right]^{\mathbb{I}[z_n = k]} \\
  &amp;= \sum_{z_1} \ldots \sum_{z_{n-1}} \prod_{i=1}^{n-1} \prod_{k=1}^{K} \left[ \pi_k \, \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_k, \mathbf{\Sigma}_k) \right]^{\mathbb{I}[z_i = k]} \left( \sum_{k=1}^{K} \left[ \pi_k \, \mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_k, \mathbf{\Sigma}_k) \right]^{\mathbb{I}[z_n = k]} \right) \\
  &amp;= \prod_{i=1}^{n} \left( \sum_{k=1}^{K} \pi_k \, \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_k, \mathbf{\Sigma}_k) \right)
\end{align*}\]</span></p>
<p>So the marginal distribution of the new model is the same as the original model.</p>
<hr />
</div>
<div id="expectation-computation" class="section level3 hasAnchor" number="10.2.4">
<h3><span class="header-section-number">10.2.4</span> Expectation Computation<a href="gaussian-mixture-models.html#expectation-computation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>First we compute <span class="math inline">\(p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta})\)</span>, the distribution we use to compute the expectation.</p>
<p><span class="math display">\[\begin{align*}
p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta})
  &amp;= \frac{p(\mathbf{Z}, \mathbf{X} \mid \boldsymbol{\theta})}{p(\mathbf{X} \mid  \boldsymbol{\theta})} \\
  &amp;= \frac{p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}) p(\mathbf{Z} \mid \boldsymbol{\theta})}{p(\mathbf{X} \mid  \boldsymbol{\theta})} \\
  &amp;= \frac{ \prod_{i=1}^n p(z_i \mid  {\boldsymbol x} _i, \boldsymbol{\theta}) \prod_{i=1}^n p(z_i \mid \boldsymbol{\theta})}{\prod_{i=1}^n p( {\boldsymbol x} _i \mid  \boldsymbol{\theta})} \\
  &amp;= \prod_{i=1}^n \frac{ p(z_i \mid  {\boldsymbol x} _i, \boldsymbol{\theta}) p(z_i \mid \boldsymbol{\theta})}{ p( {\boldsymbol x} _i \mid  \boldsymbol{\theta})} \\
  &amp;= \prod_{i=1}^n p( {\boldsymbol x} _i \mid z_i, \boldsymbol{\theta})
\end{align*}\]</span></p>
<p>Now:</p>
<p><span class="math display">\[\begin{align*}
p(z_i = k \mid \mathbf{x}_i, \boldsymbol{\theta})
  &amp;= \frac{p(\mathbf{x}_i, z_i = k \mid \boldsymbol{\theta})}{p(\mathbf{x}_i \mid \boldsymbol{\theta})} \\
  &amp;= \frac{p(\mathbf{x}_i \mid z_i = k, \boldsymbol{\theta}) p(z_i = k \mid \boldsymbol{\theta})}{\sum_{l=1}^{K} \pi_l \, \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_l, \mathbf{\Sigma}_l)} \\
  &amp;= \frac{\pi_k \, \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_k, \mathbf{\Sigma}_k)}{\sum_{l=1}^{K} \pi_l \, \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_l, \mathbf{\Sigma}_l)} \\
  &amp;= \gamma_{ik}
\end{align*}\]</span></p>
<p>Finally, let us compute:</p>
<p><span class="math display">\[
\mathbb{E}_{\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}^{(t)}}[\log p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})]
\]</span></p>
<p>First, note that:</p>
<p><span class="math display">\[\begin{align*}
\log p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})
  &amp;= \log \left( \prod_{i=1}^{n} \prod_{k=1}^{K} \left[ \pi_k \, \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_k, \mathbf{\Sigma}_k) \right]^{\mathbb{I}[z_i = k]} \right) \\
  &amp;= \sum_{i=1}^{n} \sum_{k=1}^{K} \mathbb{I}[z_i = k] \log \left( \pi_k \, \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_k, \mathbf{\Sigma}_k) \right) \\
  &amp;= w^{n}
\end{align*}\]</span></p>
<p>And note that <span class="math inline">\(w^n\)</span> can be decomposed as follows:</p>
<p><span class="math display">\[
w^n = w^{n-1} + \sum_{k=1}^{K} \mathbb{I}[z_n = k] \log \left( \pi_k \, \mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_k, \mathbf{\Sigma}_k) \right)
\]</span></p>
<p>Then:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}_{\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}^{(t)}} &amp;[\log p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})]
  = \mathbb{E}_{\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}^{(t)}}[w^n] \\
  &amp;= \sum_{ {\boldsymbol Z} } w^n p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}) \\
  &amp;= \sum_{z_1} \ldots \sum_{z_n} \left( w^{n-1} + \sum_{k=1}^{K} \mathbb{I}[z_n = k] \log \left( \pi_k \, \mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_k, \mathbf{\Sigma}_k) \right) \right) \prod_{i=1}^n p(z_i \mid  {\boldsymbol x} _i, \boldsymbol{\theta}) \\
  &amp;= \sum_{z_1} \ldots \sum_{z_{n-1}} w^{n-1} \prod_{i=1}^{n-1} p(z_i \mid  {\boldsymbol x} _i, \boldsymbol{\theta}) \\
  &amp;\quad + \sum_{z_1} \ldots \sum_{z_{n-1}} \prod_{i=1}^{n-1} p(z_i \mid  {\boldsymbol x} _i, \boldsymbol{\theta}) \sum_{z_{n}} \sum_{k=1}^{K} \mathbb{I}[z_n = k] \log \left( \pi_k \, \mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_k, \mathbf{\Sigma}_k) \right) p(z_n \mid  {\boldsymbol x} _n, \boldsymbol{\theta}) \\
  &amp;= \sum_{z_1} \ldots \sum_{z_{n-1}} w^{n-1} \prod_{i=1}^{n-1} p(z_i \mid  {\boldsymbol x} _i, \boldsymbol{\theta}) \\
  &amp;\quad + \sum_{z_1} \ldots \sum_{z_{n-1}} \prod_{i=1}^{n-1} p(z_i \mid  {\boldsymbol x} _i, \boldsymbol{\theta}) \sum_{k=1}^{K} \log \left( \pi_k \, \mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_k, \mathbf{\Sigma}_k) \right) \gamma_{n k} \\
  &amp;= \sum_{z_1} \ldots \sum_{z_{n-1}} w^{n-1} \prod_{i=1}^{n-1} p(z_i \mid  {\boldsymbol x} _i, \boldsymbol{\theta}) \\
  &amp;\quad + \sum_{k=1}^{K} \log \left( \pi_k \, \mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_k, \mathbf{\Sigma}_k) \right) \gamma_{n k} \sum_{z_1} \ldots \sum_{z_{n-1}} \prod_{i=1}^{n-1} p(z_i \mid  {\boldsymbol x} _i, \boldsymbol{\theta}) \\
  &amp;= \sum_{z_1} \ldots \sum_{z_{n-1}} w^{n-1} \prod_{i=1}^{n-1} p(z_i \mid  {\boldsymbol x} _i, \boldsymbol{\theta}) + \sum_{k=1}^{K} \log \left( \pi_k \, \mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_k, \mathbf{\Sigma}_k) \right) \gamma_{n k} \\
  &amp;= \sum_{i=1}^n \sum_{k=1}^{K} \log \left( \pi_k \, \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_k, \mathbf{\Sigma}_k) \right) \gamma_{i k} \\
\end{align*}\]</span></p>
<p>Then:</p>
<p><span class="math display">\[\begin{align*}
Q(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)})
  &amp;= \sum_{i=1}^{n} \sum_{k=1}^{K} \gamma_{ik} \log \left( \pi_k \, \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_k, \mathbf{\Sigma}_k) \right) \\
  &amp;= \sum_{i=1}^{n} \sum_{k=1}^{K} \gamma_{ik} \left[ \log \pi_k + \log \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_k, \mathbf{\Sigma}_k) \right]
\end{align*}\]</span></p>
<hr />
</div>
<div id="maximization-step" class="section level3 hasAnchor" number="10.2.5">
<h3><span class="header-section-number">10.2.5</span> Maximization Step<a href="gaussian-mixture-models.html#maximization-step" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We need to maximize:</p>
<p><span class="math display">\[
\max_{ {\boldsymbol \pi} ,  {\boldsymbol \mu} ,  {\boldsymbol \Sigma} } Q(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)}) \\
\text{s.t.}  \sum_{k=1}^{K} \pi_k = 1
\]</span></p>
<p>then, the Lagrange multiplier is:</p>
<p><span class="math display">\[
\mathcal{L} = \sum_{i=1}^{n} \sum_{k=1}^{K} \gamma_{ik} \left[ \log \pi_k + \log \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_k, \mathbf{\Sigma}_k) \right] + \lambda \left( \sum_{k=1}^{K} \pi_k - 1 \right)
\]</span></p>
<div id="update-pi_k" class="section level4 hasAnchor" number="10.2.5.1">
<h4><span class="header-section-number">10.2.5.1</span> Update <span class="math inline">\(\pi_k\)</span><a href="gaussian-mixture-models.html#update-pi_k" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[\begin{align*}
&amp; \frac{\partial \mathcal{L}}{\partial \pi_k} = \sum_{i=1}^{n} \frac{\gamma_{ik}}{\pi_k} + \lambda = 0 \\
\Rightarrow &amp; \pi_k = - \frac{1}{\lambda} \sum_{i=1}^{n} \gamma_{ik} \\
\Rightarrow &amp; \sum_{k=1}^K \pi_k = - \frac{1}{\lambda} \sum_{k=1}^K \sum_{i=1}^{n} \gamma_{ik} \\
\Rightarrow &amp; 1 = - \frac{1}{\lambda} n \\
\Rightarrow &amp; \lambda = -n \\
\end{align*}\]</span></p>
<p>Then:</p>
<p><span class="math display">\[
\pi_k = \frac{1}{n} \sum_{i=1}^{n} \gamma_{ik}
\]</span></p>
<hr />
</div>
<div id="update-boldsymbolmu_k" class="section level4 hasAnchor" number="10.2.5.2">
<h4><span class="header-section-number">10.2.5.2</span> Update <span class="math inline">\(\boldsymbol{\mu}_k\)</span><a href="gaussian-mixture-models.html#update-boldsymbolmu_k" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[\begin{align*}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{\mu}_k} &amp;= -\sum_{i=1}^{n} \gamma_{ik} \mathbf{\Sigma}_k^{-1} (\mathbf{x}_i - \boldsymbol{\mu}_k) = 0 \\
  &amp;\Rightarrow \sum_{i=1}^{n} \gamma_{ik} \mathbf{\Sigma}_k^{-1} (\mathbf{x}_i - \boldsymbol{\mu}_k) = 0 \\
  &amp;\Rightarrow \sum_{i=1}^{n} \gamma_{ik} \mathbf{x}_i = \sum_{i=1}^{n} \gamma_{ik} \boldsymbol{\mu}_k \\
  &amp;\Rightarrow \boldsymbol{\mu}_k = \frac{\sum_{i=1}^{n} \gamma_{ik} \mathbf{x}_i}{\sum_{i=1}^{n} \gamma_{ik}}
\end{align*}\]</span></p>
<hr />
</div>
<div id="update-mathbfsigma_k" class="section level4 hasAnchor" number="10.2.5.3">
<h4><span class="header-section-number">10.2.5.3</span> Update <span class="math inline">\(\mathbf{\Sigma}_k\)</span><a href="gaussian-mixture-models.html#update-mathbfsigma_k" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[\begin{align*}
\frac{\partial \mathcal{L}}{\partial \mathbf{\Sigma}_k}
  &amp;= \frac{\partial}{\partial \mathbf{\Sigma}_k} \sum_{i=1}^{n} \gamma_{ik} \left( -\frac{1}{2} \log |\mathbf{\Sigma}_k| - \frac{1}{2} (\mathbf{x}_i - \boldsymbol{\mu}_k)^\top \mathbf{\Sigma}_k^{-1} (\mathbf{x}_i - \boldsymbol{\mu}_k) \right) \\
  &amp;= \sum_{i=1}^{n} \gamma_{ik} \left( -\frac{1}{2} \mathbf{\Sigma}_k^{-1} + \frac{1}{2} \mathbf{\Sigma}_k^{-1} (\mathbf{x}_i - \boldsymbol{\mu}_k) (\mathbf{x}_i - \boldsymbol{\mu}_k)&#39;  \mathbf{\Sigma}_k^{-1} \right) \\
\end{align*}\]</span></p>
<p>Then</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial \mathcal{L}}{\partial \mathbf{\Sigma}_k}
  &amp;= 0 \\
  &amp;\Rightarrow \sum_{i=1}^{n} \gamma_{ik} \left( \Sigma_k - (\mathbf{x}_i - \boldsymbol{\mu}_k) (\mathbf{x}_i - \boldsymbol{\mu}_k)&#39; \right) = 0 \\
  &amp;\Rightarrow \sum_{i=1}^{n} \gamma_{ik} \Sigma_k = \sum_{i=1}^{n} \gamma_{ik} (\mathbf{x}_i - \boldsymbol{\mu}_k) (\mathbf{x}_i - \boldsymbol{\mu}_k)&#39; \\
  &amp;\Rightarrow \Sigma_k \sum_{i=1}^{n} \gamma_{ik} = \sum_{i=1}^{n} \gamma_{ik} (\mathbf{x}_i - \boldsymbol{\mu}_k) (\mathbf{x}_i - \boldsymbol{\mu}_k)&#39; \\
  &amp;\Rightarrow \Sigma_k = \frac{\sum_{i=1}^{n} \gamma_{ik} (\mathbf{x}_i - \boldsymbol{\mu}_k) (\mathbf{x}_i - \boldsymbol{\mu}_k)&#39;}{\sum_{i=1}^{n} \gamma_{ik}} \\
\end{align*}\]</span></p>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="expectation-maximization.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
