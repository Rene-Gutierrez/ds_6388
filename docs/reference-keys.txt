tab:lr-coefficient-comparisson
tab:lr-performance-mse-pr2
tab:lr-post-processed-coefficient-comparisson
ds-6388-multivariate-statistical-methods-for-high-dimensional-data
calendar
importatnt-dates
class-schedule
prerequisites
linear-algebra
linear-independence
definition
example
key-points
importance
column-space-of-a-matrix
definition-1
properties
geometric-interpretation
rank-of-a-matrix
definition-2
key-points-1
example-1
properties-1
full-rank-matrix
for-a-square-matrix-m-n
for-a-rectangular-matrix-m-neq-n
example-2
key-properties
importance-1
inverse-matrix
conditions-for-a-matrix-to-have-an-inverse
properties-of-the-inverse-matrix
special-case-2-by-2-matrix
special-case-2-by-2-block-matrix
conditions-for-the-inverse-to-exist
explanation-of-the-terms
sherman-morrison-formula
requirements
explanation-of-the-terms-1
positive-definite-matrix
key-properties-1
characteristics-of-a-positive-definite-matrix
example-3
singular-value-decomposition
definition-3
interpretation-of-the-components
geometric-interpretation-1
key-points-2
example-4
applications-of-svd
eigendecomposition
definition-4
eigenvalues-and-eigenvectors
conditions-for-eigendecomposition
geometric-interpretation-2
example-5
applications-of-eigendecomposition
idempotent-matrix
key-properties-of-idempotent-matrices
examples
use-in-statistics
determinant-of-a-matrix
key-charachteristics-of-the-determinant
definition-5
applications-of-the-determinant
properties-of-the-determinant
calculus
gradient
key-points-3
example-6
applications
hessian-matrix
key-properties-2
example-7
applications-1
matrix-calculus
probability
expected-value
definition-6
key-properties-of-the-expected-value-of-a-random-vector
variance
definition-7
key-properties-of-the-variance-covariance-matrix
cross-covariance-matrix
definition-8
key-properties-3
multivariate-normal-distribution
definition-9
key-properties-4
chi2-distribution
key-properties-of-the-chi-squared-distribution
t-distribution
definition-10
key-properties-of-the-t-distribution
applications-2
f-distribution
definition-11
key-properties-of-the-f-distribution
applications-3
statistics
bias-of-an-estimator
key-points-4
example-8
unbiased-estimator
key-points-5
mean-square-error-of-an-estimator
key-components
key-properties-5
introduction
key-challenges
core-concepts
applications-4
linear-regression
machine-learning
ordinary-least-squares
model-specification
minimization-problem
solution
ridge-regression
introduction-1
non-singularity-of-the-approximation
ridge-regression-as-a-minimization-problem
conclusion
lasso-regression
lasso-regression-as-an-optimization-problem
elastic-net
elastic-net-as-a-mixed-penalty
why-use-elastic-net
optimization-problem
connections-to-lasso-and-ridge-regression
other-options-of-regularization
bayesian-linear-regression
basic-bayesian-linear-regression
posterior-distribution-of-boldsymbolbeta
connection-to-ridge-regression
behavior-of-the-posterior-distribution-as-lambda-varies
conclusion-1
bayesian-lasso-regression
hierarchical-representation-of-the-laplace-prior
posterior-distribution-and-map-estimator
behavior-of-the-posterior-as-lambda-and-sigma2-vary
computational-comparisson
set-up
simulation
ols
ridge-regression-1
lasso
basic-bayesian-regression
bayesian-lasso
bayesian-horseshoe-prior
results-comparisson
estimation-comparisson
performance-metrics
predictive-performance
variable-selection-post-processing
efficient-computation
efficient-computation-of-ridge-regression-using-the-woodbury-identity
efficient-bayesian-sampling-for-gaussian-scale-mixture-priors
motivation
model-setup-gaussian-scale-mixture-priors-in-bayesian-regression
fast-gibbs-sampling-algorithm
computational-efficiency
principal-component-analysis
pca-as-a-low-rank-approximation-of-mathbfx
why-low-rank-approximation-matters
the-singular-value-solution-is-the-best-low-rank-approximation-eckartyoungmirsky-theorem
problem-statement-eckartyoungmirsky-theorem
proof-eckartyoungmirsky-theorem
uniqueness-of-the-solution
key-condition-for-uniqueness
when-is-the-solution-not-unique
intuition
example-9
summary
eckartyoungmirsky-theorem-for-the-spectral-norm
spectral-norm-theorem-statement
spectral-norm-proof-sketch
non-uniqueness-for-the-spectral-norm
how-to-find-alternative-solutions
conclusion-for-the-spectral-norm
variance-maximization-in-pca
first-principal-cmponent
variance-of-the-projection
optimization-problem-rayleigh-quotient
eigenvalue-problem
connection-to-svd
second-principal-component
optimization-problem-1
solution-using-lagrange-multipliers
intuition-1
parafac-decomposition
what-is-a-tensor
motivation-why-generalize-pca-to-tensors
parafac-decomposition-definition
how-is-parafac-related-to-pca
key-properties-of-parafac-decomposition
high-dimensional-pca
high-dimensional-pca-p-gg-n
key-idea
steps-for-efficient-computation
why-does-this-work
efficient-computation-pca-p-gg-n
pca-methods
simulate-a-data-set
compare-results
computational-efficiency-comparison
larger-example
randomized-pca
random-matrices-in-pca
randomized-pca-rpca
why-use-random-matrices
theoretical-foundations-from-random-matrix-theory
when-to-use-random-matrix-methods-in-pca
limitations
conclusion-2
when-to-use-mathbfy-mathbfomega-mathbfx
advantages-of-mathbfy-mathbfomega-mathbfx
how-it-works
choosing-mathbfy-mathbfomega-mathbfx-vs-mathbfy-mathbfx-mathbfomega
applications-5
principal-components-regression
what-is-principal-components-regression
why-use-pcr
mathematical-formulation
step-1-perform-pca-on-mathbfx
step-2-select-k-principal-components
step-3-perform-linear-regression
step-4-transform-coefficients-back
choosing-the-number-of-components
advantages-of-pcr
disadvantages
randomized-pca-and-pcr
advantages-of-randomized-pca-in-pcr
why-is-this-important-in-pcr
example-use-case
factor-analysis
introduction-2
how-factor-analysis-works
why-is-factor-analysis-useful
factor-analysis-model
model-specification-1
assumptions
variance-covariance-matrix
implications
estimation-in-factor-analysis
principal-components-method
step-1-compute-the-sample-covariance-matrix
step-2-perform-eigen-decomposition
step-3-select-the-first-k-components
step-4-compute-the-factor-loadings-mathbflambda
step-5-estimate-mathbfpsi
principal-factor-method
step-1-compute-the-sample-covariance-matrix-1
step-2-estimate-mathbfpsi-first
step-3-perform-eigen-decomposition
step-4-refine-mathbfpsi
maximum-likelihood-estimation-mle
bayesian-estimation-hierarchical-priors
summary-1
estimating-factor-scores-in-factor-analysis
ordinary-least-squares-ols-method
weighted-least-squares-wls-method-bartletts-method
regression-method-thompsons-estimator
joint-distribution-of-mathbfx_i-and-mathbff_i
conditional-distribution-of-mathbff_i-given-mathbfx_i
interpretation-of-thompsons-estimator
summary-of-factor-score-estimators
key-takeaways
factor-analysis-example
load-required-libraries
generate-sample-data
estimation-of-factor-loadings-and-unique-variances
principal-components-method-1
maximum-likelihood-estimation-mle-1
estimate-factor-scores
ordinary-least-squares-ols-factor-scores
weighted-least-squares-bartletts-method
regression-thompsons-estimator
