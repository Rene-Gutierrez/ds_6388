<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Introduction | DS 6388 Spring 2025</title>
  <meta name="description" content="3 Introduction | DS 6388 Spring 2025" />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Introduction | DS 6388 Spring 2025" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Introduction | DS 6388 Spring 2025" />
  
  
  

<meta name="author" content="Rene Gutierrez University of Texas at El Paso" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="prerequisites.html"/>
<link rel="next" href="linear-regression.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> DS 6388: Multivariate Statistical Methods for High-dimensional Data</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#calendar"><i class="fa fa-check"></i><b>1.1</b> Calendar</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#importatnt-dates"><i class="fa fa-check"></i><b>1.1.1</b> Importatnt Dates</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#class-schedule"><i class="fa fa-check"></i><b>1.1.2</b> Class Schedule</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>2</b> Prerequisites</a>
<ul>
<li class="chapter" data-level="2.1" data-path="prerequisites.html"><a href="prerequisites.html#linear-algebra"><i class="fa fa-check"></i><b>2.1</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="prerequisites.html"><a href="prerequisites.html#linear-independence"><i class="fa fa-check"></i><b>2.1.1</b> Linear Independence</a></li>
<li class="chapter" data-level="2.1.2" data-path="prerequisites.html"><a href="prerequisites.html#column-space-of-a-matrix"><i class="fa fa-check"></i><b>2.1.2</b> Column Space of a Matrix</a></li>
<li class="chapter" data-level="2.1.3" data-path="prerequisites.html"><a href="prerequisites.html#rank-of-a-matrix"><i class="fa fa-check"></i><b>2.1.3</b> Rank of a Matrix</a></li>
<li class="chapter" data-level="2.1.4" data-path="prerequisites.html"><a href="prerequisites.html#full-rank-matrix"><i class="fa fa-check"></i><b>2.1.4</b> Full Rank Matrix</a></li>
<li class="chapter" data-level="2.1.5" data-path="prerequisites.html"><a href="prerequisites.html#inverse-matrix"><i class="fa fa-check"></i><b>2.1.5</b> Inverse Matrix</a></li>
<li class="chapter" data-level="2.1.6" data-path="prerequisites.html"><a href="prerequisites.html#positive-definite-matrix"><i class="fa fa-check"></i><b>2.1.6</b> Positive Definite Matrix</a></li>
<li class="chapter" data-level="2.1.7" data-path="prerequisites.html"><a href="prerequisites.html#singular-value-decomposition"><i class="fa fa-check"></i><b>2.1.7</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="2.1.8" data-path="prerequisites.html"><a href="prerequisites.html#eigendecomposition"><i class="fa fa-check"></i><b>2.1.8</b> Eigendecomposition</a></li>
<li class="chapter" data-level="2.1.9" data-path="prerequisites.html"><a href="prerequisites.html#idempotent-matrix"><i class="fa fa-check"></i><b>2.1.9</b> Idempotent Matrix</a></li>
<li class="chapter" data-level="2.1.10" data-path="prerequisites.html"><a href="prerequisites.html#determinant-of-a-matrix"><i class="fa fa-check"></i><b>2.1.10</b> Determinant of a Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="prerequisites.html"><a href="prerequisites.html#calculus"><i class="fa fa-check"></i><b>2.2</b> Calculus</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="prerequisites.html"><a href="prerequisites.html#gradient"><i class="fa fa-check"></i><b>2.2.1</b> Gradient</a></li>
<li class="chapter" data-level="2.2.2" data-path="prerequisites.html"><a href="prerequisites.html#hessian-matrix"><i class="fa fa-check"></i><b>2.2.2</b> Hessian Matrix</a></li>
<li class="chapter" data-level="2.2.3" data-path="prerequisites.html"><a href="prerequisites.html#applications-1"><i class="fa fa-check"></i><b>2.2.3</b> Applications:</a></li>
<li class="chapter" data-level="2.2.4" data-path="prerequisites.html"><a href="prerequisites.html#matrix-calculus"><i class="fa fa-check"></i><b>2.2.4</b> Matrix Calculus</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="prerequisites.html"><a href="prerequisites.html#probability"><i class="fa fa-check"></i><b>2.3</b> Probability</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="prerequisites.html"><a href="prerequisites.html#expected-value"><i class="fa fa-check"></i><b>2.3.1</b> Expected Value</a></li>
<li class="chapter" data-level="2.3.2" data-path="prerequisites.html"><a href="prerequisites.html#variance"><i class="fa fa-check"></i><b>2.3.2</b> Variance</a></li>
<li class="chapter" data-level="2.3.3" data-path="prerequisites.html"><a href="prerequisites.html#cross-covariance-matrix"><i class="fa fa-check"></i><b>2.3.3</b> Cross-Covariance Matrix</a></li>
<li class="chapter" data-level="2.3.4" data-path="prerequisites.html"><a href="prerequisites.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>2.3.4</b> Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="2.3.5" data-path="prerequisites.html"><a href="prerequisites.html#chi2-distribution"><i class="fa fa-check"></i><b>2.3.5</b> <span class="math inline">\(\chi^2\)</span> Distribution</a></li>
<li class="chapter" data-level="2.3.6" data-path="prerequisites.html"><a href="prerequisites.html#t-distribution"><i class="fa fa-check"></i><b>2.3.6</b> <span class="math inline">\(t\)</span> Distribution</a></li>
<li class="chapter" data-level="2.3.7" data-path="prerequisites.html"><a href="prerequisites.html#f-distribution"><i class="fa fa-check"></i><b>2.3.7</b> <span class="math inline">\(F\)</span> Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="prerequisites.html"><a href="prerequisites.html#statistics"><i class="fa fa-check"></i><b>2.4</b> Statistics</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="prerequisites.html"><a href="prerequisites.html#bias-of-an-estimator"><i class="fa fa-check"></i><b>2.4.1</b> Bias of an Estimator</a></li>
<li class="chapter" data-level="2.4.2" data-path="prerequisites.html"><a href="prerequisites.html#unbiased-estimator"><i class="fa fa-check"></i><b>2.4.2</b> Unbiased Estimator</a></li>
<li class="chapter" data-level="2.4.3" data-path="prerequisites.html"><a href="prerequisites.html#mean-square-error-of-an-estimator"><i class="fa fa-check"></i><b>2.4.3</b> Mean Square Error of an Estimator</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>3</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1" data-path="introduction.html"><a href="introduction.html#key-challenges"><i class="fa fa-check"></i><b>3.1</b> Key Challenges</a></li>
<li class="chapter" data-level="3.2" data-path="introduction.html"><a href="introduction.html#core-concepts"><i class="fa fa-check"></i><b>3.2</b> Core Concepts</a></li>
<li class="chapter" data-level="3.3" data-path="introduction.html"><a href="introduction.html#applications-4"><i class="fa fa-check"></i><b>3.3</b> Applications</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>4</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="linear-regression.html"><a href="linear-regression.html#machine-learning"><i class="fa fa-check"></i><b>4.1</b> Machine Learning</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="linear-regression.html"><a href="linear-regression.html#ordinary-least-squares"><i class="fa fa-check"></i><b>4.1.1</b> Ordinary Least Squares</a></li>
<li class="chapter" data-level="4.1.2" data-path="linear-regression.html"><a href="linear-regression.html#ridge-regression"><i class="fa fa-check"></i><b>4.1.2</b> Ridge Regression</a></li>
<li class="chapter" data-level="4.1.3" data-path="linear-regression.html"><a href="linear-regression.html#lasso-regression"><i class="fa fa-check"></i><b>4.1.3</b> Lasso Regression</a></li>
<li class="chapter" data-level="4.1.4" data-path="linear-regression.html"><a href="linear-regression.html#elastic-net"><i class="fa fa-check"></i><b>4.1.4</b> Elastic Net</a></li>
<li class="chapter" data-level="4.1.5" data-path="linear-regression.html"><a href="linear-regression.html#elastic-net-as-a-mixed-penalty"><i class="fa fa-check"></i><b>4.1.5</b> <strong>Elastic Net as a Mixed Penalty</strong></a></li>
<li class="chapter" data-level="4.1.6" data-path="linear-regression.html"><a href="linear-regression.html#other-options-of-regularization"><i class="fa fa-check"></i><b>4.1.6</b> Other Options of Regularization</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-linear-regression"><i class="fa fa-check"></i><b>4.2</b> Bayesian Linear Regression</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="linear-regression.html"><a href="linear-regression.html#basic-bayesian-linear-regression"><i class="fa fa-check"></i><b>4.2.1</b> Basic Bayesian Linear Regression</a></li>
<li class="chapter" data-level="4.2.2" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-lasso-regression"><i class="fa fa-check"></i><b>4.2.2</b> Bayesian Lasso Regression</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="linear-regression.html"><a href="linear-regression.html#computational-comparisson"><i class="fa fa-check"></i><b>4.3</b> Computational Comparisson</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="linear-regression.html"><a href="linear-regression.html#set-up"><i class="fa fa-check"></i><b>4.3.1</b> Set-Up</a></li>
<li class="chapter" data-level="4.3.2" data-path="linear-regression.html"><a href="linear-regression.html#simulation"><i class="fa fa-check"></i><b>4.3.2</b> Simulation</a></li>
<li class="chapter" data-level="4.3.3" data-path="linear-regression.html"><a href="linear-regression.html#ols"><i class="fa fa-check"></i><b>4.3.3</b> OLS</a></li>
<li class="chapter" data-level="4.3.4" data-path="linear-regression.html"><a href="linear-regression.html#ridge-regression-1"><i class="fa fa-check"></i><b>4.3.4</b> Ridge Regression</a></li>
<li class="chapter" data-level="4.3.5" data-path="linear-regression.html"><a href="linear-regression.html#lasso"><i class="fa fa-check"></i><b>4.3.5</b> Lasso</a></li>
<li class="chapter" data-level="4.3.6" data-path="linear-regression.html"><a href="linear-regression.html#basic-bayesian-regression"><i class="fa fa-check"></i><b>4.3.6</b> Basic Bayesian Regression</a></li>
<li class="chapter" data-level="4.3.7" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-lasso"><i class="fa fa-check"></i><b>4.3.7</b> Bayesian Lasso</a></li>
<li class="chapter" data-level="4.3.8" data-path="linear-regression.html"><a href="linear-regression.html#bayesian-horseshoe-prior"><i class="fa fa-check"></i><b>4.3.8</b> Bayesian Horseshoe Prior</a></li>
<li class="chapter" data-level="4.3.9" data-path="linear-regression.html"><a href="linear-regression.html#results-comparisson"><i class="fa fa-check"></i><b>4.3.9</b> Results Comparisson</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="linear-regression.html"><a href="linear-regression.html#efficient-computation"><i class="fa fa-check"></i><b>4.4</b> Efficient Computation</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="linear-regression.html"><a href="linear-regression.html#efficient-computation-of-ridge-regression-using-the-woodbury-identity"><i class="fa fa-check"></i><b>4.4.1</b> Efficient Computation of Ridge Regression using the Woodbury Identity</a></li>
<li class="chapter" data-level="4.4.2" data-path="linear-regression.html"><a href="linear-regression.html#efficient-bayesian-sampling-for-gaussian-scale-mixture-priors"><i class="fa fa-check"></i><b>4.4.2</b> Efficient Bayesian Sampling for Gaussian Scale-Mixture Priors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>5</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="5.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#pca-as-a-low-rank-approximation-of-mathbfx"><i class="fa fa-check"></i><b>5.1</b> PCA as a Low-Rank Approximation of <span class="math inline">\(\mathbf{X}\)</span></a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#why-low-rank-approximation-matters"><i class="fa fa-check"></i><b>5.1.1</b> Why Low-Rank Approximation Matters</a></li>
<li class="chapter" data-level="5.1.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#the-singular-value-solution-is-the-best-low-rank-approximation-eckartyoungmirsky-theorem"><i class="fa fa-check"></i><b>5.1.2</b> The Singular Value Solution is the Best Low-Rank Approximation (Eckart–Young–Mirsky Theorem)</a></li>
<li class="chapter" data-level="5.1.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#eckartyoungmirsky-theorem-for-the-spectral-norm"><i class="fa fa-check"></i><b>5.1.3</b> Eckart–Young–Mirsky Theorem for the Spectral Norm</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#variance-maximization-in-pca"><i class="fa fa-check"></i><b>5.2</b> Variance Maximization in PCA</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#first-principal-cmponent"><i class="fa fa-check"></i><b>5.2.1</b> First Principal Cmponent</a></li>
<li class="chapter" data-level="5.2.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#second-principal-component"><i class="fa fa-check"></i><b>5.2.2</b> Second Principal Component</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#parafac-decomposition"><i class="fa fa-check"></i><b>5.3</b> PARAFAC decomposition</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#what-is-a-tensor"><i class="fa fa-check"></i><b>5.3.1</b> What is a Tensor?</a></li>
<li class="chapter" data-level="5.3.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#motivation-why-generalize-pca-to-tensors"><i class="fa fa-check"></i><b>5.3.2</b> Motivation: Why Generalize PCA to Tensors?</a></li>
<li class="chapter" data-level="5.3.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#parafac-decomposition-definition"><i class="fa fa-check"></i><b>5.3.3</b> PARAFAC Decomposition Definition</a></li>
<li class="chapter" data-level="5.3.4" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#how-is-parafac-related-to-pca"><i class="fa fa-check"></i><b>5.3.4</b> How is PARAFAC Related to PCA?</a></li>
<li class="chapter" data-level="5.3.5" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#key-properties-of-parafac-decomposition"><i class="fa fa-check"></i><b>5.3.5</b> Key Properties of PARAFAC Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#high-dimensional-pca"><i class="fa fa-check"></i><b>5.4</b> High-Dimensional PCA</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#high-dimensional-pca-p-gg-n"><i class="fa fa-check"></i><b>5.4.1</b> High-Dimensional PCA (<span class="math inline">\(p \gg n\)</span>)</a></li>
<li class="chapter" data-level="5.4.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#randomized-pca"><i class="fa fa-check"></i><b>5.4.2</b> Randomized PCA</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#principal-components-regression"><i class="fa fa-check"></i><b>5.5</b> Principal Components Regression</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#what-is-principal-components-regression"><i class="fa fa-check"></i><b>5.5.1</b> What is Principal Components Regression?</a></li>
<li class="chapter" data-level="5.5.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#why-use-pcr"><i class="fa fa-check"></i><b>5.5.2</b> Why Use PCR?</a></li>
<li class="chapter" data-level="5.5.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#mathematical-formulation"><i class="fa fa-check"></i><b>5.5.3</b> Mathematical Formulation</a></li>
<li class="chapter" data-level="5.5.4" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#choosing-the-number-of-components"><i class="fa fa-check"></i><b>5.5.4</b> Choosing the Number of Components</a></li>
<li class="chapter" data-level="5.5.5" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#advantages-of-pcr"><i class="fa fa-check"></i><b>5.5.5</b> Advantages of PCR</a></li>
<li class="chapter" data-level="5.5.6" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#disadvantages"><i class="fa fa-check"></i><b>5.5.6</b> Disadvantages</a></li>
<li class="chapter" data-level="5.5.7" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#randomized-pca-and-pcr"><i class="fa fa-check"></i><b>5.5.7</b> Randomized PCA and PCR</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="factor-analysis.html"><a href="factor-analysis.html"><i class="fa fa-check"></i><b>6</b> Factor Analysis</a>
<ul>
<li class="chapter" data-level="6.1" data-path="factor-analysis.html"><a href="factor-analysis.html#introduction-2"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="factor-analysis.html"><a href="factor-analysis.html#how-factor-analysis-works"><i class="fa fa-check"></i><b>6.1.1</b> How Factor Analysis Works</a></li>
<li class="chapter" data-level="6.1.2" data-path="factor-analysis.html"><a href="factor-analysis.html#why-is-factor-analysis-useful"><i class="fa fa-check"></i><b>6.1.2</b> Why is Factor Analysis Useful?</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="factor-analysis.html"><a href="factor-analysis.html#factor-analysis-model"><i class="fa fa-check"></i><b>6.2</b> Factor Analysis Model</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="factor-analysis.html"><a href="factor-analysis.html#model-specification-1"><i class="fa fa-check"></i><b>6.2.1</b> Model Specification</a></li>
<li class="chapter" data-level="6.2.2" data-path="factor-analysis.html"><a href="factor-analysis.html#assumptions"><i class="fa fa-check"></i><b>6.2.2</b> Assumptions</a></li>
<li class="chapter" data-level="6.2.3" data-path="factor-analysis.html"><a href="factor-analysis.html#variance-covariance-matrix"><i class="fa fa-check"></i><b>6.2.3</b> Variance-Covariance Matrix</a></li>
<li class="chapter" data-level="6.2.4" data-path="factor-analysis.html"><a href="factor-analysis.html#implications"><i class="fa fa-check"></i><b>6.2.4</b> Implications</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="factor-analysis.html"><a href="factor-analysis.html#estimation-in-factor-analysis"><i class="fa fa-check"></i><b>6.3</b> Estimation in Factor Analysis</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="factor-analysis.html"><a href="factor-analysis.html#principal-components-method"><i class="fa fa-check"></i><b>6.3.1</b> Principal Components Method</a></li>
<li class="chapter" data-level="6.3.2" data-path="factor-analysis.html"><a href="factor-analysis.html#principal-factor-method"><i class="fa fa-check"></i><b>6.3.2</b> Principal Factor Method</a></li>
<li class="chapter" data-level="6.3.3" data-path="factor-analysis.html"><a href="factor-analysis.html#maximum-likelihood-estimation-mle"><i class="fa fa-check"></i><b>6.3.3</b> Maximum Likelihood Estimation (MLE)</a></li>
<li class="chapter" data-level="6.3.4" data-path="factor-analysis.html"><a href="factor-analysis.html#bayesian-estimation-hierarchical-priors"><i class="fa fa-check"></i><b>6.3.4</b> Bayesian Estimation (Hierarchical Priors)</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="factor-analysis.html"><a href="factor-analysis.html#estimating-factor-scores-in-factor-analysis"><i class="fa fa-check"></i><b>6.4</b> Estimating Factor Scores in Factor Analysis</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="factor-analysis.html"><a href="factor-analysis.html#ordinary-least-squares-ols-method"><i class="fa fa-check"></i><b>6.4.1</b> Ordinary Least Squares (OLS) Method</a></li>
<li class="chapter" data-level="6.4.2" data-path="factor-analysis.html"><a href="factor-analysis.html#weighted-least-squares-wls-method-bartletts-method"><i class="fa fa-check"></i><b>6.4.2</b> Weighted Least Squares (WLS) Method (Bartlett’s Method)</a></li>
<li class="chapter" data-level="6.4.3" data-path="factor-analysis.html"><a href="factor-analysis.html#regression-method-thompsons-estimator"><i class="fa fa-check"></i><b>6.4.3</b> Regression Method (Thompson’s Estimator)</a></li>
<li class="chapter" data-level="6.4.4" data-path="factor-analysis.html"><a href="factor-analysis.html#summary-of-factor-score-estimators"><i class="fa fa-check"></i><b>6.4.4</b> Summary of Factor Score Estimators</a></li>
<li class="chapter" data-level="6.4.5" data-path="factor-analysis.html"><a href="factor-analysis.html#key-takeaways"><i class="fa fa-check"></i><b>6.4.5</b> Key Takeaways</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="factor-analysis.html"><a href="factor-analysis.html#factor-analysis-example"><i class="fa fa-check"></i><b>6.5</b> Factor Analysis Example</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="factor-analysis.html"><a href="factor-analysis.html#load-required-libraries"><i class="fa fa-check"></i><b>6.5.1</b> Load Required Libraries</a></li>
<li class="chapter" data-level="6.5.2" data-path="factor-analysis.html"><a href="factor-analysis.html#generate-sample-data"><i class="fa fa-check"></i><b>6.5.2</b> Generate Sample Data</a></li>
<li class="chapter" data-level="6.5.3" data-path="factor-analysis.html"><a href="factor-analysis.html#estimation-of-factor-loadings-and-unique-variances"><i class="fa fa-check"></i><b>6.5.3</b> Estimation of Factor Loadings and Unique Variances</a></li>
<li class="chapter" data-level="6.5.4" data-path="factor-analysis.html"><a href="factor-analysis.html#estimate-factor-scores"><i class="fa fa-check"></i><b>6.5.4</b> Estimate Factor Scores</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html"><i class="fa fa-check"></i><b>7</b> Canonical Correlation Analysis (CCA)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#motivation-1"><i class="fa fa-check"></i><b>7.1</b> Motivation</a></li>
<li class="chapter" data-level="7.2" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#mathematical-formulation-1"><i class="fa fa-check"></i><b>7.2</b> Mathematical Formulation</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#key-properties-6"><i class="fa fa-check"></i><b>7.2.1</b> Key Properties</a></li>
<li class="chapter" data-level="7.2.2" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#implementation-in-r"><i class="fa fa-check"></i><b>7.2.2</b> Implementation in R</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#canonical-directions-estimation"><i class="fa fa-check"></i><b>7.3</b> Canonical Directions Estimation</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#problem-definition-with-random-variables"><i class="fa fa-check"></i><b>7.3.1</b> Problem Definition with Random Variables</a></li>
<li class="chapter" data-level="7.3.2" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#canonical-correlation-maximization-problem"><i class="fa fa-check"></i><b>7.3.2</b> Canonical Correlation Maximization Problem</a></li>
<li class="chapter" data-level="7.3.3" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#solving-for-the-canonical-directions-using-lagrange-multipliers"><i class="fa fa-check"></i><b>7.3.3</b> Solving for the Canonical Directions Using Lagrange Multipliers</a></li>
<li class="chapter" data-level="7.3.4" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#showing-that-the-matrices-have-the-same-eigenvalues"><i class="fa fa-check"></i><b>7.3.4</b> Showing That the Matrices Have the Same Eigenvalues</a></li>
<li class="chapter" data-level="7.3.5" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#showing-that-the-largest-eigenvalue-maximizes-the-objective-function"><i class="fa fa-check"></i><b>7.3.5</b> Showing That the Largest Eigenvalue Maximizes the Objective Function</a></li>
<li class="chapter" data-level="7.3.6" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#conclusion-3"><i class="fa fa-check"></i><b>7.3.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#hd-cca"><i class="fa fa-check"></i><b>7.4</b> HD CCA</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#regularized-canonical-correlation-analysis-ridge-cca"><i class="fa fa-check"></i><b>7.4.1</b> Regularized Canonical Correlation Analysis (Ridge CCA)</a></li>
<li class="chapter" data-level="7.4.2" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#sparse-canonical-correlation-analysis-sparse-cca"><i class="fa fa-check"></i><b>7.4.2</b> Sparse Canonical Correlation Analysis (Sparse CCA)**</a></li>
<li class="chapter" data-level="7.4.3" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#low-rank-approximation-cca-randomized-svd-approach"><i class="fa fa-check"></i><b>7.4.3</b> Low-Rank Approximation CCA (Randomized SVD Approach)</a></li>
<li class="chapter" data-level="7.4.4" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#factor-model-based-cca"><i class="fa fa-check"></i><b>7.4.4</b> Factor Model-Based CCA</a></li>
<li class="chapter" data-level="7.4.5" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#comparison-of-methods-for-high-dimensional-cca"><i class="fa fa-check"></i><b>7.4.5</b> Comparison of Methods for High-Dimensional CCA</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="canonical-correlation-analysis-cca.html"><a href="canonical-correlation-analysis-cca.html#conclusion-which-method-to-use"><i class="fa fa-check"></i><b>7.5</b> <strong>Conclusion: Which Method to Use?</strong></a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="k-means-clustering.html"><a href="k-means-clustering.html"><i class="fa fa-check"></i><b>8</b> k-Means Clustering</a>
<ul>
<li class="chapter" data-level="8.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#introduction-3"><i class="fa fa-check"></i><b>8.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#intuition-behind-k-means"><i class="fa fa-check"></i><b>8.1.1</b> Intuition Behind k-Means</a></li>
<li class="chapter" data-level="8.1.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#applications-of-k-means"><i class="fa fa-check"></i><b>8.1.2</b> Applications of k-Means</a></li>
<li class="chapter" data-level="8.1.3" data-path="k-means-clustering.html"><a href="k-means-clustering.html#choosing-the-number-of-clusters-k"><i class="fa fa-check"></i><b>8.1.3</b> Choosing the Number of Clusters <span class="math inline">\(k\)</span></a></li>
<li class="chapter" data-level="8.1.4" data-path="k-means-clustering.html"><a href="k-means-clustering.html#r-implementation-of-k-means"><i class="fa fa-check"></i><b>8.1.4</b> R Implementation of k-Means</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#section"><i class="fa fa-check"></i><b>8.2</b> <img src="_main_files/figure-html/k_means_optimal_cluster_elbow-1.png" width="672" /></a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#strengths-and-weaknesses-of-k-means"><i class="fa fa-check"></i><b>8.2.1</b> Strengths and Weaknesses of k-Means</a></li>
<li class="chapter" data-level="8.2.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#when-to-use-k-means"><i class="fa fa-check"></i><b>8.2.2</b> When to Use k-Means</a></li>
<li class="chapter" data-level="8.2.3" data-path="k-means-clustering.html"><a href="k-means-clustering.html#conclusion-k-means"><i class="fa fa-check"></i><b>8.2.3</b> Conclusion k-means</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="k-means-clustering.html"><a href="k-means-clustering.html#problem-statement"><i class="fa fa-check"></i><b>8.3</b> Problem Statement</a></li>
<li class="chapter" data-level="8.4" data-path="k-means-clustering.html"><a href="k-means-clustering.html#naive-solution"><i class="fa fa-check"></i><b>8.4</b> Naive Solution</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#why-evaluating-all-possible-partitions-works"><i class="fa fa-check"></i><b>8.4.1</b> Why Evaluating All Possible Partitions Works?</a></li>
<li class="chapter" data-level="8.4.2" data-path="k-means-clustering.html"><a href="k-means-clustering.html#why-does-this-work-for-all-possible-partitions"><i class="fa fa-check"></i><b>8.4.2</b> Why Does This Work for All Possible Partitions?</a></li>
<li class="chapter" data-level="8.4.3" data-path="k-means-clustering.html"><a href="k-means-clustering.html#efficiency-of-the-naive-approach"><i class="fa fa-check"></i><b>8.4.3</b> Efficiency of the Naive Approach</a></li>
<li class="chapter" data-level="8.4.4" data-path="k-means-clustering.html"><a href="k-means-clustering.html#key-intuition"><i class="fa fa-check"></i><b>8.4.4</b> Key Intuition</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="k-means-clustering.html"><a href="k-means-clustering.html#the-k-means-algorithm-lloyds-algorithm"><i class="fa fa-check"></i><b>8.5</b> The k-Means Algorithm (Lloyd’s algorithm)</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="k-means-clustering.html"><a href="k-means-clustering.html#why-does-k-means-works"><i class="fa fa-check"></i><b>8.5.1</b> Why does k-Means works?</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">DS 6388 Spring 2025</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">3</span> Introduction<a href="introduction.html#introduction" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>High-dimensional multivariate statistics deals with the analysis of data where both the number of variables (<span class="math inline">\(p\)</span>) and the number of observations (<span class="math inline">\(n\)</span>) can be large, and often, <span class="math inline">\(p\)</span> is comparable to or even greater than <span class="math inline">\(n\)</span>, making traditional methods unavailable or computationally impractical. This setting arises naturally in modern applications like genomics, finance, and machine learning, where data sets contain hundreds or thousands of variables.</p>
<div id="key-challenges" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Key Challenges<a href="introduction.html#key-challenges" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><strong>Curse of Dimensionality</strong>: As <span class="math inline">\(p\)</span> grows, classical statistical methods (e.g., ordinary least squares, classical covariance estimation) break down, become unstable or are computationally expensive.<br />
</li>
<li><strong>Multicollinearity</strong>: High correlation between variables can lead to singular or nearly singular covariance matrices. As the number of variables increases this scenario becomes more likely, if <span class="math inline">\(p \gg n\)</span> this have to be the case necessarily.</li>
<li><strong>Overfitting</strong>: When <span class="math inline">\(p \gg n\)</span>, models tend to fit noise rather than signal.</li>
</ul>
</div>
<div id="core-concepts" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Core Concepts<a href="introduction.html#core-concepts" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><strong>Regularized Estimation</strong>: Methods like ridge regression, LASSO, and graphical models introduce constraints to stabilize estimation.<br />
</li>
<li><strong>Dimensionality Reduction</strong>: Techniques like Principal Component Analysis (PCA) and Factor Analysis help summarize information in fewer dimensions.<br />
</li>
<li><strong>High-Dimensional Covariance and Precision Matrices</strong>: Classical estimators (e.g., sample covariance) fail when <span class="math inline">\(p &gt; n\)</span>, requiring alternatives like shrinkage estimators or sparsity inducing approaches.<br />
</li>
<li><strong>Multiple Testing and False Discovery Rate (FDR)</strong>: In high-dimensional settings, multiple hypothesis tests lead to inflated error rates, necessitating corrections like the Benjamini-Hochberg procedure.</li>
</ol>
</div>
<div id="applications-4" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Applications<a href="introduction.html#applications-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><strong>Genomics</strong>: Identifying genes associated with diseases from thousands of genetic markers.<br />
</li>
<li><strong>Finance</strong>: Portfolio optimization where the number of assets is large relative to available data.<br />
</li>
<li><strong>Machine Learning</strong>: Feature selection and model regularization in predictive modeling.</li>
</ul>
<p>High-dimensional statistics continues to evolve, with ongoing research in areas like robust estimation, Bayesian methods, and deep learning applications. As more computational power and data becomes availale, methods that were considered High-Dimensional can be approached with traditional methods.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="prerequisites.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linear-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
